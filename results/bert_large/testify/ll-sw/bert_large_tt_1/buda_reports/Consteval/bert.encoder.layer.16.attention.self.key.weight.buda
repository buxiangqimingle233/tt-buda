{
    "graph": {},
    "nodes": {
        "bert.encoder.layer.16.attention.self.key.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.16.attention.self.key.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: transpose_861 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_861"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.16.attention.self.key.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1864
        },
        "bert.encoder.layer.16.attention.self.key.weight.consteval_graph.output": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Output",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [
                "Data: transpose_861 (port_0)"
            ],
            "input_node_to_edge_type": {
                "transpose_861": "Data"
            },
            "input_nodes": [
                "transpose_861"
            ],
            "input_tms": [
                []
            ],
            "is_cross_epoch_type": false,
            "is_saved_intermediate": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.16.attention.self.key.weight.consteval_graph.output",
            "opcode": "Output",
            "outgoing_edge_port_info": [],
            "output_df": "Float32",
            "output_nodes": [],
            "pybuda": 1,
            "queue_type": "output",
            "tags": {},
            "type": "Output",
            "unique_id": 1865
        },
        "transpose_861": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: bert.encoder.layer.16.attention.self.key.weight (port_0)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.16.attention.self.key.weight": "Data"
            },
            "input_nodes": [
                "bert.encoder.layer.16.attention.self.key.weight"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_861",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: bert.encoder.layer.16.attention.self.key.weight.consteval_graph.output (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "bert.encoder.layer.16.attention.self.key.weight.consteval_graph.output"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_861",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 1866
        }
    },
    "topological_sorted_nodes": [
        "bert.encoder.layer.16.attention.self.key.weight",
        "transpose_861",
        "bert.encoder.layer.16.attention.self.key.weight.consteval_graph.output"
    ]
}