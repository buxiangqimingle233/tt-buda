{
    "graph": {},
    "nodes": {
        "bert.encoder.layer.12.attention.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.12.attention.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: bert.encoder.layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "bert.encoder.layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.12.attention.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 3939
        },
        "bert.encoder.layer.12.attention.output.LayerNorm.bias.consteval_graph.output": {
            "cache": {
                "shape": [
                    1,
                    32,
                    1024
                ]
            },
            "class": "Output",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [
                "Data: bert.encoder.layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0 (port_0)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0": "Data"
            },
            "input_nodes": [
                "bert.encoder.layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0"
            ],
            "input_tms": [
                []
            ],
            "is_cross_epoch_type": false,
            "is_saved_intermediate": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.12.attention.output.LayerNorm.bias.consteval_graph.output",
            "opcode": "Output",
            "outgoing_edge_port_info": [],
            "output_df": "Float32",
            "output_nodes": [],
            "pybuda": 1,
            "queue_type": "output",
            "tags": {},
            "type": "Output",
            "unique_id": 3940
        },
        "bert.encoder.layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0": {
            "cache": {
                "shape": [
                    1,
                    32,
                    1024
                ]
            },
            "class": "tile_broadcast(-2,32,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: bert.encoder.layer.12.attention.output.LayerNorm.bias (port_0)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.12.attention.output.LayerNorm.bias": "Data"
            },
            "input_nodes": [
                "bert.encoder.layer.12.attention.output.LayerNorm.bias"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "bert.encoder.layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0",
            "op_type": {
                "attrs": [
                    -2,
                    32
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "tile_broadcast"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: bert.encoder.layer.12.attention.output.LayerNorm.bias.consteval_graph.output (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "bert.encoder.layer.12.attention.output.LayerNorm.bias.consteval_graph.output"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_680",
                "original_op_type": "layernorm"
            },
            "type": "tile_broadcast",
            "unique_id": 3941
        }
    },
    "topological_sorted_nodes": [
        "bert.encoder.layer.12.attention.output.LayerNorm.bias",
        "bert.encoder.layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0",
        "bert.encoder.layer.12.attention.output.LayerNorm.bias.consteval_graph.output"
    ]
}