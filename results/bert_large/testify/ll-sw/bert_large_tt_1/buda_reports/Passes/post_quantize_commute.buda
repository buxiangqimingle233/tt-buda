{
    "graph": {},
    "nodes": {
        "add_1003": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1002 (port_0)",
                "Data: bert.encoder.layer.18.intermediate.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.18.intermediate.dense.bias": "Data",
                "reshape_1002": "Data"
            },
            "input_nodes": [
                "reshape_1002",
                "bert.encoder.layer.18.intermediate.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1003",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: gelu_1004 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "gelu_1004"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_1003",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 343
        },
        "add_1009": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1008 (port_0)",
                "Data: bert.encoder.layer.18.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.18.output.dense.bias": "Data",
                "reshape_1008": "Data"
            },
            "input_nodes": [
                "reshape_1008",
                "bert.encoder.layer.18.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1009",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_1010 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_1010"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_1009",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 258
        },
        "add_1011": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_1010 (port_0)",
                "Data: layernorm_998 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_998": "Data",
                "nop_1010": "Data"
            },
            "input_nodes": [
                "nop_1010",
                "layernorm_998"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_1011",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_1012 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_1012"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertOutput::output",
                "original_op_name": "add_1011",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 224
        },
        "add_1017": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1016 (port_0)",
                "Data: bert.encoder.layer.19.attention.self.query.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.19.attention.self.query.bias": "Data",
                "reshape_1016": "Data"
            },
            "input_nodes": [
                "reshape_1016",
                "bert.encoder.layer.19.attention.self.query.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1017",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_1018 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_1018"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_1017",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 474
        },
        "add_102": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_101 (port_0)",
                "Data: bert.encoder.layer.1.intermediate.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.1.intermediate.dense.bias": "Data",
                "reshape_101": "Data"
            },
            "input_nodes": [
                "reshape_101",
                "bert.encoder.layer.1.intermediate.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_102",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: gelu_103 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "gelu_103"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_102",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1533
        },
        "add_1023": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1022 (port_0)",
                "Data: bert.encoder.layer.19.attention.self.key.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.19.attention.self.key.bias": "Data",
                "reshape_1022": "Data"
            },
            "input_nodes": [
                "reshape_1022",
                "bert.encoder.layer.19.attention.self.key.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1023",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_1024 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_1024"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_1023",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 493
        },
        "add_1030": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: multiply_1029 (port_0)",
                "Data: multiply_22 (port_1)"
            ],
            "input_node_to_edge_type": {
                "multiply_1029": "Data",
                "multiply_22": "Data"
            },
            "input_nodes": [
                "multiply_1029",
                "multiply_22"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1030",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: softmax_1031 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "softmax_1031"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "add_1030",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 374
        },
        "add_1037": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1036 (port_0)",
                "Data: bert.encoder.layer.19.attention.self.value.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.19.attention.self.value.bias": "Data",
                "reshape_1036": "Data"
            },
            "input_nodes": [
                "reshape_1036",
                "bert.encoder.layer.19.attention.self.value.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1037",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_1038 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_1038"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_1037",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 390
        },
        "add_1048": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1047 (port_0)",
                "Data: bert.encoder.layer.19.attention.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.19.attention.output.dense.bias": "Data",
                "reshape_1047": "Data"
            },
            "input_nodes": [
                "reshape_1047",
                "bert.encoder.layer.19.attention.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1048",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_1049 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_1049"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_1048",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 223
        },
        "add_1050": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_1049 (port_0)",
                "Data: layernorm_1012 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_1012": "Data",
                "nop_1049": "Data"
            },
            "input_nodes": [
                "nop_1049",
                "layernorm_1012"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_1050",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_1051 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_1051"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output",
                "original_op_name": "add_1050",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 192
        },
        "add_1056": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1055 (port_0)",
                "Data: bert.encoder.layer.19.intermediate.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.19.intermediate.dense.bias": "Data",
                "reshape_1055": "Data"
            },
            "input_nodes": [
                "reshape_1055",
                "bert.encoder.layer.19.intermediate.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1056",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: gelu_1057 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "gelu_1057"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_1056",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 273
        },
        "add_1062": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1061 (port_0)",
                "Data: bert.encoder.layer.19.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.19.output.dense.bias": "Data",
                "reshape_1061": "Data"
            },
            "input_nodes": [
                "reshape_1061",
                "bert.encoder.layer.19.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1062",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_1063 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_1063"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_1062",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 191
        },
        "add_1064": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_1063 (port_0)",
                "Data: layernorm_1051 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_1051": "Data",
                "nop_1063": "Data"
            },
            "input_nodes": [
                "nop_1063",
                "layernorm_1051"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_1064",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_1065 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_1065"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertOutput::output",
                "original_op_name": "add_1064",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 160
        },
        "add_1070": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1069 (port_0)",
                "Data: bert.encoder.layer.20.attention.self.query.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.20.attention.self.query.bias": "Data",
                "reshape_1069": "Data"
            },
            "input_nodes": [
                "reshape_1069",
                "bert.encoder.layer.20.attention.self.query.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1070",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_1071 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_1071"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_1070",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 404
        },
        "add_1076": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1075 (port_0)",
                "Data: bert.encoder.layer.20.attention.self.key.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.20.attention.self.key.bias": "Data",
                "reshape_1075": "Data"
            },
            "input_nodes": [
                "reshape_1075",
                "bert.encoder.layer.20.attention.self.key.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1076",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_1077 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_1077"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_1076",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 423
        },
        "add_108": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_107 (port_0)",
                "Data: bert.encoder.layer.1.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.1.output.dense.bias": "Data",
                "reshape_107": "Data"
            },
            "input_nodes": [
                "reshape_107",
                "bert.encoder.layer.1.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_108",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_109 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_109"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_108",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1448
        },
        "add_1083": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: multiply_1082 (port_0)",
                "Data: multiply_22 (port_1)"
            ],
            "input_node_to_edge_type": {
                "multiply_1082": "Data",
                "multiply_22": "Data"
            },
            "input_nodes": [
                "multiply_1082",
                "multiply_22"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1083",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: softmax_1084 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "softmax_1084"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "add_1083",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 304
        },
        "add_1090": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1089 (port_0)",
                "Data: bert.encoder.layer.20.attention.self.value.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.20.attention.self.value.bias": "Data",
                "reshape_1089": "Data"
            },
            "input_nodes": [
                "reshape_1089",
                "bert.encoder.layer.20.attention.self.value.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1090",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_1091 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_1091"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_1090",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 320
        },
        "add_110": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_109 (port_0)",
                "Data: layernorm_97 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_97": "Data",
                "nop_109": "Data"
            },
            "input_nodes": [
                "nop_109",
                "layernorm_97"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_110",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_111 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_111"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertOutput::output",
                "original_op_name": "add_110",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1414
        },
        "add_1101": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1100 (port_0)",
                "Data: bert.encoder.layer.20.attention.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.20.attention.output.dense.bias": "Data",
                "reshape_1100": "Data"
            },
            "input_nodes": [
                "reshape_1100",
                "bert.encoder.layer.20.attention.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1101",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_1102 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_1102"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_1101",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 159
        },
        "add_1103": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_1102 (port_0)",
                "Data: layernorm_1065 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_1065": "Data",
                "nop_1102": "Data"
            },
            "input_nodes": [
                "nop_1102",
                "layernorm_1065"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_1103",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_1104 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_1104"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output",
                "original_op_name": "add_1103",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 129
        },
        "add_1109": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1108 (port_0)",
                "Data: bert.encoder.layer.20.intermediate.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.20.intermediate.dense.bias": "Data",
                "reshape_1108": "Data"
            },
            "input_nodes": [
                "reshape_1108",
                "bert.encoder.layer.20.intermediate.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1109",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: gelu_1110 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "gelu_1110"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_1109",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 204
        },
        "add_1115": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1114 (port_0)",
                "Data: bert.encoder.layer.20.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.20.output.dense.bias": "Data",
                "reshape_1114": "Data"
            },
            "input_nodes": [
                "reshape_1114",
                "bert.encoder.layer.20.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1115",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_1116 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_1116"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_1115",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 128
        },
        "add_1117": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_1116 (port_0)",
                "Data: layernorm_1104 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_1104": "Data",
                "nop_1116": "Data"
            },
            "input_nodes": [
                "nop_1116",
                "layernorm_1104"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_1117",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_1118 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_1118"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertOutput::output",
                "original_op_name": "add_1117",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 105
        },
        "add_1123": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1122 (port_0)",
                "Data: bert.encoder.layer.21.attention.self.query.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.21.attention.self.query.bias": "Data",
                "reshape_1122": "Data"
            },
            "input_nodes": [
                "reshape_1122",
                "bert.encoder.layer.21.attention.self.query.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1123",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_1124 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_1124"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_1123",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 334
        },
        "add_1129": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1128 (port_0)",
                "Data: bert.encoder.layer.21.attention.self.key.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.21.attention.self.key.bias": "Data",
                "reshape_1128": "Data"
            },
            "input_nodes": [
                "reshape_1128",
                "bert.encoder.layer.21.attention.self.key.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1129",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_1130 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_1130"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_1129",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 353
        },
        "add_1136": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: multiply_1135 (port_0)",
                "Data: multiply_22 (port_1)"
            ],
            "input_node_to_edge_type": {
                "multiply_1135": "Data",
                "multiply_22": "Data"
            },
            "input_nodes": [
                "multiply_1135",
                "multiply_22"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1136",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: softmax_1137 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "softmax_1137"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "add_1136",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 234
        },
        "add_1143": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1142 (port_0)",
                "Data: bert.encoder.layer.21.attention.self.value.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.21.attention.self.value.bias": "Data",
                "reshape_1142": "Data"
            },
            "input_nodes": [
                "reshape_1142",
                "bert.encoder.layer.21.attention.self.value.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1143",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_1144 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_1144"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_1143",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 250
        },
        "add_1154": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1153 (port_0)",
                "Data: bert.encoder.layer.21.attention.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.21.attention.output.dense.bias": "Data",
                "reshape_1153": "Data"
            },
            "input_nodes": [
                "reshape_1153",
                "bert.encoder.layer.21.attention.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1154",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_1155 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_1155"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_1154",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 104
        },
        "add_1156": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_1155 (port_0)",
                "Data: layernorm_1118 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_1118": "Data",
                "nop_1155": "Data"
            },
            "input_nodes": [
                "nop_1155",
                "layernorm_1118"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_1156",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_1157 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_1157"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output",
                "original_op_name": "add_1156",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 84
        },
        "add_116": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_115 (port_0)",
                "Data: bert.encoder.layer.2.attention.self.query.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.2.attention.self.query.bias": "Data",
                "reshape_115": "Data"
            },
            "input_nodes": [
                "reshape_115",
                "bert.encoder.layer.2.attention.self.query.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_116",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_117 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_117"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_116",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1637
        },
        "add_1162": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1161 (port_0)",
                "Data: bert.encoder.layer.21.intermediate.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.21.intermediate.dense.bias": "Data",
                "reshape_1161": "Data"
            },
            "input_nodes": [
                "reshape_1161",
                "bert.encoder.layer.21.intermediate.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1162",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: gelu_1163 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "gelu_1163"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_1162",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 141
        },
        "add_1168": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1167 (port_0)",
                "Data: bert.encoder.layer.21.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.21.output.dense.bias": "Data",
                "reshape_1167": "Data"
            },
            "input_nodes": [
                "reshape_1167",
                "bert.encoder.layer.21.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1168",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_1169 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_1169"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_1168",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 83
        },
        "add_1170": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_1169 (port_0)",
                "Data: layernorm_1157 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_1157": "Data",
                "nop_1169": "Data"
            },
            "input_nodes": [
                "nop_1169",
                "layernorm_1157"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_1170",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_1171 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_1171"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertOutput::output",
                "original_op_name": "add_1170",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 65
        },
        "add_1176": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1175 (port_0)",
                "Data: bert.encoder.layer.22.attention.self.query.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.22.attention.self.query.bias": "Data",
                "reshape_1175": "Data"
            },
            "input_nodes": [
                "reshape_1175",
                "bert.encoder.layer.22.attention.self.query.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1176",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_1177 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_1177"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_1176",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 264
        },
        "add_1182": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1181 (port_0)",
                "Data: bert.encoder.layer.22.attention.self.key.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.22.attention.self.key.bias": "Data",
                "reshape_1181": "Data"
            },
            "input_nodes": [
                "reshape_1181",
                "bert.encoder.layer.22.attention.self.key.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1182",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_1183 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_1183"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_1182",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 283
        },
        "add_1189": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: multiply_1188 (port_0)",
                "Data: multiply_22 (port_1)"
            ],
            "input_node_to_edge_type": {
                "multiply_1188": "Data",
                "multiply_22": "Data"
            },
            "input_nodes": [
                "multiply_1188",
                "multiply_22"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1189",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: softmax_1190 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "softmax_1190"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "add_1189",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 168
        },
        "add_1196": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1195 (port_0)",
                "Data: bert.encoder.layer.22.attention.self.value.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.22.attention.self.value.bias": "Data",
                "reshape_1195": "Data"
            },
            "input_nodes": [
                "reshape_1195",
                "bert.encoder.layer.22.attention.self.value.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1196",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_1197 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_1197"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_1196",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 183
        },
        "add_12": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_11 (port_0)",
                "Data: bert.encoder.layer.0.attention.self.key.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.0.attention.self.key.bias": "Data",
                "reshape_11": "Data"
            },
            "input_nodes": [
                "reshape_11",
                "bert.encoder.layer.0.attention.self.key.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_12",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_13 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_13"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_12",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1703
        },
        "add_1207": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1206 (port_0)",
                "Data: bert.encoder.layer.22.attention.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.22.attention.output.dense.bias": "Data",
                "reshape_1206": "Data"
            },
            "input_nodes": [
                "reshape_1206",
                "bert.encoder.layer.22.attention.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1207",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_1208 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_1208"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_1207",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 64
        },
        "add_1209": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_1208 (port_0)",
                "Data: layernorm_1171 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_1171": "Data",
                "nop_1208": "Data"
            },
            "input_nodes": [
                "nop_1208",
                "layernorm_1171"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_1209",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_1210 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_1210"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output",
                "original_op_name": "add_1209",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 49
        },
        "add_1215": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1214 (port_0)",
                "Data: bert.encoder.layer.22.intermediate.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.22.intermediate.dense.bias": "Data",
                "reshape_1214": "Data"
            },
            "input_nodes": [
                "reshape_1214",
                "bert.encoder.layer.22.intermediate.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1215",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: gelu_1216 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "gelu_1216"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_1215",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 90
        },
        "add_122": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_121 (port_0)",
                "Data: bert.encoder.layer.2.attention.self.key.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.2.attention.self.key.bias": "Data",
                "reshape_121": "Data"
            },
            "input_nodes": [
                "reshape_121",
                "bert.encoder.layer.2.attention.self.key.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_122",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_123 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_123"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_122",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1649
        },
        "add_1221": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1220 (port_0)",
                "Data: bert.encoder.layer.22.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.22.output.dense.bias": "Data",
                "reshape_1220": "Data"
            },
            "input_nodes": [
                "reshape_1220",
                "bert.encoder.layer.22.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1221",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_1222 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_1222"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_1221",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 48
        },
        "add_1223": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_1222 (port_0)",
                "Data: layernorm_1210 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_1210": "Data",
                "nop_1222": "Data"
            },
            "input_nodes": [
                "nop_1222",
                "layernorm_1210"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_1223",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_1224 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_1224"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertOutput::output",
                "original_op_name": "add_1223",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 36
        },
        "add_1229": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1228 (port_0)",
                "Data: bert.encoder.layer.23.attention.self.query.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.23.attention.self.query.bias": "Data",
                "reshape_1228": "Data"
            },
            "input_nodes": [
                "reshape_1228",
                "bert.encoder.layer.23.attention.self.query.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1229",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_1230 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_1230"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_1229",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 195
        },
        "add_1235": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1234 (port_0)",
                "Data: bert.encoder.layer.23.attention.self.key.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.23.attention.self.key.bias": "Data",
                "reshape_1234": "Data"
            },
            "input_nodes": [
                "reshape_1234",
                "bert.encoder.layer.23.attention.self.key.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1235",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_1236 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_1236"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_1235",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 213
        },
        "add_1242": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: multiply_1241 (port_0)",
                "Data: multiply_22 (port_1)"
            ],
            "input_node_to_edge_type": {
                "multiply_1241": "Data",
                "multiply_22": "Data"
            },
            "input_nodes": [
                "multiply_1241",
                "multiply_22"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1242",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: softmax_1243 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "softmax_1243"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "add_1242",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 108
        },
        "add_1249": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1248 (port_0)",
                "Data: bert.encoder.layer.23.attention.self.value.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.23.attention.self.value.bias": "Data",
                "reshape_1248": "Data"
            },
            "input_nodes": [
                "reshape_1248",
                "bert.encoder.layer.23.attention.self.value.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1249",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_1250 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_1250"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_1249",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 120
        },
        "add_1260": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1259 (port_0)",
                "Data: bert.encoder.layer.23.attention.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.23.attention.output.dense.bias": "Data",
                "reshape_1259": "Data"
            },
            "input_nodes": [
                "reshape_1259",
                "bert.encoder.layer.23.attention.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1260",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_1261 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_1261"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_1260",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 35
        },
        "add_1262": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_1261 (port_0)",
                "Data: layernorm_1224 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_1224": "Data",
                "nop_1261": "Data"
            },
            "input_nodes": [
                "nop_1261",
                "layernorm_1224"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_1262",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_1263 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_1263"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output",
                "original_op_name": "add_1262",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 27
        },
        "add_1268": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1267 (port_0)",
                "Data: bert.encoder.layer.23.intermediate.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.23.intermediate.dense.bias": "Data",
                "reshape_1267": "Data"
            },
            "input_nodes": [
                "reshape_1267",
                "bert.encoder.layer.23.intermediate.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1268",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: gelu_1269 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "gelu_1269"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_1268",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 52
        },
        "add_1274": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1273 (port_0)",
                "Data: bert.encoder.layer.23.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.23.output.dense.bias": "Data",
                "reshape_1273": "Data"
            },
            "input_nodes": [
                "reshape_1273",
                "bert.encoder.layer.23.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1274",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_1275 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_1275"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_1274",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 26
        },
        "add_1276": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_1275 (port_0)",
                "Data: layernorm_1263 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_1263": "Data",
                "nop_1275": "Data"
            },
            "input_nodes": [
                "nop_1275",
                "layernorm_1263"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_1276",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_1277 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_1277"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertOutput::output",
                "original_op_name": "add_1276",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 20
        },
        "add_1283": {
            "cache": {
                "shape": [
                    384,
                    1
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1281 (port_0)",
                "Data: qa_outputs.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "matmul_1281": "Data",
                "qa_outputs.bias": "Data"
            },
            "input_nodes": [
                "matmul_1281",
                "qa_outputs.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1283",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1284 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1284"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_1283",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 7
        },
        "add_129": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: multiply_128 (port_0)",
                "Data: multiply_22 (port_1)"
            ],
            "input_node_to_edge_type": {
                "multiply_128": "Data",
                "multiply_22": "Data"
            },
            "input_nodes": [
                "multiply_128",
                "multiply_22"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_129",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: softmax_130 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "softmax_130"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "add_129",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1562
        },
        "add_1290": {
            "cache": {
                "shape": [
                    384,
                    1
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1288 (port_0)",
                "Data: qa_outputs.bias_fork_clone12 (port_1)"
            ],
            "input_node_to_edge_type": {
                "matmul_1288": "Data",
                "qa_outputs.bias_fork_clone12": "Data"
            },
            "input_nodes": [
                "matmul_1288",
                "qa_outputs.bias_fork_clone12"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_1290",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1291 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1291"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_1290",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 8
        },
        "add_136": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_135 (port_0)",
                "Data: bert.encoder.layer.2.attention.self.value.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.2.attention.self.value.bias": "Data",
                "reshape_135": "Data"
            },
            "input_nodes": [
                "reshape_135",
                "bert.encoder.layer.2.attention.self.value.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_136",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_137 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_137"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_136",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1579
        },
        "add_147": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_146 (port_0)",
                "Data: bert.encoder.layer.2.attention.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.2.attention.output.dense.bias": "Data",
                "reshape_146": "Data"
            },
            "input_nodes": [
                "reshape_146",
                "bert.encoder.layer.2.attention.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_147",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_148 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_148"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_147",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1413
        },
        "add_149": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_148 (port_0)",
                "Data: layernorm_111 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_111": "Data",
                "nop_148": "Data"
            },
            "input_nodes": [
                "nop_148",
                "layernorm_111"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_149",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_150 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_150"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output",
                "original_op_name": "add_149",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1379
        },
        "add_155": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_154 (port_0)",
                "Data: bert.encoder.layer.2.intermediate.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.2.intermediate.dense.bias": "Data",
                "reshape_154": "Data"
            },
            "input_nodes": [
                "reshape_154",
                "bert.encoder.layer.2.intermediate.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_155",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: gelu_156 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "gelu_156"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_155",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1463
        },
        "add_161": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_160 (port_0)",
                "Data: bert.encoder.layer.2.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.2.output.dense.bias": "Data",
                "reshape_160": "Data"
            },
            "input_nodes": [
                "reshape_160",
                "bert.encoder.layer.2.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_161",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_162 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_162"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_161",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1378
        },
        "add_163": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_162 (port_0)",
                "Data: layernorm_150 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_150": "Data",
                "nop_162": "Data"
            },
            "input_nodes": [
                "nop_162",
                "layernorm_150"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_163",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_164 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_164"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertOutput::output",
                "original_op_name": "add_163",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1344
        },
        "add_169": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_168 (port_0)",
                "Data: bert.encoder.layer.3.attention.self.query.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.3.attention.self.query.bias": "Data",
                "reshape_168": "Data"
            },
            "input_nodes": [
                "reshape_168",
                "bert.encoder.layer.3.attention.self.query.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_169",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_170 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_170"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_169",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1589
        },
        "add_175": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_174 (port_0)",
                "Data: bert.encoder.layer.3.attention.self.key.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.3.attention.self.key.bias": "Data",
                "reshape_174": "Data"
            },
            "input_nodes": [
                "reshape_174",
                "bert.encoder.layer.3.attention.self.key.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_175",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_176 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_176"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_175",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1604
        },
        "add_182": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: multiply_181 (port_0)",
                "Data: multiply_22 (port_1)"
            ],
            "input_node_to_edge_type": {
                "multiply_181": "Data",
                "multiply_22": "Data"
            },
            "input_nodes": [
                "multiply_181",
                "multiply_22"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_182",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: softmax_183 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "softmax_183"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "add_182",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1494
        },
        "add_189": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_188 (port_0)",
                "Data: bert.encoder.layer.3.attention.self.value.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.3.attention.self.value.bias": "Data",
                "reshape_188": "Data"
            },
            "input_nodes": [
                "reshape_188",
                "bert.encoder.layer.3.attention.self.value.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_189",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_190 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_190"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_189",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1510
        },
        "add_200": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_199 (port_0)",
                "Data: bert.encoder.layer.3.attention.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.3.attention.output.dense.bias": "Data",
                "reshape_199": "Data"
            },
            "input_nodes": [
                "reshape_199",
                "bert.encoder.layer.3.attention.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_200",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_201 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_201"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_200",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1343
        },
        "add_202": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_201 (port_0)",
                "Data: layernorm_164 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_164": "Data",
                "nop_201": "Data"
            },
            "input_nodes": [
                "nop_201",
                "layernorm_164"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_202",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_203 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_203"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output",
                "original_op_name": "add_202",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1309
        },
        "add_208": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_207 (port_0)",
                "Data: bert.encoder.layer.3.intermediate.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.3.intermediate.dense.bias": "Data",
                "reshape_207": "Data"
            },
            "input_nodes": [
                "reshape_207",
                "bert.encoder.layer.3.intermediate.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_208",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: gelu_209 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "gelu_209"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_208",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1393
        },
        "add_214": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_213 (port_0)",
                "Data: bert.encoder.layer.3.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.3.output.dense.bias": "Data",
                "reshape_213": "Data"
            },
            "input_nodes": [
                "reshape_213",
                "bert.encoder.layer.3.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_214",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_215 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_215"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_214",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1308
        },
        "add_216": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_215 (port_0)",
                "Data: layernorm_203 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_203": "Data",
                "nop_215": "Data"
            },
            "input_nodes": [
                "nop_215",
                "layernorm_203"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_216",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_217 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_217"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertOutput::output",
                "original_op_name": "add_216",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1274
        },
        "add_222": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_221 (port_0)",
                "Data: bert.encoder.layer.4.attention.self.query.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.4.attention.self.query.bias": "Data",
                "reshape_221": "Data"
            },
            "input_nodes": [
                "reshape_221",
                "bert.encoder.layer.4.attention.self.query.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_222",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_223 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_223"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_222",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1524
        },
        "add_228": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_227 (port_0)",
                "Data: bert.encoder.layer.4.attention.self.key.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.4.attention.self.key.bias": "Data",
                "reshape_227": "Data"
            },
            "input_nodes": [
                "reshape_227",
                "bert.encoder.layer.4.attention.self.key.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_228",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_229 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_229"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_228",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1543
        },
        "add_23": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: multiply_18 (port_0)",
                "Data: multiply_22 (port_1)"
            ],
            "input_node_to_edge_type": {
                "multiply_18": "Data",
                "multiply_22": "Data"
            },
            "input_nodes": [
                "multiply_18",
                "multiply_22"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_23",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: softmax_24 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "softmax_24"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "add_23",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1661
        },
        "add_235": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: multiply_234 (port_0)",
                "Data: multiply_22 (port_1)"
            ],
            "input_node_to_edge_type": {
                "multiply_22": "Data",
                "multiply_234": "Data"
            },
            "input_nodes": [
                "multiply_234",
                "multiply_22"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_235",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: softmax_236 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "softmax_236"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "add_235",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1424
        },
        "add_242": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_241 (port_0)",
                "Data: bert.encoder.layer.4.attention.self.value.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.4.attention.self.value.bias": "Data",
                "reshape_241": "Data"
            },
            "input_nodes": [
                "reshape_241",
                "bert.encoder.layer.4.attention.self.value.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_242",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_243 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_243"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_242",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1440
        },
        "add_253": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_252 (port_0)",
                "Data: bert.encoder.layer.4.attention.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.4.attention.output.dense.bias": "Data",
                "reshape_252": "Data"
            },
            "input_nodes": [
                "reshape_252",
                "bert.encoder.layer.4.attention.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_253",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_254 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_254"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_253",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1273
        },
        "add_255": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_254 (port_0)",
                "Data: layernorm_217 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_217": "Data",
                "nop_254": "Data"
            },
            "input_nodes": [
                "nop_254",
                "layernorm_217"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_255",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_256 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_256"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output",
                "original_op_name": "add_255",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1239
        },
        "add_261": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_260 (port_0)",
                "Data: bert.encoder.layer.4.intermediate.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.4.intermediate.dense.bias": "Data",
                "reshape_260": "Data"
            },
            "input_nodes": [
                "reshape_260",
                "bert.encoder.layer.4.intermediate.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_261",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: gelu_262 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "gelu_262"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_261",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1323
        },
        "add_267": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_266 (port_0)",
                "Data: bert.encoder.layer.4.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.4.output.dense.bias": "Data",
                "reshape_266": "Data"
            },
            "input_nodes": [
                "reshape_266",
                "bert.encoder.layer.4.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_267",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_268 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_268"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_267",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1238
        },
        "add_269": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_268 (port_0)",
                "Data: layernorm_256 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_256": "Data",
                "nop_268": "Data"
            },
            "input_nodes": [
                "nop_268",
                "layernorm_256"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_269",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_270 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_270"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertOutput::output",
                "original_op_name": "add_269",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1204
        },
        "add_275": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_274 (port_0)",
                "Data: bert.encoder.layer.5.attention.self.query.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.5.attention.self.query.bias": "Data",
                "reshape_274": "Data"
            },
            "input_nodes": [
                "reshape_274",
                "bert.encoder.layer.5.attention.self.query.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_275",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_276 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_276"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_275",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1454
        },
        "add_281": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_280 (port_0)",
                "Data: bert.encoder.layer.5.attention.self.key.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.5.attention.self.key.bias": "Data",
                "reshape_280": "Data"
            },
            "input_nodes": [
                "reshape_280",
                "bert.encoder.layer.5.attention.self.key.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_281",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_282 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_282"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_281",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1473
        },
        "add_288": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: multiply_287 (port_0)",
                "Data: multiply_22 (port_1)"
            ],
            "input_node_to_edge_type": {
                "multiply_22": "Data",
                "multiply_287": "Data"
            },
            "input_nodes": [
                "multiply_287",
                "multiply_22"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_288",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: softmax_289 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "softmax_289"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "add_288",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1354
        },
        "add_295": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_294 (port_0)",
                "Data: bert.encoder.layer.5.attention.self.value.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.5.attention.self.value.bias": "Data",
                "reshape_294": "Data"
            },
            "input_nodes": [
                "reshape_294",
                "bert.encoder.layer.5.attention.self.value.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_295",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_296 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_296"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_295",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1370
        },
        "add_30": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_29 (port_0)",
                "Data: bert.encoder.layer.0.attention.self.value.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.0.attention.self.value.bias": "Data",
                "reshape_29": "Data"
            },
            "input_nodes": [
                "reshape_29",
                "bert.encoder.layer.0.attention.self.value.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_30",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_31 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_31"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_30",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1669
        },
        "add_306": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_305 (port_0)",
                "Data: bert.encoder.layer.5.attention.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.5.attention.output.dense.bias": "Data",
                "reshape_305": "Data"
            },
            "input_nodes": [
                "reshape_305",
                "bert.encoder.layer.5.attention.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_306",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_307 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_307"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_306",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1203
        },
        "add_308": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_307 (port_0)",
                "Data: layernorm_270 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_270": "Data",
                "nop_307": "Data"
            },
            "input_nodes": [
                "nop_307",
                "layernorm_270"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_308",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_309 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_309"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output",
                "original_op_name": "add_308",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1169
        },
        "add_314": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_313 (port_0)",
                "Data: bert.encoder.layer.5.intermediate.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.5.intermediate.dense.bias": "Data",
                "reshape_313": "Data"
            },
            "input_nodes": [
                "reshape_313",
                "bert.encoder.layer.5.intermediate.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_314",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: gelu_315 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "gelu_315"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_314",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1253
        },
        "add_320": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_319 (port_0)",
                "Data: bert.encoder.layer.5.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.5.output.dense.bias": "Data",
                "reshape_319": "Data"
            },
            "input_nodes": [
                "reshape_319",
                "bert.encoder.layer.5.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_320",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_321 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_321"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_320",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1168
        },
        "add_322": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_321 (port_0)",
                "Data: layernorm_309 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_309": "Data",
                "nop_321": "Data"
            },
            "input_nodes": [
                "nop_321",
                "layernorm_309"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_322",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_323 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_323"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertOutput::output",
                "original_op_name": "add_322",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1134
        },
        "add_328": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_327 (port_0)",
                "Data: bert.encoder.layer.6.attention.self.query.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.6.attention.self.query.bias": "Data",
                "reshape_327": "Data"
            },
            "input_nodes": [
                "reshape_327",
                "bert.encoder.layer.6.attention.self.query.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_328",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_329 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_329"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_328",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1384
        },
        "add_334": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_333 (port_0)",
                "Data: bert.encoder.layer.6.attention.self.key.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.6.attention.self.key.bias": "Data",
                "reshape_333": "Data"
            },
            "input_nodes": [
                "reshape_333",
                "bert.encoder.layer.6.attention.self.key.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_334",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_335 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_335"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_334",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1403
        },
        "add_341": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: multiply_340 (port_0)",
                "Data: multiply_22 (port_1)"
            ],
            "input_node_to_edge_type": {
                "multiply_22": "Data",
                "multiply_340": "Data"
            },
            "input_nodes": [
                "multiply_340",
                "multiply_22"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_341",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: softmax_342 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "softmax_342"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "add_341",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1284
        },
        "add_348": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_347 (port_0)",
                "Data: bert.encoder.layer.6.attention.self.value.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.6.attention.self.value.bias": "Data",
                "reshape_347": "Data"
            },
            "input_nodes": [
                "reshape_347",
                "bert.encoder.layer.6.attention.self.value.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_348",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_349 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_349"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_348",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1300
        },
        "add_359": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_358 (port_0)",
                "Data: bert.encoder.layer.6.attention.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.6.attention.output.dense.bias": "Data",
                "reshape_358": "Data"
            },
            "input_nodes": [
                "reshape_358",
                "bert.encoder.layer.6.attention.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_359",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_360 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_360"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_359",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1133
        },
        "add_361": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_360 (port_0)",
                "Data: layernorm_323 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_323": "Data",
                "nop_360": "Data"
            },
            "input_nodes": [
                "nop_360",
                "layernorm_323"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_361",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_362 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_362"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output",
                "original_op_name": "add_361",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1099
        },
        "add_367": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_366 (port_0)",
                "Data: bert.encoder.layer.6.intermediate.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.6.intermediate.dense.bias": "Data",
                "reshape_366": "Data"
            },
            "input_nodes": [
                "reshape_366",
                "bert.encoder.layer.6.intermediate.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_367",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: gelu_368 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "gelu_368"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_367",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1183
        },
        "add_373": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_372 (port_0)",
                "Data: bert.encoder.layer.6.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.6.output.dense.bias": "Data",
                "reshape_372": "Data"
            },
            "input_nodes": [
                "reshape_372",
                "bert.encoder.layer.6.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_373",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_374 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_374"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_373",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1098
        },
        "add_375": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_374 (port_0)",
                "Data: layernorm_362 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_362": "Data",
                "nop_374": "Data"
            },
            "input_nodes": [
                "nop_374",
                "layernorm_362"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_375",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_376 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_376"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertOutput::output",
                "original_op_name": "add_375",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1064
        },
        "add_381": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_380 (port_0)",
                "Data: bert.encoder.layer.7.attention.self.query.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.7.attention.self.query.bias": "Data",
                "reshape_380": "Data"
            },
            "input_nodes": [
                "reshape_380",
                "bert.encoder.layer.7.attention.self.query.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_381",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_382 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_382"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_381",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1314
        },
        "add_387": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_386 (port_0)",
                "Data: bert.encoder.layer.7.attention.self.key.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.7.attention.self.key.bias": "Data",
                "reshape_386": "Data"
            },
            "input_nodes": [
                "reshape_386",
                "bert.encoder.layer.7.attention.self.key.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_387",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_388 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_388"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_387",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1333
        },
        "add_394": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: multiply_393 (port_0)",
                "Data: multiply_22 (port_1)"
            ],
            "input_node_to_edge_type": {
                "multiply_22": "Data",
                "multiply_393": "Data"
            },
            "input_nodes": [
                "multiply_393",
                "multiply_22"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_394",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: softmax_395 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "softmax_395"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "add_394",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1214
        },
        "add_401": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_400 (port_0)",
                "Data: bert.encoder.layer.7.attention.self.value.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.7.attention.self.value.bias": "Data",
                "reshape_400": "Data"
            },
            "input_nodes": [
                "reshape_400",
                "bert.encoder.layer.7.attention.self.value.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_401",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_402 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_402"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_401",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1230
        },
        "add_41": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_40 (port_0)",
                "Data: bert.encoder.layer.0.attention.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.0.attention.output.dense.bias": "Data",
                "reshape_40": "Data"
            },
            "input_nodes": [
                "reshape_40",
                "bert.encoder.layer.0.attention.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_41",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_42 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_42"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_41",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1553
        },
        "add_412": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_411 (port_0)",
                "Data: bert.encoder.layer.7.attention.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.7.attention.output.dense.bias": "Data",
                "reshape_411": "Data"
            },
            "input_nodes": [
                "reshape_411",
                "bert.encoder.layer.7.attention.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_412",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_413 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_413"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_412",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1063
        },
        "add_414": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_413 (port_0)",
                "Data: layernorm_376 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_376": "Data",
                "nop_413": "Data"
            },
            "input_nodes": [
                "nop_413",
                "layernorm_376"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_414",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_415 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_415"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output",
                "original_op_name": "add_414",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1029
        },
        "add_420": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_419 (port_0)",
                "Data: bert.encoder.layer.7.intermediate.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.7.intermediate.dense.bias": "Data",
                "reshape_419": "Data"
            },
            "input_nodes": [
                "reshape_419",
                "bert.encoder.layer.7.intermediate.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_420",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: gelu_421 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "gelu_421"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_420",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1113
        },
        "add_426": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_425 (port_0)",
                "Data: bert.encoder.layer.7.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.7.output.dense.bias": "Data",
                "reshape_425": "Data"
            },
            "input_nodes": [
                "reshape_425",
                "bert.encoder.layer.7.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_426",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_427 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_427"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_426",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1028
        },
        "add_428": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_427 (port_0)",
                "Data: layernorm_415 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_415": "Data",
                "nop_427": "Data"
            },
            "input_nodes": [
                "nop_427",
                "layernorm_415"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_428",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_429 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_429"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertOutput::output",
                "original_op_name": "add_428",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 994
        },
        "add_43": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_42 (port_0)",
                "Data: nop_1 (port_1)"
            ],
            "input_node_to_edge_type": {
                "nop_1": "Data",
                "nop_42": "Data"
            },
            "input_nodes": [
                "nop_42",
                "nop_1"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_43",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_44 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_44"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output",
                "original_op_name": "add_43",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1519
        },
        "add_434": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_433 (port_0)",
                "Data: bert.encoder.layer.8.attention.self.query.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.8.attention.self.query.bias": "Data",
                "reshape_433": "Data"
            },
            "input_nodes": [
                "reshape_433",
                "bert.encoder.layer.8.attention.self.query.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_434",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_435 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_435"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_434",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1244
        },
        "add_440": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_439 (port_0)",
                "Data: bert.encoder.layer.8.attention.self.key.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.8.attention.self.key.bias": "Data",
                "reshape_439": "Data"
            },
            "input_nodes": [
                "reshape_439",
                "bert.encoder.layer.8.attention.self.key.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_440",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_441 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_441"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_440",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1263
        },
        "add_447": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: multiply_446 (port_0)",
                "Data: multiply_22 (port_1)"
            ],
            "input_node_to_edge_type": {
                "multiply_22": "Data",
                "multiply_446": "Data"
            },
            "input_nodes": [
                "multiply_446",
                "multiply_22"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_447",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: softmax_448 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "softmax_448"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "add_447",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1144
        },
        "add_454": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_453 (port_0)",
                "Data: bert.encoder.layer.8.attention.self.value.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.8.attention.self.value.bias": "Data",
                "reshape_453": "Data"
            },
            "input_nodes": [
                "reshape_453",
                "bert.encoder.layer.8.attention.self.value.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_454",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_455 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_455"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_454",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1160
        },
        "add_465": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_464 (port_0)",
                "Data: bert.encoder.layer.8.attention.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.8.attention.output.dense.bias": "Data",
                "reshape_464": "Data"
            },
            "input_nodes": [
                "reshape_464",
                "bert.encoder.layer.8.attention.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_465",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_466 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_466"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_465",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 993
        },
        "add_467": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_466 (port_0)",
                "Data: layernorm_429 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_429": "Data",
                "nop_466": "Data"
            },
            "input_nodes": [
                "nop_466",
                "layernorm_429"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_467",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_468 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_468"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output",
                "original_op_name": "add_467",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 959
        },
        "add_473": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_472 (port_0)",
                "Data: bert.encoder.layer.8.intermediate.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.8.intermediate.dense.bias": "Data",
                "reshape_472": "Data"
            },
            "input_nodes": [
                "reshape_472",
                "bert.encoder.layer.8.intermediate.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_473",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: gelu_474 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "gelu_474"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_473",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1043
        },
        "add_479": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_478 (port_0)",
                "Data: bert.encoder.layer.8.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.8.output.dense.bias": "Data",
                "reshape_478": "Data"
            },
            "input_nodes": [
                "reshape_478",
                "bert.encoder.layer.8.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_479",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_480 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_480"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_479",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 958
        },
        "add_481": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_480 (port_0)",
                "Data: layernorm_468 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_468": "Data",
                "nop_480": "Data"
            },
            "input_nodes": [
                "nop_480",
                "layernorm_468"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_481",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_482 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_482"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertOutput::output",
                "original_op_name": "add_481",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 924
        },
        "add_487": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_486 (port_0)",
                "Data: bert.encoder.layer.9.attention.self.query.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.9.attention.self.query.bias": "Data",
                "reshape_486": "Data"
            },
            "input_nodes": [
                "reshape_486",
                "bert.encoder.layer.9.attention.self.query.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_487",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_488 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_488"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_487",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1174
        },
        "add_49": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_48 (port_0)",
                "Data: bert.encoder.layer.0.intermediate.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.0.intermediate.dense.bias": "Data",
                "reshape_48": "Data"
            },
            "input_nodes": [
                "reshape_48",
                "bert.encoder.layer.0.intermediate.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_49",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: gelu_50 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "gelu_50"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_49",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1598
        },
        "add_493": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_492 (port_0)",
                "Data: bert.encoder.layer.9.attention.self.key.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.9.attention.self.key.bias": "Data",
                "reshape_492": "Data"
            },
            "input_nodes": [
                "reshape_492",
                "bert.encoder.layer.9.attention.self.key.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_493",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_494 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_494"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_493",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1193
        },
        "add_500": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: multiply_499 (port_0)",
                "Data: multiply_22 (port_1)"
            ],
            "input_node_to_edge_type": {
                "multiply_22": "Data",
                "multiply_499": "Data"
            },
            "input_nodes": [
                "multiply_499",
                "multiply_22"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_500",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: softmax_501 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "softmax_501"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "add_500",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1074
        },
        "add_507": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_506 (port_0)",
                "Data: bert.encoder.layer.9.attention.self.value.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.9.attention.self.value.bias": "Data",
                "reshape_506": "Data"
            },
            "input_nodes": [
                "reshape_506",
                "bert.encoder.layer.9.attention.self.value.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_507",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_508 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_508"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_507",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1090
        },
        "add_518": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_517 (port_0)",
                "Data: bert.encoder.layer.9.attention.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.9.attention.output.dense.bias": "Data",
                "reshape_517": "Data"
            },
            "input_nodes": [
                "reshape_517",
                "bert.encoder.layer.9.attention.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_518",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_519 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_519"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_518",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 923
        },
        "add_520": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_519 (port_0)",
                "Data: layernorm_482 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_482": "Data",
                "nop_519": "Data"
            },
            "input_nodes": [
                "nop_519",
                "layernorm_482"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_520",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_521 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_521"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output",
                "original_op_name": "add_520",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 889
        },
        "add_526": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_525 (port_0)",
                "Data: bert.encoder.layer.9.intermediate.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.9.intermediate.dense.bias": "Data",
                "reshape_525": "Data"
            },
            "input_nodes": [
                "reshape_525",
                "bert.encoder.layer.9.intermediate.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_526",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: gelu_527 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "gelu_527"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_526",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 973
        },
        "add_532": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_531 (port_0)",
                "Data: bert.encoder.layer.9.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.9.output.dense.bias": "Data",
                "reshape_531": "Data"
            },
            "input_nodes": [
                "reshape_531",
                "bert.encoder.layer.9.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_532",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_533 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_533"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_532",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 888
        },
        "add_534": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_533 (port_0)",
                "Data: layernorm_521 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_521": "Data",
                "nop_533": "Data"
            },
            "input_nodes": [
                "nop_533",
                "layernorm_521"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_534",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_535 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_535"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertOutput::output",
                "original_op_name": "add_534",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 854
        },
        "add_540": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_539 (port_0)",
                "Data: bert.encoder.layer.10.attention.self.query.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.10.attention.self.query.bias": "Data",
                "reshape_539": "Data"
            },
            "input_nodes": [
                "reshape_539",
                "bert.encoder.layer.10.attention.self.query.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_540",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_541 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_541"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_540",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1104
        },
        "add_546": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_545 (port_0)",
                "Data: bert.encoder.layer.10.attention.self.key.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.10.attention.self.key.bias": "Data",
                "reshape_545": "Data"
            },
            "input_nodes": [
                "reshape_545",
                "bert.encoder.layer.10.attention.self.key.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_546",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_547 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_547"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_546",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1123
        },
        "add_55": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_54 (port_0)",
                "Data: bert.encoder.layer.0.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.0.output.dense.bias": "Data",
                "reshape_54": "Data"
            },
            "input_nodes": [
                "reshape_54",
                "bert.encoder.layer.0.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_55",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_56 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_56"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_55",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1518
        },
        "add_553": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: multiply_552 (port_0)",
                "Data: multiply_22 (port_1)"
            ],
            "input_node_to_edge_type": {
                "multiply_22": "Data",
                "multiply_552": "Data"
            },
            "input_nodes": [
                "multiply_552",
                "multiply_22"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_553",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: softmax_554 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "softmax_554"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "add_553",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1004
        },
        "add_560": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_559 (port_0)",
                "Data: bert.encoder.layer.10.attention.self.value.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.10.attention.self.value.bias": "Data",
                "reshape_559": "Data"
            },
            "input_nodes": [
                "reshape_559",
                "bert.encoder.layer.10.attention.self.value.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_560",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_561 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_561"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_560",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1020
        },
        "add_57": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_56 (port_0)",
                "Data: layernorm_44 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_44": "Data",
                "nop_56": "Data"
            },
            "input_nodes": [
                "nop_56",
                "layernorm_44"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_57",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_58 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_58"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertOutput::output",
                "original_op_name": "add_57",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1484
        },
        "add_571": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_570 (port_0)",
                "Data: bert.encoder.layer.10.attention.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.10.attention.output.dense.bias": "Data",
                "reshape_570": "Data"
            },
            "input_nodes": [
                "reshape_570",
                "bert.encoder.layer.10.attention.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_571",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_572 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_572"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_571",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 853
        },
        "add_573": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_572 (port_0)",
                "Data: layernorm_535 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_535": "Data",
                "nop_572": "Data"
            },
            "input_nodes": [
                "nop_572",
                "layernorm_535"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_573",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_574 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_574"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output",
                "original_op_name": "add_573",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 819
        },
        "add_579": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_578 (port_0)",
                "Data: bert.encoder.layer.10.intermediate.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.10.intermediate.dense.bias": "Data",
                "reshape_578": "Data"
            },
            "input_nodes": [
                "reshape_578",
                "bert.encoder.layer.10.intermediate.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_579",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: gelu_580 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "gelu_580"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_579",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 903
        },
        "add_585": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_584 (port_0)",
                "Data: bert.encoder.layer.10.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.10.output.dense.bias": "Data",
                "reshape_584": "Data"
            },
            "input_nodes": [
                "reshape_584",
                "bert.encoder.layer.10.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_585",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_586 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_586"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_585",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 818
        },
        "add_587": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_586 (port_0)",
                "Data: layernorm_574 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_574": "Data",
                "nop_586": "Data"
            },
            "input_nodes": [
                "nop_586",
                "layernorm_574"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_587",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_588 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_588"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertOutput::output",
                "original_op_name": "add_587",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 784
        },
        "add_593": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_592 (port_0)",
                "Data: bert.encoder.layer.11.attention.self.query.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.11.attention.self.query.bias": "Data",
                "reshape_592": "Data"
            },
            "input_nodes": [
                "reshape_592",
                "bert.encoder.layer.11.attention.self.query.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_593",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_594 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_594"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_593",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1034
        },
        "add_599": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_598 (port_0)",
                "Data: bert.encoder.layer.11.attention.self.key.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.11.attention.self.key.bias": "Data",
                "reshape_598": "Data"
            },
            "input_nodes": [
                "reshape_598",
                "bert.encoder.layer.11.attention.self.key.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_599",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_600 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_600"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_599",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1053
        },
        "add_6": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_5 (port_0)",
                "Data: bert.encoder.layer.0.attention.self.query.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.0.attention.self.query.bias": "Data",
                "reshape_5": "Data"
            },
            "input_nodes": [
                "reshape_5",
                "bert.encoder.layer.0.attention.self.query.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_6",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_7 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_7"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_6",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1698
        },
        "add_606": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: multiply_605 (port_0)",
                "Data: multiply_22 (port_1)"
            ],
            "input_node_to_edge_type": {
                "multiply_22": "Data",
                "multiply_605": "Data"
            },
            "input_nodes": [
                "multiply_605",
                "multiply_22"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_606",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: softmax_607 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "softmax_607"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "add_606",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 934
        },
        "add_613": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_612 (port_0)",
                "Data: bert.encoder.layer.11.attention.self.value.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.11.attention.self.value.bias": "Data",
                "reshape_612": "Data"
            },
            "input_nodes": [
                "reshape_612",
                "bert.encoder.layer.11.attention.self.value.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_613",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_614 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_614"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_613",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 950
        },
        "add_624": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_623 (port_0)",
                "Data: bert.encoder.layer.11.attention.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.11.attention.output.dense.bias": "Data",
                "reshape_623": "Data"
            },
            "input_nodes": [
                "reshape_623",
                "bert.encoder.layer.11.attention.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_624",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_625 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_625"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_624",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 783
        },
        "add_626": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_625 (port_0)",
                "Data: layernorm_588 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_588": "Data",
                "nop_625": "Data"
            },
            "input_nodes": [
                "nop_625",
                "layernorm_588"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_626",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_627 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_627"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output",
                "original_op_name": "add_626",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 749
        },
        "add_63": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_62 (port_0)",
                "Data: bert.encoder.layer.1.attention.self.query.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.1.attention.self.query.bias": "Data",
                "reshape_62": "Data"
            },
            "input_nodes": [
                "reshape_62",
                "bert.encoder.layer.1.attention.self.query.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_63",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_64 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_64"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_63",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1672
        },
        "add_632": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_631 (port_0)",
                "Data: bert.encoder.layer.11.intermediate.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.11.intermediate.dense.bias": "Data",
                "reshape_631": "Data"
            },
            "input_nodes": [
                "reshape_631",
                "bert.encoder.layer.11.intermediate.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_632",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: gelu_633 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "gelu_633"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_632",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 833
        },
        "add_638": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_637 (port_0)",
                "Data: bert.encoder.layer.11.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.11.output.dense.bias": "Data",
                "reshape_637": "Data"
            },
            "input_nodes": [
                "reshape_637",
                "bert.encoder.layer.11.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_638",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_639 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_639"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_638",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 748
        },
        "add_640": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_639 (port_0)",
                "Data: layernorm_627 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_627": "Data",
                "nop_639": "Data"
            },
            "input_nodes": [
                "nop_639",
                "layernorm_627"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_640",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_641 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_641"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertOutput::output",
                "original_op_name": "add_640",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 714
        },
        "add_646": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_645 (port_0)",
                "Data: bert.encoder.layer.12.attention.self.query.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.12.attention.self.query.bias": "Data",
                "reshape_645": "Data"
            },
            "input_nodes": [
                "reshape_645",
                "bert.encoder.layer.12.attention.self.query.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_646",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_647 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_647"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_646",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 964
        },
        "add_652": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_651 (port_0)",
                "Data: bert.encoder.layer.12.attention.self.key.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.12.attention.self.key.bias": "Data",
                "reshape_651": "Data"
            },
            "input_nodes": [
                "reshape_651",
                "bert.encoder.layer.12.attention.self.key.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_652",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_653 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_653"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_652",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 983
        },
        "add_659": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: multiply_658 (port_0)",
                "Data: multiply_22 (port_1)"
            ],
            "input_node_to_edge_type": {
                "multiply_22": "Data",
                "multiply_658": "Data"
            },
            "input_nodes": [
                "multiply_658",
                "multiply_22"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_659",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: softmax_660 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "softmax_660"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "add_659",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 864
        },
        "add_666": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_665 (port_0)",
                "Data: bert.encoder.layer.12.attention.self.value.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.12.attention.self.value.bias": "Data",
                "reshape_665": "Data"
            },
            "input_nodes": [
                "reshape_665",
                "bert.encoder.layer.12.attention.self.value.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_666",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_667 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_667"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_666",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 880
        },
        "add_677": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_676 (port_0)",
                "Data: bert.encoder.layer.12.attention.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.12.attention.output.dense.bias": "Data",
                "reshape_676": "Data"
            },
            "input_nodes": [
                "reshape_676",
                "bert.encoder.layer.12.attention.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_677",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_678 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_678"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_677",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 713
        },
        "add_679": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_678 (port_0)",
                "Data: layernorm_641 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_641": "Data",
                "nop_678": "Data"
            },
            "input_nodes": [
                "nop_678",
                "layernorm_641"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_679",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_680 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_680"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output",
                "original_op_name": "add_679",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 679
        },
        "add_685": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_684 (port_0)",
                "Data: bert.encoder.layer.12.intermediate.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.12.intermediate.dense.bias": "Data",
                "reshape_684": "Data"
            },
            "input_nodes": [
                "reshape_684",
                "bert.encoder.layer.12.intermediate.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_685",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: gelu_686 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "gelu_686"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_685",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 763
        },
        "add_69": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_68 (port_0)",
                "Data: bert.encoder.layer.1.attention.self.key.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.1.attention.self.key.bias": "Data",
                "reshape_68": "Data"
            },
            "input_nodes": [
                "reshape_68",
                "bert.encoder.layer.1.attention.self.key.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_69",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_70 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_70"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_69",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1681
        },
        "add_691": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_690 (port_0)",
                "Data: bert.encoder.layer.12.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.12.output.dense.bias": "Data",
                "reshape_690": "Data"
            },
            "input_nodes": [
                "reshape_690",
                "bert.encoder.layer.12.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_691",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_692 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_692"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_691",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 678
        },
        "add_693": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_692 (port_0)",
                "Data: layernorm_680 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_680": "Data",
                "nop_692": "Data"
            },
            "input_nodes": [
                "nop_692",
                "layernorm_680"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_693",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_694 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_694"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertOutput::output",
                "original_op_name": "add_693",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 644
        },
        "add_699": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_698 (port_0)",
                "Data: bert.encoder.layer.13.attention.self.query.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.13.attention.self.query.bias": "Data",
                "reshape_698": "Data"
            },
            "input_nodes": [
                "reshape_698",
                "bert.encoder.layer.13.attention.self.query.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_699",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_700 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_700"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_699",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 894
        },
        "add_705": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_704 (port_0)",
                "Data: bert.encoder.layer.13.attention.self.key.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.13.attention.self.key.bias": "Data",
                "reshape_704": "Data"
            },
            "input_nodes": [
                "reshape_704",
                "bert.encoder.layer.13.attention.self.key.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_705",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_706 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_706"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_705",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 913
        },
        "add_712": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: multiply_711 (port_0)",
                "Data: multiply_22 (port_1)"
            ],
            "input_node_to_edge_type": {
                "multiply_22": "Data",
                "multiply_711": "Data"
            },
            "input_nodes": [
                "multiply_711",
                "multiply_22"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_712",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: softmax_713 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "softmax_713"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "add_712",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 794
        },
        "add_719": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_718 (port_0)",
                "Data: bert.encoder.layer.13.attention.self.value.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.13.attention.self.value.bias": "Data",
                "reshape_718": "Data"
            },
            "input_nodes": [
                "reshape_718",
                "bert.encoder.layer.13.attention.self.value.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_719",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_720 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_720"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_719",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 810
        },
        "add_730": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_729 (port_0)",
                "Data: bert.encoder.layer.13.attention.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.13.attention.output.dense.bias": "Data",
                "reshape_729": "Data"
            },
            "input_nodes": [
                "reshape_729",
                "bert.encoder.layer.13.attention.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_730",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_731 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_731"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_730",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 643
        },
        "add_732": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_731 (port_0)",
                "Data: layernorm_694 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_694": "Data",
                "nop_731": "Data"
            },
            "input_nodes": [
                "nop_731",
                "layernorm_694"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_732",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_733 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_733"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output",
                "original_op_name": "add_732",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 609
        },
        "add_738": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_737 (port_0)",
                "Data: bert.encoder.layer.13.intermediate.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.13.intermediate.dense.bias": "Data",
                "reshape_737": "Data"
            },
            "input_nodes": [
                "reshape_737",
                "bert.encoder.layer.13.intermediate.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_738",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: gelu_739 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "gelu_739"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_738",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 693
        },
        "add_744": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_743 (port_0)",
                "Data: bert.encoder.layer.13.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.13.output.dense.bias": "Data",
                "reshape_743": "Data"
            },
            "input_nodes": [
                "reshape_743",
                "bert.encoder.layer.13.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_744",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_745 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_745"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_744",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 608
        },
        "add_746": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_745 (port_0)",
                "Data: layernorm_733 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_733": "Data",
                "nop_745": "Data"
            },
            "input_nodes": [
                "nop_745",
                "layernorm_733"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_746",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_747 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_747"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertOutput::output",
                "original_op_name": "add_746",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 574
        },
        "add_752": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_751 (port_0)",
                "Data: bert.encoder.layer.14.attention.self.query.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.14.attention.self.query.bias": "Data",
                "reshape_751": "Data"
            },
            "input_nodes": [
                "reshape_751",
                "bert.encoder.layer.14.attention.self.query.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_752",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_753 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_753"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_752",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 824
        },
        "add_758": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_757 (port_0)",
                "Data: bert.encoder.layer.14.attention.self.key.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.14.attention.self.key.bias": "Data",
                "reshape_757": "Data"
            },
            "input_nodes": [
                "reshape_757",
                "bert.encoder.layer.14.attention.self.key.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_758",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_759 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_759"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_758",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 843
        },
        "add_76": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: multiply_75 (port_0)",
                "Data: multiply_22 (port_1)"
            ],
            "input_node_to_edge_type": {
                "multiply_22": "Data",
                "multiply_75": "Data"
            },
            "input_nodes": [
                "multiply_75",
                "multiply_22"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_76",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: softmax_77 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "softmax_77"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "add_76",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1620
        },
        "add_765": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: multiply_764 (port_0)",
                "Data: multiply_22 (port_1)"
            ],
            "input_node_to_edge_type": {
                "multiply_22": "Data",
                "multiply_764": "Data"
            },
            "input_nodes": [
                "multiply_764",
                "multiply_22"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_765",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: softmax_766 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "softmax_766"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "add_765",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 724
        },
        "add_772": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_771 (port_0)",
                "Data: bert.encoder.layer.14.attention.self.value.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.14.attention.self.value.bias": "Data",
                "reshape_771": "Data"
            },
            "input_nodes": [
                "reshape_771",
                "bert.encoder.layer.14.attention.self.value.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_772",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_773 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_773"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_772",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 740
        },
        "add_783": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_782 (port_0)",
                "Data: bert.encoder.layer.14.attention.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.14.attention.output.dense.bias": "Data",
                "reshape_782": "Data"
            },
            "input_nodes": [
                "reshape_782",
                "bert.encoder.layer.14.attention.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_783",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_784 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_784"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_783",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 573
        },
        "add_785": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_784 (port_0)",
                "Data: layernorm_747 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_747": "Data",
                "nop_784": "Data"
            },
            "input_nodes": [
                "nop_784",
                "layernorm_747"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_785",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_786 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_786"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output",
                "original_op_name": "add_785",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 539
        },
        "add_791": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_790 (port_0)",
                "Data: bert.encoder.layer.14.intermediate.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.14.intermediate.dense.bias": "Data",
                "reshape_790": "Data"
            },
            "input_nodes": [
                "reshape_790",
                "bert.encoder.layer.14.intermediate.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_791",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: gelu_792 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "gelu_792"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_791",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 623
        },
        "add_797": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_796 (port_0)",
                "Data: bert.encoder.layer.14.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.14.output.dense.bias": "Data",
                "reshape_796": "Data"
            },
            "input_nodes": [
                "reshape_796",
                "bert.encoder.layer.14.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_797",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_798 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_798"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_797",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 538
        },
        "add_799": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_798 (port_0)",
                "Data: layernorm_786 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_786": "Data",
                "nop_798": "Data"
            },
            "input_nodes": [
                "nop_798",
                "layernorm_786"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_799",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_800 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_800"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertOutput::output",
                "original_op_name": "add_799",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 504
        },
        "add_805": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_804 (port_0)",
                "Data: bert.encoder.layer.15.attention.self.query.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.15.attention.self.query.bias": "Data",
                "reshape_804": "Data"
            },
            "input_nodes": [
                "reshape_804",
                "bert.encoder.layer.15.attention.self.query.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_805",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_806 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_806"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_805",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 754
        },
        "add_811": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_810 (port_0)",
                "Data: bert.encoder.layer.15.attention.self.key.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.15.attention.self.key.bias": "Data",
                "reshape_810": "Data"
            },
            "input_nodes": [
                "reshape_810",
                "bert.encoder.layer.15.attention.self.key.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_811",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_812 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_812"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_811",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 773
        },
        "add_818": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: multiply_817 (port_0)",
                "Data: multiply_22 (port_1)"
            ],
            "input_node_to_edge_type": {
                "multiply_22": "Data",
                "multiply_817": "Data"
            },
            "input_nodes": [
                "multiply_817",
                "multiply_22"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_818",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: softmax_819 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "softmax_819"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "add_818",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 654
        },
        "add_825": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_824 (port_0)",
                "Data: bert.encoder.layer.15.attention.self.value.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.15.attention.self.value.bias": "Data",
                "reshape_824": "Data"
            },
            "input_nodes": [
                "reshape_824",
                "bert.encoder.layer.15.attention.self.value.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_825",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_826 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_826"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_825",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 670
        },
        "add_83": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_82 (port_0)",
                "Data: bert.encoder.layer.1.attention.self.value.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.1.attention.self.value.bias": "Data",
                "reshape_82": "Data"
            },
            "input_nodes": [
                "reshape_82",
                "bert.encoder.layer.1.attention.self.value.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_83",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_84 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_84"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_83",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1630
        },
        "add_836": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_835 (port_0)",
                "Data: bert.encoder.layer.15.attention.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.15.attention.output.dense.bias": "Data",
                "reshape_835": "Data"
            },
            "input_nodes": [
                "reshape_835",
                "bert.encoder.layer.15.attention.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_836",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_837 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_837"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_836",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 503
        },
        "add_838": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_837 (port_0)",
                "Data: layernorm_800 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_800": "Data",
                "nop_837": "Data"
            },
            "input_nodes": [
                "nop_837",
                "layernorm_800"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_838",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_839 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_839"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output",
                "original_op_name": "add_838",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 469
        },
        "add_844": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_843 (port_0)",
                "Data: bert.encoder.layer.15.intermediate.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.15.intermediate.dense.bias": "Data",
                "reshape_843": "Data"
            },
            "input_nodes": [
                "reshape_843",
                "bert.encoder.layer.15.intermediate.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_844",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: gelu_845 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "gelu_845"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_844",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 553
        },
        "add_850": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_849 (port_0)",
                "Data: bert.encoder.layer.15.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.15.output.dense.bias": "Data",
                "reshape_849": "Data"
            },
            "input_nodes": [
                "reshape_849",
                "bert.encoder.layer.15.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_850",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_851 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_851"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_850",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 468
        },
        "add_852": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_851 (port_0)",
                "Data: layernorm_839 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_839": "Data",
                "nop_851": "Data"
            },
            "input_nodes": [
                "nop_851",
                "layernorm_839"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_852",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_853 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_853"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertOutput::output",
                "original_op_name": "add_852",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 434
        },
        "add_858": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_857 (port_0)",
                "Data: bert.encoder.layer.16.attention.self.query.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.16.attention.self.query.bias": "Data",
                "reshape_857": "Data"
            },
            "input_nodes": [
                "reshape_857",
                "bert.encoder.layer.16.attention.self.query.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_858",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_859 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_859"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_858",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 684
        },
        "add_864": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_863 (port_0)",
                "Data: bert.encoder.layer.16.attention.self.key.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.16.attention.self.key.bias": "Data",
                "reshape_863": "Data"
            },
            "input_nodes": [
                "reshape_863",
                "bert.encoder.layer.16.attention.self.key.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_864",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_865 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_865"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_864",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 703
        },
        "add_871": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: multiply_870 (port_0)",
                "Data: multiply_22 (port_1)"
            ],
            "input_node_to_edge_type": {
                "multiply_22": "Data",
                "multiply_870": "Data"
            },
            "input_nodes": [
                "multiply_870",
                "multiply_22"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_871",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: softmax_872 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "softmax_872"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "add_871",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 584
        },
        "add_878": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_877 (port_0)",
                "Data: bert.encoder.layer.16.attention.self.value.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.16.attention.self.value.bias": "Data",
                "reshape_877": "Data"
            },
            "input_nodes": [
                "reshape_877",
                "bert.encoder.layer.16.attention.self.value.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_878",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_879 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_879"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_878",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 600
        },
        "add_889": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_888 (port_0)",
                "Data: bert.encoder.layer.16.attention.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.16.attention.output.dense.bias": "Data",
                "reshape_888": "Data"
            },
            "input_nodes": [
                "reshape_888",
                "bert.encoder.layer.16.attention.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_889",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_890 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_890"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_889",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 433
        },
        "add_891": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_890 (port_0)",
                "Data: layernorm_853 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_853": "Data",
                "nop_890": "Data"
            },
            "input_nodes": [
                "nop_890",
                "layernorm_853"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_891",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_892 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_892"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output",
                "original_op_name": "add_891",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 399
        },
        "add_897": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_896 (port_0)",
                "Data: bert.encoder.layer.16.intermediate.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.16.intermediate.dense.bias": "Data",
                "reshape_896": "Data"
            },
            "input_nodes": [
                "reshape_896",
                "bert.encoder.layer.16.intermediate.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_897",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: gelu_898 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "gelu_898"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_897",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 483
        },
        "add_903": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_902 (port_0)",
                "Data: bert.encoder.layer.16.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.16.output.dense.bias": "Data",
                "reshape_902": "Data"
            },
            "input_nodes": [
                "reshape_902",
                "bert.encoder.layer.16.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_903",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_904 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_904"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_903",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 398
        },
        "add_905": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_904 (port_0)",
                "Data: layernorm_892 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_892": "Data",
                "nop_904": "Data"
            },
            "input_nodes": [
                "nop_904",
                "layernorm_892"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_905",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_906 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_906"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertOutput::output",
                "original_op_name": "add_905",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 364
        },
        "add_911": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_910 (port_0)",
                "Data: bert.encoder.layer.17.attention.self.query.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.17.attention.self.query.bias": "Data",
                "reshape_910": "Data"
            },
            "input_nodes": [
                "reshape_910",
                "bert.encoder.layer.17.attention.self.query.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_911",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_912 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_912"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_911",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 614
        },
        "add_917": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_916 (port_0)",
                "Data: bert.encoder.layer.17.attention.self.key.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.17.attention.self.key.bias": "Data",
                "reshape_916": "Data"
            },
            "input_nodes": [
                "reshape_916",
                "bert.encoder.layer.17.attention.self.key.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_917",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_918 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_918"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_917",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 633
        },
        "add_924": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: multiply_923 (port_0)",
                "Data: multiply_22 (port_1)"
            ],
            "input_node_to_edge_type": {
                "multiply_22": "Data",
                "multiply_923": "Data"
            },
            "input_nodes": [
                "multiply_923",
                "multiply_22"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_924",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: softmax_925 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "softmax_925"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "add_924",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 514
        },
        "add_931": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_930 (port_0)",
                "Data: bert.encoder.layer.17.attention.self.value.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.17.attention.self.value.bias": "Data",
                "reshape_930": "Data"
            },
            "input_nodes": [
                "reshape_930",
                "bert.encoder.layer.17.attention.self.value.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_931",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_932 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_932"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_931",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 530
        },
        "add_94": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_93 (port_0)",
                "Data: bert.encoder.layer.1.attention.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.1.attention.output.dense.bias": "Data",
                "reshape_93": "Data"
            },
            "input_nodes": [
                "reshape_93",
                "bert.encoder.layer.1.attention.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_94",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_95 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_95"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_94",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1483
        },
        "add_942": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_941 (port_0)",
                "Data: bert.encoder.layer.17.attention.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.17.attention.output.dense.bias": "Data",
                "reshape_941": "Data"
            },
            "input_nodes": [
                "reshape_941",
                "bert.encoder.layer.17.attention.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_942",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_943 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_943"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_942",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 363
        },
        "add_944": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_943 (port_0)",
                "Data: layernorm_906 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_906": "Data",
                "nop_943": "Data"
            },
            "input_nodes": [
                "nop_943",
                "layernorm_906"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_944",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_945 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_945"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output",
                "original_op_name": "add_944",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 329
        },
        "add_950": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_949 (port_0)",
                "Data: bert.encoder.layer.17.intermediate.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.17.intermediate.dense.bias": "Data",
                "reshape_949": "Data"
            },
            "input_nodes": [
                "reshape_949",
                "bert.encoder.layer.17.intermediate.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_950",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: gelu_951 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "gelu_951"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_950",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 413
        },
        "add_956": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_955 (port_0)",
                "Data: bert.encoder.layer.17.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.17.output.dense.bias": "Data",
                "reshape_955": "Data"
            },
            "input_nodes": [
                "reshape_955",
                "bert.encoder.layer.17.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_956",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_957 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_957"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_956",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 328
        },
        "add_958": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_957 (port_0)",
                "Data: layernorm_945 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_945": "Data",
                "nop_957": "Data"
            },
            "input_nodes": [
                "nop_957",
                "layernorm_945"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_958",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_959 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_959"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertOutput::output",
                "original_op_name": "add_958",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 294
        },
        "add_96": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_95 (port_0)",
                "Data: layernorm_58 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_58": "Data",
                "nop_95": "Data"
            },
            "input_nodes": [
                "nop_95",
                "layernorm_58"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_96",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_97 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_97"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output",
                "original_op_name": "add_96",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 1449
        },
        "add_964": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_963 (port_0)",
                "Data: bert.encoder.layer.18.attention.self.query.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.18.attention.self.query.bias": "Data",
                "reshape_963": "Data"
            },
            "input_nodes": [
                "reshape_963",
                "bert.encoder.layer.18.attention.self.query.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_964",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_965 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_965"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_964",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 544
        },
        "add_970": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_969 (port_0)",
                "Data: bert.encoder.layer.18.attention.self.key.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.18.attention.self.key.bias": "Data",
                "reshape_969": "Data"
            },
            "input_nodes": [
                "reshape_969",
                "bert.encoder.layer.18.attention.self.key.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_970",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_971 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_971"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_970",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 563
        },
        "add_977": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: multiply_976 (port_0)",
                "Data: multiply_22 (port_1)"
            ],
            "input_node_to_edge_type": {
                "multiply_22": "Data",
                "multiply_976": "Data"
            },
            "input_nodes": [
                "multiply_976",
                "multiply_22"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_977",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: softmax_978 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "softmax_978"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "add_977",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 444
        },
        "add_984": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_983 (port_0)",
                "Data: bert.encoder.layer.18.attention.self.value.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.18.attention.self.value.bias": "Data",
                "reshape_983": "Data"
            },
            "input_nodes": [
                "reshape_983",
                "bert.encoder.layer.18.attention.self.value.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_984",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hslice_985 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hslice_985"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_984",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 460
        },
        "add_995": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_994 (port_0)",
                "Data: bert.encoder.layer.18.attention.output.dense.bias (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.18.attention.output.dense.bias": "Data",
                "reshape_994": "Data"
            },
            "input_nodes": [
                "reshape_994",
                "bert.encoder.layer.18.attention.output.dense.bias"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "add_995",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_996 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_996"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "add_995",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 293
        },
        "add_997": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "add",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_996 (port_0)",
                "Data: layernorm_959 (port_1)"
            ],
            "input_node_to_edge_type": {
                "layernorm_959": "Data",
                "nop_996": "Data"
            },
            "input_nodes": [
                "nop_996",
                "layernorm_959"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "add_997",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "add"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: layernorm_998 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_998"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output",
                "original_op_name": "add_997",
                "original_op_type": "add"
            },
            "type": "add",
            "unique_id": 259
        },
        "attention_mask_1": {
            "cache": {
                "shape": [
                    1,
                    384
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "attention_mask_1",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: reshape_19 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_19"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": false,
            "tags": {
                "original_op_name": "attention_mask_1"
            },
            "tile_broadcast": [],
            "type": "Input::input",
            "unique_id": 180
        },
        "bert.embeddings.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.embeddings.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_0 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_0"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.embeddings.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1572
        },
        "bert.embeddings.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.embeddings.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_0 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_0"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.embeddings.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1571
        },
        "bert.encoder.layer.0.attention.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.0.attention.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_44 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_44"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.0.attention.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1521
        },
        "bert.encoder.layer.0.attention.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.0.attention.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_44 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_44"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.0.attention.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1520
        },
        "bert.encoder.layer.0.attention.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.0.attention.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_41 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_41"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.0.attention.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1569
        },
        "bert.encoder.layer.0.attention.output.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.0.attention.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_39 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_39"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.0.attention.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1612
        },
        "bert.encoder.layer.0.attention.self.key.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.0.attention.self.key.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_12 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_12"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.0.attention.self.key.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1706
        },
        "bert.encoder.layer.0.attention.self.key.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.0.attention.self.key.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_10 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_10"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.0.attention.self.key.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1711
        },
        "bert.encoder.layer.0.attention.self.query.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.0.attention.self.query.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_6 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_6"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.0.attention.self.query.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1702
        },
        "bert.encoder.layer.0.attention.self.query.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.0.attention.self.query.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_4 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_4"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.0.attention.self.query.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1709
        },
        "bert.encoder.layer.0.attention.self.value.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.0.attention.self.value.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_30 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_30"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.0.attention.self.value.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1677
        },
        "bert.encoder.layer.0.attention.self.value.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.0.attention.self.value.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_28 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_28"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.0.attention.self.value.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1695
        },
        "bert.encoder.layer.0.intermediate.dense.bias": {
            "cache": {
                "shape": [
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.0.intermediate.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_49 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_49"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.0.intermediate.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1610
        },
        "bert.encoder.layer.0.intermediate.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.0.intermediate.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_47 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_47"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.0.intermediate.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1643
        },
        "bert.encoder.layer.0.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.0.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_58 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_58"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.0.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1486
        },
        "bert.encoder.layer.0.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.0.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_58 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_58"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.0.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1485
        },
        "bert.encoder.layer.0.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.0.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_55 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_55"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.0.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1537
        },
        "bert.encoder.layer.0.output.dense.weight": {
            "cache": {
                "shape": [
                    4096,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.0.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_53 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_53"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.0.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1585
        },
        "bert.encoder.layer.1.attention.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.1.attention.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_97 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_97"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.1.attention.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1451
        },
        "bert.encoder.layer.1.attention.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.1.attention.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_97 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_97"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.1.attention.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1450
        },
        "bert.encoder.layer.1.attention.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.1.attention.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_94 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_94"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.1.attention.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1501
        },
        "bert.encoder.layer.1.attention.output.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.1.attention.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_92 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_92"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.1.attention.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1551
        },
        "bert.encoder.layer.1.attention.self.key.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.1.attention.self.key.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_69 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_69"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.1.attention.self.key.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1686
        },
        "bert.encoder.layer.1.attention.self.key.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.1.attention.self.key.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_67 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_67"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.1.attention.self.key.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1700
        },
        "bert.encoder.layer.1.attention.self.query.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.1.attention.self.query.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_63 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_63"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.1.attention.self.query.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1680
        },
        "bert.encoder.layer.1.attention.self.query.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.1.attention.self.query.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_61 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_61"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.1.attention.self.query.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1696
        },
        "bert.encoder.layer.1.attention.self.value.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.1.attention.self.value.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_83 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_83"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.1.attention.self.value.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1642
        },
        "bert.encoder.layer.1.attention.self.value.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.1.attention.self.value.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_81 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_81"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.1.attention.self.value.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1667
        },
        "bert.encoder.layer.1.intermediate.dense.bias": {
            "cache": {
                "shape": [
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.1.intermediate.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_102 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_102"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.1.intermediate.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1549
        },
        "bert.encoder.layer.1.intermediate.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.1.intermediate.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_100 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_100"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.1.intermediate.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1595
        },
        "bert.encoder.layer.1.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.1.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_111 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_111"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.1.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1416
        },
        "bert.encoder.layer.1.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.1.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_111 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_111"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.1.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1415
        },
        "bert.encoder.layer.1.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.1.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_108 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_108"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.1.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1467
        },
        "bert.encoder.layer.1.output.dense.weight": {
            "cache": {
                "shape": [
                    4096,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.1.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_106 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_106"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.1.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1516
        },
        "bert.encoder.layer.10.attention.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.10.attention.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_574 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_574"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.10.attention.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 821
        },
        "bert.encoder.layer.10.attention.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.10.attention.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_574 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_574"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.10.attention.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 820
        },
        "bert.encoder.layer.10.attention.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.10.attention.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_571 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_571"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.10.attention.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 871
        },
        "bert.encoder.layer.10.attention.output.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.10.attention.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_569 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_569"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.10.attention.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 921
        },
        "bert.encoder.layer.10.attention.self.key.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.10.attention.self.key.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_546 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_546"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.10.attention.self.key.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1139
        },
        "bert.encoder.layer.10.attention.self.key.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.10.attention.self.key.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_544 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_544"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.10.attention.self.key.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1190
        },
        "bert.encoder.layer.10.attention.self.query.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.10.attention.self.query.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_540 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_540"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.10.attention.self.query.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1122
        },
        "bert.encoder.layer.10.attention.self.query.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.10.attention.self.query.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_538 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_538"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.10.attention.self.query.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1172
        },
        "bert.encoder.layer.10.attention.self.value.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.10.attention.self.value.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_560 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_560"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.10.attention.self.value.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1039
        },
        "bert.encoder.layer.10.attention.self.value.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.10.attention.self.value.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_558 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_558"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.10.attention.self.value.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1088
        },
        "bert.encoder.layer.10.intermediate.dense.bias": {
            "cache": {
                "shape": [
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.10.intermediate.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_579 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_579"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.10.intermediate.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 919
        },
        "bert.encoder.layer.10.intermediate.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.10.intermediate.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_577 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_577"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.10.intermediate.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 970
        },
        "bert.encoder.layer.10.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.10.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_588 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_588"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.10.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 786
        },
        "bert.encoder.layer.10.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.10.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_588 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_588"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.10.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 785
        },
        "bert.encoder.layer.10.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.10.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_585 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_585"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.10.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 837
        },
        "bert.encoder.layer.10.output.dense.weight": {
            "cache": {
                "shape": [
                    4096,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.10.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_583 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_583"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.10.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 886
        },
        "bert.encoder.layer.11.attention.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.11.attention.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_627 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_627"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.11.attention.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 751
        },
        "bert.encoder.layer.11.attention.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.11.attention.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_627 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_627"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.11.attention.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 750
        },
        "bert.encoder.layer.11.attention.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.11.attention.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_624 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_624"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.11.attention.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 801
        },
        "bert.encoder.layer.11.attention.output.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.11.attention.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_622 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_622"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.11.attention.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 851
        },
        "bert.encoder.layer.11.attention.self.key.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.11.attention.self.key.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_599 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_599"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.11.attention.self.key.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1069
        },
        "bert.encoder.layer.11.attention.self.key.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.11.attention.self.key.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_597 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_597"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.11.attention.self.key.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1120
        },
        "bert.encoder.layer.11.attention.self.query.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.11.attention.self.query.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_593 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_593"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.11.attention.self.query.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1052
        },
        "bert.encoder.layer.11.attention.self.query.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.11.attention.self.query.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_591 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_591"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.11.attention.self.query.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1102
        },
        "bert.encoder.layer.11.attention.self.value.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.11.attention.self.value.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_613 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_613"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.11.attention.self.value.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 969
        },
        "bert.encoder.layer.11.attention.self.value.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.11.attention.self.value.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_611 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_611"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.11.attention.self.value.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1018
        },
        "bert.encoder.layer.11.intermediate.dense.bias": {
            "cache": {
                "shape": [
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.11.intermediate.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_632 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_632"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.11.intermediate.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 849
        },
        "bert.encoder.layer.11.intermediate.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.11.intermediate.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_630 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_630"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.11.intermediate.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 900
        },
        "bert.encoder.layer.11.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.11.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_641 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_641"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.11.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 716
        },
        "bert.encoder.layer.11.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.11.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_641 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_641"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.11.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 715
        },
        "bert.encoder.layer.11.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.11.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_638 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_638"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.11.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 767
        },
        "bert.encoder.layer.11.output.dense.weight": {
            "cache": {
                "shape": [
                    4096,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.11.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_636 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_636"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.11.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 816
        },
        "bert.encoder.layer.12.attention.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.12.attention.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_680 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_680"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.12.attention.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 681
        },
        "bert.encoder.layer.12.attention.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.12.attention.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_680 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_680"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.12.attention.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 680
        },
        "bert.encoder.layer.12.attention.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.12.attention.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_677 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_677"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.12.attention.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 731
        },
        "bert.encoder.layer.12.attention.output.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.12.attention.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_675 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_675"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.12.attention.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 781
        },
        "bert.encoder.layer.12.attention.self.key.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.12.attention.self.key.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_652 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_652"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.12.attention.self.key.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 999
        },
        "bert.encoder.layer.12.attention.self.key.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.12.attention.self.key.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_650 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_650"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.12.attention.self.key.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1050
        },
        "bert.encoder.layer.12.attention.self.query.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.12.attention.self.query.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_646 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_646"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.12.attention.self.query.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 982
        },
        "bert.encoder.layer.12.attention.self.query.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.12.attention.self.query.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_644 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_644"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.12.attention.self.query.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1032
        },
        "bert.encoder.layer.12.attention.self.value.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.12.attention.self.value.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_666 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_666"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.12.attention.self.value.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 899
        },
        "bert.encoder.layer.12.attention.self.value.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.12.attention.self.value.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_664 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_664"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.12.attention.self.value.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 948
        },
        "bert.encoder.layer.12.intermediate.dense.bias": {
            "cache": {
                "shape": [
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.12.intermediate.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_685 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_685"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.12.intermediate.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 779
        },
        "bert.encoder.layer.12.intermediate.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.12.intermediate.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_683 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_683"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.12.intermediate.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 830
        },
        "bert.encoder.layer.12.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.12.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_694 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_694"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.12.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 646
        },
        "bert.encoder.layer.12.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.12.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_694 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_694"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.12.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 645
        },
        "bert.encoder.layer.12.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.12.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_691 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_691"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.12.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 697
        },
        "bert.encoder.layer.12.output.dense.weight": {
            "cache": {
                "shape": [
                    4096,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.12.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_689 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_689"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.12.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 746
        },
        "bert.encoder.layer.13.attention.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.13.attention.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_733 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_733"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.13.attention.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 611
        },
        "bert.encoder.layer.13.attention.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.13.attention.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_733 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_733"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.13.attention.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 610
        },
        "bert.encoder.layer.13.attention.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.13.attention.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_730 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_730"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.13.attention.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 661
        },
        "bert.encoder.layer.13.attention.output.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.13.attention.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_728 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_728"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.13.attention.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 711
        },
        "bert.encoder.layer.13.attention.self.key.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.13.attention.self.key.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_705 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_705"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.13.attention.self.key.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 929
        },
        "bert.encoder.layer.13.attention.self.key.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.13.attention.self.key.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_703 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_703"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.13.attention.self.key.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 980
        },
        "bert.encoder.layer.13.attention.self.query.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.13.attention.self.query.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_699 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_699"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.13.attention.self.query.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 912
        },
        "bert.encoder.layer.13.attention.self.query.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.13.attention.self.query.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_697 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_697"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.13.attention.self.query.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 962
        },
        "bert.encoder.layer.13.attention.self.value.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.13.attention.self.value.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_719 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_719"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.13.attention.self.value.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 829
        },
        "bert.encoder.layer.13.attention.self.value.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.13.attention.self.value.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_717 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_717"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.13.attention.self.value.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 878
        },
        "bert.encoder.layer.13.intermediate.dense.bias": {
            "cache": {
                "shape": [
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.13.intermediate.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_738 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_738"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.13.intermediate.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 709
        },
        "bert.encoder.layer.13.intermediate.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.13.intermediate.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_736 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_736"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.13.intermediate.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 760
        },
        "bert.encoder.layer.13.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.13.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_747 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_747"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.13.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 576
        },
        "bert.encoder.layer.13.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.13.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_747 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_747"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.13.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 575
        },
        "bert.encoder.layer.13.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.13.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_744 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_744"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.13.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 627
        },
        "bert.encoder.layer.13.output.dense.weight": {
            "cache": {
                "shape": [
                    4096,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.13.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_742 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_742"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.13.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 676
        },
        "bert.encoder.layer.14.attention.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.14.attention.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_786 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_786"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.14.attention.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 541
        },
        "bert.encoder.layer.14.attention.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.14.attention.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_786 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_786"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.14.attention.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 540
        },
        "bert.encoder.layer.14.attention.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.14.attention.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_783 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_783"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.14.attention.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 591
        },
        "bert.encoder.layer.14.attention.output.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.14.attention.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_781 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_781"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.14.attention.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 641
        },
        "bert.encoder.layer.14.attention.self.key.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.14.attention.self.key.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_758 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_758"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.14.attention.self.key.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 859
        },
        "bert.encoder.layer.14.attention.self.key.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.14.attention.self.key.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_756 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_756"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.14.attention.self.key.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 910
        },
        "bert.encoder.layer.14.attention.self.query.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.14.attention.self.query.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_752 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_752"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.14.attention.self.query.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 842
        },
        "bert.encoder.layer.14.attention.self.query.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.14.attention.self.query.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_750 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_750"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.14.attention.self.query.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 892
        },
        "bert.encoder.layer.14.attention.self.value.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.14.attention.self.value.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_772 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_772"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.14.attention.self.value.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 759
        },
        "bert.encoder.layer.14.attention.self.value.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.14.attention.self.value.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_770 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_770"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.14.attention.self.value.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 808
        },
        "bert.encoder.layer.14.intermediate.dense.bias": {
            "cache": {
                "shape": [
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.14.intermediate.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_791 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_791"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.14.intermediate.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 639
        },
        "bert.encoder.layer.14.intermediate.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.14.intermediate.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_789 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_789"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.14.intermediate.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 690
        },
        "bert.encoder.layer.14.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.14.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_800 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_800"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.14.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 506
        },
        "bert.encoder.layer.14.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.14.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_800 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_800"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.14.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 505
        },
        "bert.encoder.layer.14.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.14.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_797 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_797"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.14.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 557
        },
        "bert.encoder.layer.14.output.dense.weight": {
            "cache": {
                "shape": [
                    4096,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.14.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_795 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_795"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.14.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 606
        },
        "bert.encoder.layer.15.attention.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.15.attention.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_839 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_839"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.15.attention.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 471
        },
        "bert.encoder.layer.15.attention.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.15.attention.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_839 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_839"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.15.attention.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 470
        },
        "bert.encoder.layer.15.attention.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.15.attention.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_836 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_836"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.15.attention.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 521
        },
        "bert.encoder.layer.15.attention.output.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.15.attention.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_834 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_834"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.15.attention.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 571
        },
        "bert.encoder.layer.15.attention.self.key.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.15.attention.self.key.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_811 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_811"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.15.attention.self.key.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 789
        },
        "bert.encoder.layer.15.attention.self.key.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.15.attention.self.key.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_809 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_809"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.15.attention.self.key.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 840
        },
        "bert.encoder.layer.15.attention.self.query.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.15.attention.self.query.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_805 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_805"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.15.attention.self.query.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 772
        },
        "bert.encoder.layer.15.attention.self.query.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.15.attention.self.query.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_803 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_803"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.15.attention.self.query.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 822
        },
        "bert.encoder.layer.15.attention.self.value.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.15.attention.self.value.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_825 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_825"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.15.attention.self.value.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 689
        },
        "bert.encoder.layer.15.attention.self.value.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.15.attention.self.value.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_823 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_823"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.15.attention.self.value.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 738
        },
        "bert.encoder.layer.15.intermediate.dense.bias": {
            "cache": {
                "shape": [
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.15.intermediate.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_844 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_844"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.15.intermediate.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 569
        },
        "bert.encoder.layer.15.intermediate.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.15.intermediate.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_842 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_842"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.15.intermediate.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 620
        },
        "bert.encoder.layer.15.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.15.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_853 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_853"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.15.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 436
        },
        "bert.encoder.layer.15.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.15.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_853 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_853"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.15.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 435
        },
        "bert.encoder.layer.15.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.15.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_850 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_850"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.15.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 487
        },
        "bert.encoder.layer.15.output.dense.weight": {
            "cache": {
                "shape": [
                    4096,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.15.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_848 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_848"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.15.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 536
        },
        "bert.encoder.layer.16.attention.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.16.attention.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_892 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_892"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.16.attention.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 401
        },
        "bert.encoder.layer.16.attention.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.16.attention.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_892 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_892"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.16.attention.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 400
        },
        "bert.encoder.layer.16.attention.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.16.attention.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_889 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_889"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.16.attention.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 451
        },
        "bert.encoder.layer.16.attention.output.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.16.attention.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_887 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_887"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.16.attention.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 501
        },
        "bert.encoder.layer.16.attention.self.key.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.16.attention.self.key.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_864 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_864"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.16.attention.self.key.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 719
        },
        "bert.encoder.layer.16.attention.self.key.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.16.attention.self.key.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_862 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_862"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.16.attention.self.key.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 770
        },
        "bert.encoder.layer.16.attention.self.query.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.16.attention.self.query.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_858 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_858"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.16.attention.self.query.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 702
        },
        "bert.encoder.layer.16.attention.self.query.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.16.attention.self.query.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_856 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_856"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.16.attention.self.query.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 752
        },
        "bert.encoder.layer.16.attention.self.value.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.16.attention.self.value.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_878 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_878"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.16.attention.self.value.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 619
        },
        "bert.encoder.layer.16.attention.self.value.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.16.attention.self.value.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_876 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_876"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.16.attention.self.value.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 668
        },
        "bert.encoder.layer.16.intermediate.dense.bias": {
            "cache": {
                "shape": [
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.16.intermediate.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_897 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_897"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.16.intermediate.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 499
        },
        "bert.encoder.layer.16.intermediate.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.16.intermediate.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_895 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_895"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.16.intermediate.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 550
        },
        "bert.encoder.layer.16.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.16.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_906 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_906"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.16.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 366
        },
        "bert.encoder.layer.16.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.16.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_906 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_906"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.16.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 365
        },
        "bert.encoder.layer.16.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.16.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_903 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_903"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.16.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 417
        },
        "bert.encoder.layer.16.output.dense.weight": {
            "cache": {
                "shape": [
                    4096,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.16.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_901 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_901"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.16.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 466
        },
        "bert.encoder.layer.17.attention.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.17.attention.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_945 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_945"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.17.attention.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 331
        },
        "bert.encoder.layer.17.attention.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.17.attention.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_945 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_945"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.17.attention.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 330
        },
        "bert.encoder.layer.17.attention.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.17.attention.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_942 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_942"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.17.attention.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 381
        },
        "bert.encoder.layer.17.attention.output.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.17.attention.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_940 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_940"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.17.attention.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 431
        },
        "bert.encoder.layer.17.attention.self.key.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.17.attention.self.key.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_917 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_917"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.17.attention.self.key.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 649
        },
        "bert.encoder.layer.17.attention.self.key.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.17.attention.self.key.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_915 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_915"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.17.attention.self.key.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 700
        },
        "bert.encoder.layer.17.attention.self.query.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.17.attention.self.query.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_911 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_911"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.17.attention.self.query.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 632
        },
        "bert.encoder.layer.17.attention.self.query.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.17.attention.self.query.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_909 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_909"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.17.attention.self.query.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 682
        },
        "bert.encoder.layer.17.attention.self.value.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.17.attention.self.value.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_931 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_931"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.17.attention.self.value.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 549
        },
        "bert.encoder.layer.17.attention.self.value.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.17.attention.self.value.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_929 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_929"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.17.attention.self.value.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 598
        },
        "bert.encoder.layer.17.intermediate.dense.bias": {
            "cache": {
                "shape": [
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.17.intermediate.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_950 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_950"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.17.intermediate.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 429
        },
        "bert.encoder.layer.17.intermediate.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.17.intermediate.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_948 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_948"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.17.intermediate.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 480
        },
        "bert.encoder.layer.17.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.17.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_959 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_959"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.17.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 296
        },
        "bert.encoder.layer.17.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.17.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_959 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_959"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.17.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 295
        },
        "bert.encoder.layer.17.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.17.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_956 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_956"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.17.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 347
        },
        "bert.encoder.layer.17.output.dense.weight": {
            "cache": {
                "shape": [
                    4096,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.17.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_954 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_954"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.17.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 396
        },
        "bert.encoder.layer.18.attention.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.18.attention.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_998 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_998"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.18.attention.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 261
        },
        "bert.encoder.layer.18.attention.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.18.attention.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_998 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_998"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.18.attention.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 260
        },
        "bert.encoder.layer.18.attention.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.18.attention.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_995 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_995"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.18.attention.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 311
        },
        "bert.encoder.layer.18.attention.output.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.18.attention.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_993 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_993"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.18.attention.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 361
        },
        "bert.encoder.layer.18.attention.self.key.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.18.attention.self.key.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_970 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_970"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.18.attention.self.key.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 579
        },
        "bert.encoder.layer.18.attention.self.key.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.18.attention.self.key.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_968 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_968"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.18.attention.self.key.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 630
        },
        "bert.encoder.layer.18.attention.self.query.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.18.attention.self.query.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_964 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_964"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.18.attention.self.query.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 562
        },
        "bert.encoder.layer.18.attention.self.query.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.18.attention.self.query.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_962 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_962"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.18.attention.self.query.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 612
        },
        "bert.encoder.layer.18.attention.self.value.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.18.attention.self.value.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_984 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_984"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.18.attention.self.value.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 479
        },
        "bert.encoder.layer.18.attention.self.value.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.18.attention.self.value.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_982 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_982"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.18.attention.self.value.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 528
        },
        "bert.encoder.layer.18.intermediate.dense.bias": {
            "cache": {
                "shape": [
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.18.intermediate.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_1003 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1003"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.18.intermediate.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 359
        },
        "bert.encoder.layer.18.intermediate.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.18.intermediate.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_1001 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1001"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.18.intermediate.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 410
        },
        "bert.encoder.layer.18.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.18.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_1012 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_1012"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.18.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 226
        },
        "bert.encoder.layer.18.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.18.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_1012 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_1012"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.18.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 225
        },
        "bert.encoder.layer.18.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.18.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_1009 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1009"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.18.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 277
        },
        "bert.encoder.layer.18.output.dense.weight": {
            "cache": {
                "shape": [
                    4096,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.18.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_1007 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1007"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.18.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 326
        },
        "bert.encoder.layer.19.attention.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.19.attention.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_1051 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_1051"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.19.attention.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 194
        },
        "bert.encoder.layer.19.attention.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.19.attention.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_1051 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_1051"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.19.attention.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 193
        },
        "bert.encoder.layer.19.attention.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.19.attention.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_1048 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1048"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.19.attention.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 241
        },
        "bert.encoder.layer.19.attention.output.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.19.attention.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_1046 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1046"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.19.attention.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 291
        },
        "bert.encoder.layer.19.attention.self.key.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.19.attention.self.key.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_1023 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1023"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.19.attention.self.key.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 509
        },
        "bert.encoder.layer.19.attention.self.key.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.19.attention.self.key.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_1021 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1021"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.19.attention.self.key.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 560
        },
        "bert.encoder.layer.19.attention.self.query.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.19.attention.self.query.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_1017 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1017"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.19.attention.self.query.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 492
        },
        "bert.encoder.layer.19.attention.self.query.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.19.attention.self.query.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_1015 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1015"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.19.attention.self.query.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 542
        },
        "bert.encoder.layer.19.attention.self.value.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.19.attention.self.value.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_1037 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1037"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.19.attention.self.value.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 409
        },
        "bert.encoder.layer.19.attention.self.value.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.19.attention.self.value.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_1035 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1035"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.19.attention.self.value.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 458
        },
        "bert.encoder.layer.19.intermediate.dense.bias": {
            "cache": {
                "shape": [
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.19.intermediate.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_1056 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1056"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.19.intermediate.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 289
        },
        "bert.encoder.layer.19.intermediate.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.19.intermediate.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_1054 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1054"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.19.intermediate.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 340
        },
        "bert.encoder.layer.19.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.19.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_1065 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_1065"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.19.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 162
        },
        "bert.encoder.layer.19.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.19.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_1065 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_1065"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.19.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 161
        },
        "bert.encoder.layer.19.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.19.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_1062 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1062"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.19.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 208
        },
        "bert.encoder.layer.19.output.dense.weight": {
            "cache": {
                "shape": [
                    4096,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.19.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_1060 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1060"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.19.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 256
        },
        "bert.encoder.layer.2.attention.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.2.attention.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_150 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_150"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.2.attention.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1381
        },
        "bert.encoder.layer.2.attention.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.2.attention.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_150 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_150"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.2.attention.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1380
        },
        "bert.encoder.layer.2.attention.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.2.attention.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_147 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_147"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.2.attention.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1431
        },
        "bert.encoder.layer.2.attention.output.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.2.attention.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_145 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_145"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.2.attention.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1481
        },
        "bert.encoder.layer.2.attention.self.key.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.2.attention.self.key.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_122 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_122"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.2.attention.self.key.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1656
        },
        "bert.encoder.layer.2.attention.self.key.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.2.attention.self.key.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_120 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_120"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.2.attention.self.key.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1678
        },
        "bert.encoder.layer.2.attention.self.query.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.2.attention.self.query.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_116 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_116"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.2.attention.self.query.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1648
        },
        "bert.encoder.layer.2.attention.self.query.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.2.attention.self.query.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_114 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_114"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.2.attention.self.query.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1670
        },
        "bert.encoder.layer.2.attention.self.value.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.2.attention.self.value.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_136 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_136"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.2.attention.self.value.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1594
        },
        "bert.encoder.layer.2.attention.self.value.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.2.attention.self.value.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_134 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_134"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.2.attention.self.value.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1628
        },
        "bert.encoder.layer.2.intermediate.dense.bias": {
            "cache": {
                "shape": [
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.2.intermediate.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_155 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_155"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.2.intermediate.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1479
        },
        "bert.encoder.layer.2.intermediate.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.2.intermediate.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_153 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_153"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.2.intermediate.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1530
        },
        "bert.encoder.layer.2.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.2.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_164 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_164"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.2.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1346
        },
        "bert.encoder.layer.2.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.2.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_164 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_164"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.2.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1345
        },
        "bert.encoder.layer.2.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.2.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_161 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_161"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.2.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1397
        },
        "bert.encoder.layer.2.output.dense.weight": {
            "cache": {
                "shape": [
                    4096,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.2.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_159 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_159"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.2.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1446
        },
        "bert.encoder.layer.20.attention.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.20.attention.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_1104 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_1104"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.20.attention.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 131
        },
        "bert.encoder.layer.20.attention.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.20.attention.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_1104 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_1104"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.20.attention.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 130
        },
        "bert.encoder.layer.20.attention.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.20.attention.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_1101 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1101"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.20.attention.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 175
        },
        "bert.encoder.layer.20.attention.output.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.20.attention.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_1099 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1099"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.20.attention.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 221
        },
        "bert.encoder.layer.20.attention.self.key.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.20.attention.self.key.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_1076 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1076"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.20.attention.self.key.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 439
        },
        "bert.encoder.layer.20.attention.self.key.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.20.attention.self.key.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_1074 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1074"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.20.attention.self.key.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 490
        },
        "bert.encoder.layer.20.attention.self.query.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.20.attention.self.query.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_1070 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1070"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.20.attention.self.query.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 422
        },
        "bert.encoder.layer.20.attention.self.query.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.20.attention.self.query.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_1068 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1068"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.20.attention.self.query.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 472
        },
        "bert.encoder.layer.20.attention.self.value.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.20.attention.self.value.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_1090 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1090"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.20.attention.self.value.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 339
        },
        "bert.encoder.layer.20.attention.self.value.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.20.attention.self.value.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_1088 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1088"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.20.attention.self.value.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 388
        },
        "bert.encoder.layer.20.intermediate.dense.bias": {
            "cache": {
                "shape": [
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.20.intermediate.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_1109 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1109"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.20.intermediate.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 219
        },
        "bert.encoder.layer.20.intermediate.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.20.intermediate.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_1107 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1107"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.20.intermediate.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 270
        },
        "bert.encoder.layer.20.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.20.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_1118 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_1118"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.20.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 107
        },
        "bert.encoder.layer.20.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.20.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_1118 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_1118"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.20.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 106
        },
        "bert.encoder.layer.20.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.20.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_1115 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1115"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.20.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 145
        },
        "bert.encoder.layer.20.output.dense.weight": {
            "cache": {
                "shape": [
                    4096,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.20.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_1113 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1113"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.20.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 189
        },
        "bert.encoder.layer.21.attention.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.21.attention.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_1157 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_1157"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.21.attention.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 86
        },
        "bert.encoder.layer.21.attention.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.21.attention.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_1157 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_1157"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.21.attention.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 85
        },
        "bert.encoder.layer.21.attention.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.21.attention.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_1154 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1154"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.21.attention.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 115
        },
        "bert.encoder.layer.21.attention.output.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.21.attention.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_1152 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1152"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.21.attention.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 157
        },
        "bert.encoder.layer.21.attention.self.key.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.21.attention.self.key.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_1129 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1129"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.21.attention.self.key.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 369
        },
        "bert.encoder.layer.21.attention.self.key.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.21.attention.self.key.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_1127 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1127"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.21.attention.self.key.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 420
        },
        "bert.encoder.layer.21.attention.self.query.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.21.attention.self.query.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_1123 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1123"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.21.attention.self.query.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 352
        },
        "bert.encoder.layer.21.attention.self.query.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.21.attention.self.query.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_1121 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1121"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.21.attention.self.query.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 402
        },
        "bert.encoder.layer.21.attention.self.value.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.21.attention.self.value.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_1143 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1143"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.21.attention.self.value.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 269
        },
        "bert.encoder.layer.21.attention.self.value.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.21.attention.self.value.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_1141 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1141"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.21.attention.self.value.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 318
        },
        "bert.encoder.layer.21.intermediate.dense.bias": {
            "cache": {
                "shape": [
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.21.intermediate.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_1162 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1162"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.21.intermediate.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 155
        },
        "bert.encoder.layer.21.intermediate.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.21.intermediate.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_1160 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1160"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.21.intermediate.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 201
        },
        "bert.encoder.layer.21.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.21.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_1171 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_1171"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.21.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 67
        },
        "bert.encoder.layer.21.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.21.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_1171 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_1171"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.21.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 66
        },
        "bert.encoder.layer.21.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.21.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_1168 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1168"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.21.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 94
        },
        "bert.encoder.layer.21.output.dense.weight": {
            "cache": {
                "shape": [
                    4096,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.21.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_1166 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1166"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.21.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 126
        },
        "bert.encoder.layer.22.attention.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.22.attention.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_1210 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_1210"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.22.attention.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 51
        },
        "bert.encoder.layer.22.attention.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.22.attention.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_1210 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_1210"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.22.attention.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 50
        },
        "bert.encoder.layer.22.attention.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.22.attention.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_1207 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1207"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.22.attention.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 73
        },
        "bert.encoder.layer.22.attention.output.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.22.attention.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_1205 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1205"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.22.attention.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 102
        },
        "bert.encoder.layer.22.attention.self.key.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.22.attention.self.key.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_1182 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1182"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.22.attention.self.key.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 299
        },
        "bert.encoder.layer.22.attention.self.key.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.22.attention.self.key.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_1180 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1180"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.22.attention.self.key.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 350
        },
        "bert.encoder.layer.22.attention.self.query.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.22.attention.self.query.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_1176 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1176"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.22.attention.self.query.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 282
        },
        "bert.encoder.layer.22.attention.self.query.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.22.attention.self.query.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_1174 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1174"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.22.attention.self.query.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 332
        },
        "bert.encoder.layer.22.attention.self.value.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.22.attention.self.value.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_1196 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1196"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.22.attention.self.value.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 200
        },
        "bert.encoder.layer.22.attention.self.value.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.22.attention.self.value.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_1194 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1194"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.22.attention.self.value.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 248
        },
        "bert.encoder.layer.22.intermediate.dense.bias": {
            "cache": {
                "shape": [
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.22.intermediate.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_1215 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1215"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.22.intermediate.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 100
        },
        "bert.encoder.layer.22.intermediate.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.22.intermediate.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_1213 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1213"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.22.intermediate.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 138
        },
        "bert.encoder.layer.22.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.22.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_1224 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_1224"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.22.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 38
        },
        "bert.encoder.layer.22.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.22.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_1224 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_1224"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.22.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 37
        },
        "bert.encoder.layer.22.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.22.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_1221 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1221"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.22.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 56
        },
        "bert.encoder.layer.22.output.dense.weight": {
            "cache": {
                "shape": [
                    4096,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.22.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_1219 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1219"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.22.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 81
        },
        "bert.encoder.layer.23.attention.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.23.attention.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_1263 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_1263"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.23.attention.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 29
        },
        "bert.encoder.layer.23.attention.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.23.attention.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_1263 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_1263"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.23.attention.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 28
        },
        "bert.encoder.layer.23.attention.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.23.attention.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_1260 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1260"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.23.attention.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 42
        },
        "bert.encoder.layer.23.attention.output.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.23.attention.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_1258 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1258"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.23.attention.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 62
        },
        "bert.encoder.layer.23.attention.self.key.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.23.attention.self.key.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_1235 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1235"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.23.attention.self.key.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 229
        },
        "bert.encoder.layer.23.attention.self.key.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.23.attention.self.key.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_1233 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1233"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.23.attention.self.key.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 280
        },
        "bert.encoder.layer.23.attention.self.query.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.23.attention.self.query.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_1229 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1229"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.23.attention.self.query.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 212
        },
        "bert.encoder.layer.23.attention.self.query.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.23.attention.self.query.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_1227 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1227"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.23.attention.self.query.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 262
        },
        "bert.encoder.layer.23.attention.self.value.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.23.attention.self.value.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_1249 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1249"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.23.attention.self.value.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 137
        },
        "bert.encoder.layer.23.attention.self.value.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.23.attention.self.value.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_1247 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1247"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.23.attention.self.value.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 181
        },
        "bert.encoder.layer.23.intermediate.dense.bias": {
            "cache": {
                "shape": [
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.23.intermediate.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_1268 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1268"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.23.intermediate.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 60
        },
        "bert.encoder.layer.23.intermediate.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.23.intermediate.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_1266 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1266"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.23.intermediate.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 87
        },
        "bert.encoder.layer.23.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.23.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_1277 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_1277"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.23.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 22
        },
        "bert.encoder.layer.23.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.23.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_1277 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_1277"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.23.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 21
        },
        "bert.encoder.layer.23.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.23.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_1274 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1274"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.23.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 31
        },
        "bert.encoder.layer.23.output.dense.weight": {
            "cache": {
                "shape": [
                    4096,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.23.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_1272 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1272"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.23.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 46
        },
        "bert.encoder.layer.3.attention.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.3.attention.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_203 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_203"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.3.attention.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1311
        },
        "bert.encoder.layer.3.attention.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.3.attention.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_203 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_203"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.3.attention.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1310
        },
        "bert.encoder.layer.3.attention.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.3.attention.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_200 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_200"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.3.attention.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1361
        },
        "bert.encoder.layer.3.attention.output.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.3.attention.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_198 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_198"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.3.attention.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1411
        },
        "bert.encoder.layer.3.attention.self.key.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.3.attention.self.key.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_175 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_175"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.3.attention.self.key.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1615
        },
        "bert.encoder.layer.3.attention.self.key.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.3.attention.self.key.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_173 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_173"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.3.attention.self.key.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1646
        },
        "bert.encoder.layer.3.attention.self.query.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.3.attention.self.query.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_169 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_169"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.3.attention.self.query.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1603
        },
        "bert.encoder.layer.3.attention.self.query.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.3.attention.self.query.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_167 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_167"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.3.attention.self.query.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1635
        },
        "bert.encoder.layer.3.attention.self.value.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.3.attention.self.value.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_189 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_189"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.3.attention.self.value.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1529
        },
        "bert.encoder.layer.3.attention.self.value.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.3.attention.self.value.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_187 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_187"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.3.attention.self.value.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1577
        },
        "bert.encoder.layer.3.intermediate.dense.bias": {
            "cache": {
                "shape": [
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.3.intermediate.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_208 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_208"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.3.intermediate.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1409
        },
        "bert.encoder.layer.3.intermediate.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.3.intermediate.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_206 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_206"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.3.intermediate.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1460
        },
        "bert.encoder.layer.3.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.3.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_217 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_217"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.3.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1276
        },
        "bert.encoder.layer.3.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.3.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_217 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_217"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.3.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1275
        },
        "bert.encoder.layer.3.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.3.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_214 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_214"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.3.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1327
        },
        "bert.encoder.layer.3.output.dense.weight": {
            "cache": {
                "shape": [
                    4096,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.3.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_212 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_212"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.3.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1376
        },
        "bert.encoder.layer.4.attention.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.4.attention.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_256 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_256"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.4.attention.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1241
        },
        "bert.encoder.layer.4.attention.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.4.attention.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_256 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_256"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.4.attention.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1240
        },
        "bert.encoder.layer.4.attention.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.4.attention.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_253 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_253"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.4.attention.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1291
        },
        "bert.encoder.layer.4.attention.output.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.4.attention.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_251 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_251"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.4.attention.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1341
        },
        "bert.encoder.layer.4.attention.self.key.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.4.attention.self.key.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_228 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_228"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.4.attention.self.key.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1557
        },
        "bert.encoder.layer.4.attention.self.key.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.4.attention.self.key.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_226 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_226"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.4.attention.self.key.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1601
        },
        "bert.encoder.layer.4.attention.self.query.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.4.attention.self.query.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_222 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_222"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.4.attention.self.query.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1542
        },
        "bert.encoder.layer.4.attention.self.query.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.4.attention.self.query.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_220 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_220"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.4.attention.self.query.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1587
        },
        "bert.encoder.layer.4.attention.self.value.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.4.attention.self.value.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_242 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_242"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.4.attention.self.value.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1459
        },
        "bert.encoder.layer.4.attention.self.value.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.4.attention.self.value.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_240 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_240"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.4.attention.self.value.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1508
        },
        "bert.encoder.layer.4.intermediate.dense.bias": {
            "cache": {
                "shape": [
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.4.intermediate.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_261 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_261"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.4.intermediate.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1339
        },
        "bert.encoder.layer.4.intermediate.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.4.intermediate.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_259 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_259"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.4.intermediate.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1390
        },
        "bert.encoder.layer.4.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.4.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_270 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_270"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.4.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1206
        },
        "bert.encoder.layer.4.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.4.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_270 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_270"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.4.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1205
        },
        "bert.encoder.layer.4.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.4.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_267 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_267"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.4.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1257
        },
        "bert.encoder.layer.4.output.dense.weight": {
            "cache": {
                "shape": [
                    4096,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.4.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_265 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_265"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.4.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1306
        },
        "bert.encoder.layer.5.attention.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.5.attention.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_309 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_309"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.5.attention.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1171
        },
        "bert.encoder.layer.5.attention.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.5.attention.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_309 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_309"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.5.attention.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1170
        },
        "bert.encoder.layer.5.attention.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.5.attention.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_306 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_306"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.5.attention.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1221
        },
        "bert.encoder.layer.5.attention.output.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.5.attention.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_304 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_304"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.5.attention.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1271
        },
        "bert.encoder.layer.5.attention.self.key.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.5.attention.self.key.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_281 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_281"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.5.attention.self.key.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1489
        },
        "bert.encoder.layer.5.attention.self.key.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.5.attention.self.key.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_279 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_279"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.5.attention.self.key.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1540
        },
        "bert.encoder.layer.5.attention.self.query.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.5.attention.self.query.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_275 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_275"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.5.attention.self.query.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1472
        },
        "bert.encoder.layer.5.attention.self.query.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.5.attention.self.query.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_273 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_273"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.5.attention.self.query.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1522
        },
        "bert.encoder.layer.5.attention.self.value.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.5.attention.self.value.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_295 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_295"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.5.attention.self.value.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1389
        },
        "bert.encoder.layer.5.attention.self.value.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.5.attention.self.value.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_293 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_293"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.5.attention.self.value.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1438
        },
        "bert.encoder.layer.5.intermediate.dense.bias": {
            "cache": {
                "shape": [
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.5.intermediate.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_314 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_314"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.5.intermediate.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1269
        },
        "bert.encoder.layer.5.intermediate.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.5.intermediate.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_312 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_312"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.5.intermediate.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1320
        },
        "bert.encoder.layer.5.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.5.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_323 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_323"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.5.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1136
        },
        "bert.encoder.layer.5.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.5.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_323 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_323"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.5.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1135
        },
        "bert.encoder.layer.5.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.5.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_320 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_320"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.5.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1187
        },
        "bert.encoder.layer.5.output.dense.weight": {
            "cache": {
                "shape": [
                    4096,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.5.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_318 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_318"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.5.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1236
        },
        "bert.encoder.layer.6.attention.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.6.attention.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_362 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_362"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.6.attention.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1101
        },
        "bert.encoder.layer.6.attention.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.6.attention.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_362 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_362"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.6.attention.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1100
        },
        "bert.encoder.layer.6.attention.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.6.attention.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_359 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_359"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.6.attention.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1151
        },
        "bert.encoder.layer.6.attention.output.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.6.attention.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_357 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_357"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.6.attention.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1201
        },
        "bert.encoder.layer.6.attention.self.key.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.6.attention.self.key.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_334 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_334"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.6.attention.self.key.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1419
        },
        "bert.encoder.layer.6.attention.self.key.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.6.attention.self.key.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_332 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_332"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.6.attention.self.key.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1470
        },
        "bert.encoder.layer.6.attention.self.query.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.6.attention.self.query.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_328 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_328"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.6.attention.self.query.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1402
        },
        "bert.encoder.layer.6.attention.self.query.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.6.attention.self.query.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_326 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_326"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.6.attention.self.query.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1452
        },
        "bert.encoder.layer.6.attention.self.value.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.6.attention.self.value.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_348 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_348"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.6.attention.self.value.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1319
        },
        "bert.encoder.layer.6.attention.self.value.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.6.attention.self.value.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_346 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_346"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.6.attention.self.value.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1368
        },
        "bert.encoder.layer.6.intermediate.dense.bias": {
            "cache": {
                "shape": [
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.6.intermediate.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_367 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_367"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.6.intermediate.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1199
        },
        "bert.encoder.layer.6.intermediate.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.6.intermediate.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_365 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_365"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.6.intermediate.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1250
        },
        "bert.encoder.layer.6.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.6.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_376 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_376"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.6.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1066
        },
        "bert.encoder.layer.6.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.6.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_376 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_376"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.6.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1065
        },
        "bert.encoder.layer.6.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.6.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_373 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_373"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.6.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1117
        },
        "bert.encoder.layer.6.output.dense.weight": {
            "cache": {
                "shape": [
                    4096,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.6.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_371 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_371"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.6.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1166
        },
        "bert.encoder.layer.7.attention.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.7.attention.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_415 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_415"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.7.attention.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1031
        },
        "bert.encoder.layer.7.attention.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.7.attention.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_415 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_415"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.7.attention.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1030
        },
        "bert.encoder.layer.7.attention.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.7.attention.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_412 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_412"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.7.attention.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1081
        },
        "bert.encoder.layer.7.attention.output.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.7.attention.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_410 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_410"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.7.attention.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1131
        },
        "bert.encoder.layer.7.attention.self.key.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.7.attention.self.key.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_387 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_387"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.7.attention.self.key.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1349
        },
        "bert.encoder.layer.7.attention.self.key.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.7.attention.self.key.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_385 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_385"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.7.attention.self.key.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1400
        },
        "bert.encoder.layer.7.attention.self.query.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.7.attention.self.query.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_381 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_381"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.7.attention.self.query.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1332
        },
        "bert.encoder.layer.7.attention.self.query.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.7.attention.self.query.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_379 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_379"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.7.attention.self.query.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1382
        },
        "bert.encoder.layer.7.attention.self.value.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.7.attention.self.value.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_401 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_401"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.7.attention.self.value.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1249
        },
        "bert.encoder.layer.7.attention.self.value.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.7.attention.self.value.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_399 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_399"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.7.attention.self.value.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1298
        },
        "bert.encoder.layer.7.intermediate.dense.bias": {
            "cache": {
                "shape": [
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.7.intermediate.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_420 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_420"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.7.intermediate.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1129
        },
        "bert.encoder.layer.7.intermediate.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.7.intermediate.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_418 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_418"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.7.intermediate.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1180
        },
        "bert.encoder.layer.7.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.7.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_429 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_429"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.7.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 996
        },
        "bert.encoder.layer.7.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.7.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_429 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_429"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.7.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 995
        },
        "bert.encoder.layer.7.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.7.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_426 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_426"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.7.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1047
        },
        "bert.encoder.layer.7.output.dense.weight": {
            "cache": {
                "shape": [
                    4096,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.7.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_424 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_424"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.7.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1096
        },
        "bert.encoder.layer.8.attention.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.8.attention.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_468 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_468"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.8.attention.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 961
        },
        "bert.encoder.layer.8.attention.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.8.attention.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_468 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_468"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.8.attention.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 960
        },
        "bert.encoder.layer.8.attention.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.8.attention.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_465 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_465"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.8.attention.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1011
        },
        "bert.encoder.layer.8.attention.output.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.8.attention.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_463 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_463"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.8.attention.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1061
        },
        "bert.encoder.layer.8.attention.self.key.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.8.attention.self.key.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_440 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_440"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.8.attention.self.key.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1279
        },
        "bert.encoder.layer.8.attention.self.key.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.8.attention.self.key.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_438 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_438"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.8.attention.self.key.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1330
        },
        "bert.encoder.layer.8.attention.self.query.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.8.attention.self.query.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_434 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_434"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.8.attention.self.query.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1262
        },
        "bert.encoder.layer.8.attention.self.query.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.8.attention.self.query.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_432 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_432"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.8.attention.self.query.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1312
        },
        "bert.encoder.layer.8.attention.self.value.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.8.attention.self.value.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_454 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_454"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.8.attention.self.value.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1179
        },
        "bert.encoder.layer.8.attention.self.value.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.8.attention.self.value.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_452 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_452"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.8.attention.self.value.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1228
        },
        "bert.encoder.layer.8.intermediate.dense.bias": {
            "cache": {
                "shape": [
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.8.intermediate.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_473 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_473"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.8.intermediate.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1059
        },
        "bert.encoder.layer.8.intermediate.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.8.intermediate.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_471 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_471"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.8.intermediate.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1110
        },
        "bert.encoder.layer.8.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.8.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_482 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_482"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.8.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 926
        },
        "bert.encoder.layer.8.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.8.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_482 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_482"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.8.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 925
        },
        "bert.encoder.layer.8.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.8.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_479 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_479"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.8.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 977
        },
        "bert.encoder.layer.8.output.dense.weight": {
            "cache": {
                "shape": [
                    4096,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.8.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_477 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_477"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.8.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1026
        },
        "bert.encoder.layer.9.attention.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.9.attention.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_521 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_521"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.9.attention.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 891
        },
        "bert.encoder.layer.9.attention.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.9.attention.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_521 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_521"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.9.attention.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 890
        },
        "bert.encoder.layer.9.attention.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.9.attention.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_518 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_518"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.9.attention.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 941
        },
        "bert.encoder.layer.9.attention.output.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.9.attention.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_516 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_516"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.9.attention.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 991
        },
        "bert.encoder.layer.9.attention.self.key.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.9.attention.self.key.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_493 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_493"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.9.attention.self.key.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1209
        },
        "bert.encoder.layer.9.attention.self.key.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.9.attention.self.key.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_491 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_491"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.9.attention.self.key.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1260
        },
        "bert.encoder.layer.9.attention.self.query.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.9.attention.self.query.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_487 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_487"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.9.attention.self.query.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1192
        },
        "bert.encoder.layer.9.attention.self.query.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.9.attention.self.query.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_485 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_485"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.9.attention.self.query.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1242
        },
        "bert.encoder.layer.9.attention.self.value.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.9.attention.self.value.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_507 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_507"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.9.attention.self.value.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1109
        },
        "bert.encoder.layer.9.attention.self.value.weight": {
            "cache": {
                "shape": [
                    1024,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.9.attention.self.value.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_505 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_505"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.9.attention.self.value.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1158
        },
        "bert.encoder.layer.9.intermediate.dense.bias": {
            "cache": {
                "shape": [
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.9.intermediate.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_526 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_526"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.9.intermediate.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 989
        },
        "bert.encoder.layer.9.intermediate.dense.weight": {
            "cache": {
                "shape": [
                    1024,
                    4096
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.9.intermediate.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_524 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_524"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.9.intermediate.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1040
        },
        "bert.encoder.layer.9.output.LayerNorm.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.9.output.LayerNorm.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_535 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_535"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.9.output.LayerNorm.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 856
        },
        "bert.encoder.layer.9.output.LayerNorm.weight": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.9.output.LayerNorm.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_535 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_535"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.9.output.LayerNorm.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 855
        },
        "bert.encoder.layer.9.output.dense.bias": {
            "cache": {
                "shape": [
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.9.output.dense.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_532 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_532"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.9.output.dense.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 907
        },
        "bert.encoder.layer.9.output.dense.weight": {
            "cache": {
                "shape": [
                    4096,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "bert.encoder.layer.9.output.dense.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_530 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_530"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "bert.encoder.layer.9.output.dense.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 956
        },
        "bert_large_tt_1.output_reshape_1285": {
            "cache": {
                "shape": [
                    1,
                    384
                ]
            },
            "class": "Output",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [
                "Data: reshape_1285 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_1285": "Data"
            },
            "input_nodes": [
                "reshape_1285"
            ],
            "input_tms": [
                []
            ],
            "is_cross_epoch_type": false,
            "is_saved_intermediate": false,
            "memory_access": "FIFO",
            "name": "bert_large_tt_1.output_reshape_1285",
            "opcode": "Output",
            "outgoing_edge_port_info": [],
            "output_df": "Float32",
            "output_nodes": [],
            "pybuda": 1,
            "queue_type": "output",
            "tags": {},
            "type": "Output",
            "unique_id": 1
        },
        "bert_large_tt_1.output_reshape_1292": {
            "cache": {
                "shape": [
                    1,
                    384
                ]
            },
            "class": "Output",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [
                "Data: reshape_1292 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_1292": "Data"
            },
            "input_nodes": [
                "reshape_1292"
            ],
            "input_tms": [
                []
            ],
            "is_cross_epoch_type": false,
            "is_saved_intermediate": false,
            "memory_access": "FIFO",
            "name": "bert_large_tt_1.output_reshape_1292",
            "opcode": "Output",
            "outgoing_edge_port_info": [],
            "output_df": "Float32",
            "output_nodes": [],
            "pybuda": 1,
            "queue_type": "output",
            "tags": {},
            "type": "Output",
            "unique_id": 2
        },
        "gelu_1004": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "gelu(none,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1003 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1003": "Data"
            },
            "input_nodes": [
                "add_1003"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "gelu_1004",
            "op_type": {
                "attrs": [
                    "none"
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "gelu"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1005 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1005"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/transformers.activations.GELUActivation::intermediate_act_fn",
                "original_op_name": "gelu_1004",
                "original_op_type": "gelu"
            },
            "type": "gelu",
            "unique_id": 325
        },
        "gelu_103": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "gelu(none,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_102 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_102": "Data"
            },
            "input_nodes": [
                "add_102"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "gelu_103",
            "op_type": {
                "attrs": [
                    "none"
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "gelu"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_104 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_104"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/transformers.activations.GELUActivation::intermediate_act_fn",
                "original_op_name": "gelu_103",
                "original_op_type": "gelu"
            },
            "type": "gelu",
            "unique_id": 1515
        },
        "gelu_1057": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "gelu(none,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1056 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1056": "Data"
            },
            "input_nodes": [
                "add_1056"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "gelu_1057",
            "op_type": {
                "attrs": [
                    "none"
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "gelu"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1058 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1058"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/transformers.activations.GELUActivation::intermediate_act_fn",
                "original_op_name": "gelu_1057",
                "original_op_type": "gelu"
            },
            "type": "gelu",
            "unique_id": 255
        },
        "gelu_1110": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "gelu(none,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1109 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1109": "Data"
            },
            "input_nodes": [
                "add_1109"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "gelu_1110",
            "op_type": {
                "attrs": [
                    "none"
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "gelu"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1111 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1111"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/transformers.activations.GELUActivation::intermediate_act_fn",
                "original_op_name": "gelu_1110",
                "original_op_type": "gelu"
            },
            "type": "gelu",
            "unique_id": 188
        },
        "gelu_1163": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "gelu(none,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1162 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1162": "Data"
            },
            "input_nodes": [
                "add_1162"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "gelu_1163",
            "op_type": {
                "attrs": [
                    "none"
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "gelu"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1164 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1164"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/transformers.activations.GELUActivation::intermediate_act_fn",
                "original_op_name": "gelu_1163",
                "original_op_type": "gelu"
            },
            "type": "gelu",
            "unique_id": 125
        },
        "gelu_1216": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "gelu(none,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1215 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1215": "Data"
            },
            "input_nodes": [
                "add_1215"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "gelu_1216",
            "op_type": {
                "attrs": [
                    "none"
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "gelu"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1217 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1217"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/transformers.activations.GELUActivation::intermediate_act_fn",
                "original_op_name": "gelu_1216",
                "original_op_type": "gelu"
            },
            "type": "gelu",
            "unique_id": 80
        },
        "gelu_1269": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "gelu(none,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1268 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1268": "Data"
            },
            "input_nodes": [
                "add_1268"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "gelu_1269",
            "op_type": {
                "attrs": [
                    "none"
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "gelu"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1270 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1270"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/transformers.activations.GELUActivation::intermediate_act_fn",
                "original_op_name": "gelu_1269",
                "original_op_type": "gelu"
            },
            "type": "gelu",
            "unique_id": 45
        },
        "gelu_156": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "gelu(none,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_155 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_155": "Data"
            },
            "input_nodes": [
                "add_155"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "gelu_156",
            "op_type": {
                "attrs": [
                    "none"
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "gelu"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_157 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_157"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/transformers.activations.GELUActivation::intermediate_act_fn",
                "original_op_name": "gelu_156",
                "original_op_type": "gelu"
            },
            "type": "gelu",
            "unique_id": 1445
        },
        "gelu_209": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "gelu(none,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_208 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_208": "Data"
            },
            "input_nodes": [
                "add_208"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "gelu_209",
            "op_type": {
                "attrs": [
                    "none"
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "gelu"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_210 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_210"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/transformers.activations.GELUActivation::intermediate_act_fn",
                "original_op_name": "gelu_209",
                "original_op_type": "gelu"
            },
            "type": "gelu",
            "unique_id": 1375
        },
        "gelu_262": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "gelu(none,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_261 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_261": "Data"
            },
            "input_nodes": [
                "add_261"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "gelu_262",
            "op_type": {
                "attrs": [
                    "none"
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "gelu"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_263 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_263"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/transformers.activations.GELUActivation::intermediate_act_fn",
                "original_op_name": "gelu_262",
                "original_op_type": "gelu"
            },
            "type": "gelu",
            "unique_id": 1305
        },
        "gelu_315": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "gelu(none,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_314 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_314": "Data"
            },
            "input_nodes": [
                "add_314"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "gelu_315",
            "op_type": {
                "attrs": [
                    "none"
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "gelu"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_316 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_316"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/transformers.activations.GELUActivation::intermediate_act_fn",
                "original_op_name": "gelu_315",
                "original_op_type": "gelu"
            },
            "type": "gelu",
            "unique_id": 1235
        },
        "gelu_368": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "gelu(none,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_367 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_367": "Data"
            },
            "input_nodes": [
                "add_367"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "gelu_368",
            "op_type": {
                "attrs": [
                    "none"
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "gelu"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_369 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_369"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/transformers.activations.GELUActivation::intermediate_act_fn",
                "original_op_name": "gelu_368",
                "original_op_type": "gelu"
            },
            "type": "gelu",
            "unique_id": 1165
        },
        "gelu_421": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "gelu(none,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_420 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_420": "Data"
            },
            "input_nodes": [
                "add_420"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "gelu_421",
            "op_type": {
                "attrs": [
                    "none"
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "gelu"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_422 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_422"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/transformers.activations.GELUActivation::intermediate_act_fn",
                "original_op_name": "gelu_421",
                "original_op_type": "gelu"
            },
            "type": "gelu",
            "unique_id": 1095
        },
        "gelu_474": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "gelu(none,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_473 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_473": "Data"
            },
            "input_nodes": [
                "add_473"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "gelu_474",
            "op_type": {
                "attrs": [
                    "none"
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "gelu"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_475 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_475"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/transformers.activations.GELUActivation::intermediate_act_fn",
                "original_op_name": "gelu_474",
                "original_op_type": "gelu"
            },
            "type": "gelu",
            "unique_id": 1025
        },
        "gelu_50": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "gelu(none,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_49 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_49": "Data"
            },
            "input_nodes": [
                "add_49"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "gelu_50",
            "op_type": {
                "attrs": [
                    "none"
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "gelu"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_51 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_51"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/transformers.activations.GELUActivation::intermediate_act_fn",
                "original_op_name": "gelu_50",
                "original_op_type": "gelu"
            },
            "type": "gelu",
            "unique_id": 1584
        },
        "gelu_527": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "gelu(none,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_526 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_526": "Data"
            },
            "input_nodes": [
                "add_526"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "gelu_527",
            "op_type": {
                "attrs": [
                    "none"
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "gelu"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_528 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_528"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/transformers.activations.GELUActivation::intermediate_act_fn",
                "original_op_name": "gelu_527",
                "original_op_type": "gelu"
            },
            "type": "gelu",
            "unique_id": 955
        },
        "gelu_580": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "gelu(none,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_579 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_579": "Data"
            },
            "input_nodes": [
                "add_579"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "gelu_580",
            "op_type": {
                "attrs": [
                    "none"
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "gelu"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_581 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_581"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/transformers.activations.GELUActivation::intermediate_act_fn",
                "original_op_name": "gelu_580",
                "original_op_type": "gelu"
            },
            "type": "gelu",
            "unique_id": 885
        },
        "gelu_633": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "gelu(none,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_632 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_632": "Data"
            },
            "input_nodes": [
                "add_632"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "gelu_633",
            "op_type": {
                "attrs": [
                    "none"
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "gelu"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_634 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_634"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/transformers.activations.GELUActivation::intermediate_act_fn",
                "original_op_name": "gelu_633",
                "original_op_type": "gelu"
            },
            "type": "gelu",
            "unique_id": 815
        },
        "gelu_686": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "gelu(none,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_685 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_685": "Data"
            },
            "input_nodes": [
                "add_685"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "gelu_686",
            "op_type": {
                "attrs": [
                    "none"
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "gelu"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_687 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_687"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/transformers.activations.GELUActivation::intermediate_act_fn",
                "original_op_name": "gelu_686",
                "original_op_type": "gelu"
            },
            "type": "gelu",
            "unique_id": 745
        },
        "gelu_739": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "gelu(none,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_738 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_738": "Data"
            },
            "input_nodes": [
                "add_738"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "gelu_739",
            "op_type": {
                "attrs": [
                    "none"
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "gelu"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_740 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_740"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/transformers.activations.GELUActivation::intermediate_act_fn",
                "original_op_name": "gelu_739",
                "original_op_type": "gelu"
            },
            "type": "gelu",
            "unique_id": 675
        },
        "gelu_792": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "gelu(none,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_791 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_791": "Data"
            },
            "input_nodes": [
                "add_791"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "gelu_792",
            "op_type": {
                "attrs": [
                    "none"
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "gelu"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_793 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_793"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/transformers.activations.GELUActivation::intermediate_act_fn",
                "original_op_name": "gelu_792",
                "original_op_type": "gelu"
            },
            "type": "gelu",
            "unique_id": 605
        },
        "gelu_845": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "gelu(none,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_844 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_844": "Data"
            },
            "input_nodes": [
                "add_844"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "gelu_845",
            "op_type": {
                "attrs": [
                    "none"
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "gelu"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_846 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_846"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/transformers.activations.GELUActivation::intermediate_act_fn",
                "original_op_name": "gelu_845",
                "original_op_type": "gelu"
            },
            "type": "gelu",
            "unique_id": 535
        },
        "gelu_898": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "gelu(none,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_897 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_897": "Data"
            },
            "input_nodes": [
                "add_897"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "gelu_898",
            "op_type": {
                "attrs": [
                    "none"
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "gelu"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_899 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_899"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/transformers.activations.GELUActivation::intermediate_act_fn",
                "original_op_name": "gelu_898",
                "original_op_type": "gelu"
            },
            "type": "gelu",
            "unique_id": 465
        },
        "gelu_951": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "gelu(none,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_950 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_950": "Data"
            },
            "input_nodes": [
                "add_950"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "gelu_951",
            "op_type": {
                "attrs": [
                    "none"
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "gelu"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_952 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_952"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/transformers.activations.GELUActivation::intermediate_act_fn",
                "original_op_name": "gelu_951",
                "original_op_type": "gelu"
            },
            "type": "gelu",
            "unique_id": 395
        },
        "hslice_1018": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1017 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1017": "Data"
            },
            "input_nodes": [
                "add_1017"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_1018",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1019 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1019"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_1018",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 456
        },
        "hslice_1024": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1023 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1023": "Data"
            },
            "input_nodes": [
                "add_1023"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_1024",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1025 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1025"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_1024",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 475
        },
        "hslice_1038": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1037 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1037": "Data"
            },
            "input_nodes": [
                "add_1037"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_1038",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_1039 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_1039"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_1038",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 375
        },
        "hslice_1071": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1070 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1070": "Data"
            },
            "input_nodes": [
                "add_1070"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_1071",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1072 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1072"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_1071",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 386
        },
        "hslice_1077": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1076 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1076": "Data"
            },
            "input_nodes": [
                "add_1076"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_1077",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1078 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1078"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_1077",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 405
        },
        "hslice_1091": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1090 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1090": "Data"
            },
            "input_nodes": [
                "add_1090"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_1091",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_1092 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_1092"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_1091",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 305
        },
        "hslice_1124": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1123 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1123": "Data"
            },
            "input_nodes": [
                "add_1123"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_1124",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1125 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1125"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_1124",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 316
        },
        "hslice_1130": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1129 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1129": "Data"
            },
            "input_nodes": [
                "add_1129"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_1130",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1131 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1131"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_1130",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 335
        },
        "hslice_1144": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1143 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1143": "Data"
            },
            "input_nodes": [
                "add_1143"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_1144",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_1145 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_1145"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_1144",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 235
        },
        "hslice_117": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_116 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_116": "Data"
            },
            "input_nodes": [
                "add_116"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_117",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_118 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_118"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_117",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 1626
        },
        "hslice_1177": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1176 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1176": "Data"
            },
            "input_nodes": [
                "add_1176"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_1177",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1178 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1178"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_1177",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 246
        },
        "hslice_1183": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1182 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1182": "Data"
            },
            "input_nodes": [
                "add_1182"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_1183",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1184 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1184"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_1183",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 265
        },
        "hslice_1197": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1196 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1196": "Data"
            },
            "input_nodes": [
                "add_1196"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_1197",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_1198 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_1198"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_1197",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 169
        },
        "hslice_123": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_122 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_122": "Data"
            },
            "input_nodes": [
                "add_122"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_123",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_124 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_124"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_123",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 1638
        },
        "hslice_1230": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1229 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1229": "Data"
            },
            "input_nodes": [
                "add_1229"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_1230",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1231 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1231"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_1230",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 178
        },
        "hslice_1236": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1235 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1235": "Data"
            },
            "input_nodes": [
                "add_1235"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_1236",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1237 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1237"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_1236",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 196
        },
        "hslice_1250": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1249 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1249": "Data"
            },
            "input_nodes": [
                "add_1249"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_1250",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_1251 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_1251"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_1250",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 109
        },
        "hslice_13": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_12 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_12": "Data"
            },
            "input_nodes": [
                "add_12"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_13",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_14 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_14"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_13",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 1699
        },
        "hslice_137": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_136 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_136": "Data"
            },
            "input_nodes": [
                "add_136"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_137",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_138 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_138"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_137",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 1563
        },
        "hslice_170": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_169 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_169": "Data"
            },
            "input_nodes": [
                "add_169"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_170",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_171 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_171"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_170",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 1575
        },
        "hslice_176": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_175 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_175": "Data"
            },
            "input_nodes": [
                "add_175"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_176",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_177 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_177"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_176",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 1590
        },
        "hslice_190": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_189 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_189": "Data"
            },
            "input_nodes": [
                "add_189"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_190",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_191 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_191"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_190",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 1495
        },
        "hslice_223": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_222 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_222": "Data"
            },
            "input_nodes": [
                "add_222"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_223",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_224 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_224"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_223",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 1506
        },
        "hslice_229": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_228 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_228": "Data"
            },
            "input_nodes": [
                "add_228"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_229",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_230 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_230"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_229",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 1525
        },
        "hslice_243": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_242 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_242": "Data"
            },
            "input_nodes": [
                "add_242"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_243",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_244 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_244"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_243",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 1425
        },
        "hslice_276": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_275 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_275": "Data"
            },
            "input_nodes": [
                "add_275"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_276",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_277 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_277"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_276",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 1436
        },
        "hslice_282": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_281 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_281": "Data"
            },
            "input_nodes": [
                "add_281"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_282",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_283 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_283"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_282",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 1455
        },
        "hslice_296": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_295 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_295": "Data"
            },
            "input_nodes": [
                "add_295"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_296",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_297 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_297"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_296",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 1355
        },
        "hslice_31": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_30 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_30": "Data"
            },
            "input_nodes": [
                "add_30"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_31",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_32 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_32"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_31",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 1662
        },
        "hslice_329": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_328 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_328": "Data"
            },
            "input_nodes": [
                "add_328"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_329",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_330 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_330"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_329",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 1366
        },
        "hslice_335": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_334 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_334": "Data"
            },
            "input_nodes": [
                "add_334"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_335",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_336 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_336"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_335",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 1385
        },
        "hslice_349": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_348 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_348": "Data"
            },
            "input_nodes": [
                "add_348"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_349",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_350 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_350"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_349",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 1285
        },
        "hslice_382": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_381 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_381": "Data"
            },
            "input_nodes": [
                "add_381"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_382",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_383 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_383"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_382",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 1296
        },
        "hslice_388": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_387 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_387": "Data"
            },
            "input_nodes": [
                "add_387"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_388",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_389 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_389"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_388",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 1315
        },
        "hslice_402": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_401 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_401": "Data"
            },
            "input_nodes": [
                "add_401"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_402",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_403 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_403"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_402",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 1215
        },
        "hslice_435": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_434 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_434": "Data"
            },
            "input_nodes": [
                "add_434"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_435",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_436 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_436"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_435",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 1226
        },
        "hslice_441": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_440 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_440": "Data"
            },
            "input_nodes": [
                "add_440"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_441",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_442 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_442"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_441",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 1245
        },
        "hslice_455": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_454 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_454": "Data"
            },
            "input_nodes": [
                "add_454"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_455",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_456 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_456"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_455",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 1145
        },
        "hslice_488": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_487 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_487": "Data"
            },
            "input_nodes": [
                "add_487"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_488",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_489 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_489"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_488",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 1156
        },
        "hslice_494": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_493 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_493": "Data"
            },
            "input_nodes": [
                "add_493"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_494",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_495 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_495"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_494",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 1175
        },
        "hslice_508": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_507 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_507": "Data"
            },
            "input_nodes": [
                "add_507"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_508",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_509 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_509"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_508",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 1075
        },
        "hslice_541": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_540 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_540": "Data"
            },
            "input_nodes": [
                "add_540"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_541",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_542 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_542"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_541",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 1086
        },
        "hslice_547": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_546 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_546": "Data"
            },
            "input_nodes": [
                "add_546"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_547",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_548 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_548"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_547",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 1105
        },
        "hslice_561": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_560 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_560": "Data"
            },
            "input_nodes": [
                "add_560"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_561",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_562 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_562"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_561",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 1005
        },
        "hslice_594": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_593 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_593": "Data"
            },
            "input_nodes": [
                "add_593"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_594",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_595 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_595"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_594",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 1016
        },
        "hslice_600": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_599 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_599": "Data"
            },
            "input_nodes": [
                "add_599"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_600",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_601 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_601"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_600",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 1035
        },
        "hslice_614": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_613 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_613": "Data"
            },
            "input_nodes": [
                "add_613"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_614",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_615 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_615"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_614",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 935
        },
        "hslice_64": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_63 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_63": "Data"
            },
            "input_nodes": [
                "add_63"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_64",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_65 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_65"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_64",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 1665
        },
        "hslice_647": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_646 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_646": "Data"
            },
            "input_nodes": [
                "add_646"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_647",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_648 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_648"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_647",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 946
        },
        "hslice_653": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_652 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_652": "Data"
            },
            "input_nodes": [
                "add_652"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_653",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_654 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_654"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_653",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 965
        },
        "hslice_667": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_666 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_666": "Data"
            },
            "input_nodes": [
                "add_666"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_667",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_668 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_668"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_667",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 865
        },
        "hslice_7": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_6 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_6": "Data"
            },
            "input_nodes": [
                "add_6"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_7",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_8 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_8"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_7",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 1693
        },
        "hslice_70": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_69 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_69": "Data"
            },
            "input_nodes": [
                "add_69"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_70",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_71 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_71"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_70",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 1673
        },
        "hslice_700": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_699 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_699": "Data"
            },
            "input_nodes": [
                "add_699"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_700",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_701 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_701"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_700",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 876
        },
        "hslice_706": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_705 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_705": "Data"
            },
            "input_nodes": [
                "add_705"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_706",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_707 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_707"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_706",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 895
        },
        "hslice_720": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_719 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_719": "Data"
            },
            "input_nodes": [
                "add_719"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_720",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_721 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_721"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_720",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 795
        },
        "hslice_753": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_752 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_752": "Data"
            },
            "input_nodes": [
                "add_752"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_753",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_754 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_754"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_753",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 806
        },
        "hslice_759": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_758 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_758": "Data"
            },
            "input_nodes": [
                "add_758"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_759",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_760 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_760"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_759",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 825
        },
        "hslice_773": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_772 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_772": "Data"
            },
            "input_nodes": [
                "add_772"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_773",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_774 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_774"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_773",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 725
        },
        "hslice_806": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_805 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_805": "Data"
            },
            "input_nodes": [
                "add_805"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_806",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_807 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_807"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_806",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 736
        },
        "hslice_812": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_811 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_811": "Data"
            },
            "input_nodes": [
                "add_811"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_812",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_813 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_813"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_812",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 755
        },
        "hslice_826": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_825 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_825": "Data"
            },
            "input_nodes": [
                "add_825"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_826",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_827 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_827"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_826",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 655
        },
        "hslice_84": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_83 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_83": "Data"
            },
            "input_nodes": [
                "add_83"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_84",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_85 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_85"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_84",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 1621
        },
        "hslice_859": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_858 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_858": "Data"
            },
            "input_nodes": [
                "add_858"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_859",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_860 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_860"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_859",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 666
        },
        "hslice_865": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_864 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_864": "Data"
            },
            "input_nodes": [
                "add_864"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_865",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_866 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_866"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_865",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 685
        },
        "hslice_879": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_878 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_878": "Data"
            },
            "input_nodes": [
                "add_878"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_879",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_880 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_880"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_879",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 585
        },
        "hslice_912": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_911 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_911": "Data"
            },
            "input_nodes": [
                "add_911"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_912",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_913 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_913"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_912",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 596
        },
        "hslice_918": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_917 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_917": "Data"
            },
            "input_nodes": [
                "add_917"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_918",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_919 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_919"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_918",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 615
        },
        "hslice_932": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_931 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_931": "Data"
            },
            "input_nodes": [
                "add_931"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_932",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_933 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_933"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_932",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 515
        },
        "hslice_965": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_964 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_964": "Data"
            },
            "input_nodes": [
                "add_964"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_965",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_966 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_966"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_965",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 526
        },
        "hslice_971": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_970 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_970": "Data"
            },
            "input_nodes": [
                "add_970"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_971",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_972 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_972"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_971",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 545
        },
        "hslice_985": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "hslice(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_984 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_984": "Data"
            },
            "input_nodes": [
                "add_984"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hslice_985",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hslice"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_986 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_986"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "hslice_985",
                "original_op_type": "hslice"
            },
            "type": "hslice",
            "unique_id": 445
        },
        "hstack_1044": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "hstack(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1043 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_1043": "Data"
            },
            "input_nodes": [
                "reshape_1043"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hstack_1044",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hstack"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1046 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1046"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "hstack_1044",
                "original_op_type": "hstack"
            },
            "type": "hstack",
            "unique_id": 274
        },
        "hstack_1097": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "hstack(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1096 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_1096": "Data"
            },
            "input_nodes": [
                "reshape_1096"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hstack_1097",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hstack"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1099 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1099"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "hstack_1097",
                "original_op_type": "hstack"
            },
            "type": "hstack",
            "unique_id": 205
        },
        "hstack_1150": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "hstack(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1149 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_1149": "Data"
            },
            "input_nodes": [
                "reshape_1149"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hstack_1150",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hstack"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1152 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1152"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "hstack_1150",
                "original_op_type": "hstack"
            },
            "type": "hstack",
            "unique_id": 142
        },
        "hstack_1203": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "hstack(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1202 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_1202": "Data"
            },
            "input_nodes": [
                "reshape_1202"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hstack_1203",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hstack"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1205 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1205"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "hstack_1203",
                "original_op_type": "hstack"
            },
            "type": "hstack",
            "unique_id": 91
        },
        "hstack_1256": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "hstack(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1255 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_1255": "Data"
            },
            "input_nodes": [
                "reshape_1255"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hstack_1256",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hstack"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1258 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1258"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "hstack_1256",
                "original_op_type": "hstack"
            },
            "type": "hstack",
            "unique_id": 53
        },
        "hstack_143": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "hstack(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_142 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_142": "Data"
            },
            "input_nodes": [
                "reshape_142"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hstack_143",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hstack"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_145 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_145"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "hstack_143",
                "original_op_type": "hstack"
            },
            "type": "hstack",
            "unique_id": 1464
        },
        "hstack_196": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "hstack(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_195 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_195": "Data"
            },
            "input_nodes": [
                "reshape_195"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hstack_196",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hstack"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_198 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_198"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "hstack_196",
                "original_op_type": "hstack"
            },
            "type": "hstack",
            "unique_id": 1394
        },
        "hstack_249": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "hstack(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_248 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_248": "Data"
            },
            "input_nodes": [
                "reshape_248"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hstack_249",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hstack"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_251 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_251"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "hstack_249",
                "original_op_type": "hstack"
            },
            "type": "hstack",
            "unique_id": 1324
        },
        "hstack_302": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "hstack(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_301 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_301": "Data"
            },
            "input_nodes": [
                "reshape_301"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hstack_302",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hstack"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_304 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_304"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "hstack_302",
                "original_op_type": "hstack"
            },
            "type": "hstack",
            "unique_id": 1254
        },
        "hstack_355": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "hstack(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_354 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_354": "Data"
            },
            "input_nodes": [
                "reshape_354"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hstack_355",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hstack"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_357 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_357"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "hstack_355",
                "original_op_type": "hstack"
            },
            "type": "hstack",
            "unique_id": 1184
        },
        "hstack_37": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "hstack(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_36 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_36": "Data"
            },
            "input_nodes": [
                "reshape_36"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hstack_37",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hstack"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_39 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_39"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "hstack_37",
                "original_op_type": "hstack"
            },
            "type": "hstack",
            "unique_id": 1599
        },
        "hstack_408": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "hstack(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_407 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_407": "Data"
            },
            "input_nodes": [
                "reshape_407"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hstack_408",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hstack"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_410 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_410"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "hstack_408",
                "original_op_type": "hstack"
            },
            "type": "hstack",
            "unique_id": 1114
        },
        "hstack_461": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "hstack(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_460 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_460": "Data"
            },
            "input_nodes": [
                "reshape_460"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hstack_461",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hstack"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_463 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_463"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "hstack_461",
                "original_op_type": "hstack"
            },
            "type": "hstack",
            "unique_id": 1044
        },
        "hstack_514": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "hstack(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_513 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_513": "Data"
            },
            "input_nodes": [
                "reshape_513"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hstack_514",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hstack"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_516 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_516"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "hstack_514",
                "original_op_type": "hstack"
            },
            "type": "hstack",
            "unique_id": 974
        },
        "hstack_567": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "hstack(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_566 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_566": "Data"
            },
            "input_nodes": [
                "reshape_566"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hstack_567",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hstack"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_569 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_569"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "hstack_567",
                "original_op_type": "hstack"
            },
            "type": "hstack",
            "unique_id": 904
        },
        "hstack_620": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "hstack(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_619 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_619": "Data"
            },
            "input_nodes": [
                "reshape_619"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hstack_620",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hstack"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_622 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_622"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "hstack_620",
                "original_op_type": "hstack"
            },
            "type": "hstack",
            "unique_id": 834
        },
        "hstack_673": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "hstack(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_672 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_672": "Data"
            },
            "input_nodes": [
                "reshape_672"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hstack_673",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hstack"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_675 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_675"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "hstack_673",
                "original_op_type": "hstack"
            },
            "type": "hstack",
            "unique_id": 764
        },
        "hstack_726": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "hstack(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_725 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_725": "Data"
            },
            "input_nodes": [
                "reshape_725"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hstack_726",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hstack"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_728 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_728"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "hstack_726",
                "original_op_type": "hstack"
            },
            "type": "hstack",
            "unique_id": 694
        },
        "hstack_779": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "hstack(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_778 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_778": "Data"
            },
            "input_nodes": [
                "reshape_778"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hstack_779",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hstack"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_781 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_781"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "hstack_779",
                "original_op_type": "hstack"
            },
            "type": "hstack",
            "unique_id": 624
        },
        "hstack_832": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "hstack(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_831 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_831": "Data"
            },
            "input_nodes": [
                "reshape_831"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hstack_832",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hstack"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_834 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_834"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "hstack_832",
                "original_op_type": "hstack"
            },
            "type": "hstack",
            "unique_id": 554
        },
        "hstack_885": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "hstack(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_884 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_884": "Data"
            },
            "input_nodes": [
                "reshape_884"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hstack_885",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hstack"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_887 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_887"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "hstack_885",
                "original_op_type": "hstack"
            },
            "type": "hstack",
            "unique_id": 484
        },
        "hstack_90": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "hstack(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_89 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_89": "Data"
            },
            "input_nodes": [
                "reshape_89"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hstack_90",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hstack"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_92 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_92"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "hstack_90",
                "original_op_type": "hstack"
            },
            "type": "hstack",
            "unique_id": 1534
        },
        "hstack_938": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "hstack(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_937 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_937": "Data"
            },
            "input_nodes": [
                "reshape_937"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hstack_938",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hstack"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_940 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_940"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "hstack_938",
                "original_op_type": "hstack"
            },
            "type": "hstack",
            "unique_id": 414
        },
        "hstack_991": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "hstack(16,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_990 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_990": "Data"
            },
            "input_nodes": [
                "reshape_990"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "hstack_991",
            "op_type": {
                "attrs": [
                    16
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "hstack"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_993 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_993"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "hstack_991",
                "original_op_type": "hstack"
            },
            "type": "hstack",
            "unique_id": 344
        },
        "input_0_subtract_21": {
            "cache": {
                "shape": [
                    1
                ]
            },
            "class": "constant",
            "constant_dims": [
                1
            ],
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "input_0_subtract_21",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: subtract_21 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "subtract_21"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": false,
            "tags": {},
            "tile_broadcast": [],
            "type": "Constant",
            "unique_id": 149
        },
        "input_1_multiply_1029": {
            "cache": {
                "shape": [
                    1
                ]
            },
            "class": "constant",
            "constant_dims": [
                1
            ],
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "input_1_multiply_1029",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: multiply_1029 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_1029"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": false,
            "tags": {},
            "tile_broadcast": [],
            "type": "Constant",
            "unique_id": 407
        },
        "input_1_multiply_1082": {
            "cache": {
                "shape": [
                    1
                ]
            },
            "class": "constant",
            "constant_dims": [
                1
            ],
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "input_1_multiply_1082",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: multiply_1082 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_1082"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": false,
            "tags": {},
            "tile_broadcast": [],
            "type": "Constant",
            "unique_id": 337
        },
        "input_1_multiply_1135": {
            "cache": {
                "shape": [
                    1
                ]
            },
            "class": "constant",
            "constant_dims": [
                1
            ],
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "input_1_multiply_1135",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: multiply_1135 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_1135"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": false,
            "tags": {},
            "tile_broadcast": [],
            "type": "Constant",
            "unique_id": 267
        },
        "input_1_multiply_1188": {
            "cache": {
                "shape": [
                    1
                ]
            },
            "class": "constant",
            "constant_dims": [
                1
            ],
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "input_1_multiply_1188",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: multiply_1188 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_1188"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": false,
            "tags": {},
            "tile_broadcast": [],
            "type": "Constant",
            "unique_id": 198
        },
        "input_1_multiply_1241": {
            "cache": {
                "shape": [
                    1
                ]
            },
            "class": "constant",
            "constant_dims": [
                1
            ],
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "input_1_multiply_1241",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: multiply_1241 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_1241"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": false,
            "tags": {},
            "tile_broadcast": [],
            "type": "Constant",
            "unique_id": 133
        },
        "input_1_multiply_128": {
            "cache": {
                "shape": [
                    1
                ]
            },
            "class": "constant",
            "constant_dims": [
                1
            ],
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "input_1_multiply_128",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: multiply_128 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_128"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": false,
            "tags": {},
            "tile_broadcast": [],
            "type": "Constant",
            "unique_id": 1592
        },
        "input_1_multiply_18": {
            "cache": {
                "shape": [
                    1
                ]
            },
            "class": "constant",
            "constant_dims": [
                1
            ],
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "input_1_multiply_18",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: multiply_18 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_18"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": false,
            "tags": {},
            "tile_broadcast": [],
            "type": "Constant",
            "unique_id": 1675
        },
        "input_1_multiply_181": {
            "cache": {
                "shape": [
                    1
                ]
            },
            "class": "constant",
            "constant_dims": [
                1
            ],
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "input_1_multiply_181",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: multiply_181 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_181"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": false,
            "tags": {},
            "tile_broadcast": [],
            "type": "Constant",
            "unique_id": 1527
        },
        "input_1_multiply_22": {
            "cache": {
                "shape": [
                    1
                ]
            },
            "class": "constant",
            "constant_dims": [
                1
            ],
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "input_1_multiply_22",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: multiply_22 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_22"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": false,
            "tags": {},
            "tile_broadcast": [],
            "type": "Constant",
            "unique_id": 135
        },
        "input_1_multiply_234": {
            "cache": {
                "shape": [
                    1
                ]
            },
            "class": "constant",
            "constant_dims": [
                1
            ],
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "input_1_multiply_234",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: multiply_234 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_234"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": false,
            "tags": {},
            "tile_broadcast": [],
            "type": "Constant",
            "unique_id": 1457
        },
        "input_1_multiply_287": {
            "cache": {
                "shape": [
                    1
                ]
            },
            "class": "constant",
            "constant_dims": [
                1
            ],
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "input_1_multiply_287",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: multiply_287 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_287"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": false,
            "tags": {},
            "tile_broadcast": [],
            "type": "Constant",
            "unique_id": 1387
        },
        "input_1_multiply_340": {
            "cache": {
                "shape": [
                    1
                ]
            },
            "class": "constant",
            "constant_dims": [
                1
            ],
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "input_1_multiply_340",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: multiply_340 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_340"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": false,
            "tags": {},
            "tile_broadcast": [],
            "type": "Constant",
            "unique_id": 1317
        },
        "input_1_multiply_393": {
            "cache": {
                "shape": [
                    1
                ]
            },
            "class": "constant",
            "constant_dims": [
                1
            ],
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "input_1_multiply_393",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: multiply_393 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_393"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": false,
            "tags": {},
            "tile_broadcast": [],
            "type": "Constant",
            "unique_id": 1247
        },
        "input_1_multiply_446": {
            "cache": {
                "shape": [
                    1
                ]
            },
            "class": "constant",
            "constant_dims": [
                1
            ],
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "input_1_multiply_446",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: multiply_446 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_446"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": false,
            "tags": {},
            "tile_broadcast": [],
            "type": "Constant",
            "unique_id": 1177
        },
        "input_1_multiply_499": {
            "cache": {
                "shape": [
                    1
                ]
            },
            "class": "constant",
            "constant_dims": [
                1
            ],
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "input_1_multiply_499",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: multiply_499 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_499"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": false,
            "tags": {},
            "tile_broadcast": [],
            "type": "Constant",
            "unique_id": 1107
        },
        "input_1_multiply_552": {
            "cache": {
                "shape": [
                    1
                ]
            },
            "class": "constant",
            "constant_dims": [
                1
            ],
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "input_1_multiply_552",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: multiply_552 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_552"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": false,
            "tags": {},
            "tile_broadcast": [],
            "type": "Constant",
            "unique_id": 1037
        },
        "input_1_multiply_605": {
            "cache": {
                "shape": [
                    1
                ]
            },
            "class": "constant",
            "constant_dims": [
                1
            ],
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "input_1_multiply_605",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: multiply_605 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_605"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": false,
            "tags": {},
            "tile_broadcast": [],
            "type": "Constant",
            "unique_id": 967
        },
        "input_1_multiply_658": {
            "cache": {
                "shape": [
                    1
                ]
            },
            "class": "constant",
            "constant_dims": [
                1
            ],
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "input_1_multiply_658",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: multiply_658 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_658"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": false,
            "tags": {},
            "tile_broadcast": [],
            "type": "Constant",
            "unique_id": 897
        },
        "input_1_multiply_711": {
            "cache": {
                "shape": [
                    1
                ]
            },
            "class": "constant",
            "constant_dims": [
                1
            ],
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "input_1_multiply_711",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: multiply_711 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_711"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": false,
            "tags": {},
            "tile_broadcast": [],
            "type": "Constant",
            "unique_id": 827
        },
        "input_1_multiply_75": {
            "cache": {
                "shape": [
                    1
                ]
            },
            "class": "constant",
            "constant_dims": [
                1
            ],
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "input_1_multiply_75",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: multiply_75 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_75"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": false,
            "tags": {},
            "tile_broadcast": [],
            "type": "Constant",
            "unique_id": 1640
        },
        "input_1_multiply_764": {
            "cache": {
                "shape": [
                    1
                ]
            },
            "class": "constant",
            "constant_dims": [
                1
            ],
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "input_1_multiply_764",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: multiply_764 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_764"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": false,
            "tags": {},
            "tile_broadcast": [],
            "type": "Constant",
            "unique_id": 757
        },
        "input_1_multiply_817": {
            "cache": {
                "shape": [
                    1
                ]
            },
            "class": "constant",
            "constant_dims": [
                1
            ],
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "input_1_multiply_817",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: multiply_817 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_817"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": false,
            "tags": {},
            "tile_broadcast": [],
            "type": "Constant",
            "unique_id": 687
        },
        "input_1_multiply_870": {
            "cache": {
                "shape": [
                    1
                ]
            },
            "class": "constant",
            "constant_dims": [
                1
            ],
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "input_1_multiply_870",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: multiply_870 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_870"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": false,
            "tags": {},
            "tile_broadcast": [],
            "type": "Constant",
            "unique_id": 617
        },
        "input_1_multiply_923": {
            "cache": {
                "shape": [
                    1
                ]
            },
            "class": "constant",
            "constant_dims": [
                1
            ],
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "input_1_multiply_923",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: multiply_923 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_923"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": false,
            "tags": {},
            "tile_broadcast": [],
            "type": "Constant",
            "unique_id": 547
        },
        "input_1_multiply_976": {
            "cache": {
                "shape": [
                    1
                ]
            },
            "class": "constant",
            "constant_dims": [
                1
            ],
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "input_1_multiply_976",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: multiply_976 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_976"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": false,
            "tags": {},
            "tile_broadcast": [],
            "type": "Constant",
            "unique_id": 477
        },
        "layernorm_0": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: pybuda_6_i0 (port_0)",
                "Data: bert.embeddings.LayerNorm.weight (port_1)",
                "Data: bert.embeddings.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "bert.embeddings.LayerNorm.bias": "Data",
                "bert.embeddings.LayerNorm.weight": "Data",
                "pybuda_6_i0": "Data"
            },
            "input_nodes": [
                "pybuda_6_i0",
                "bert.embeddings.LayerNorm.weight",
                "bert.embeddings.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_0",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_1 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_1"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEmbeddings::embeddings/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_0",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 1554
        },
        "layernorm_1012": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1011 (port_0)",
                "Data: bert.encoder.layer.18.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.18.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_1011": "Data",
                "bert.encoder.layer.18.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.18.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_1011",
                "bert.encoder.layer.18.output.LayerNorm.weight",
                "bert.encoder.layer.18.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_1012",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1013 (port_0)",
                "Data: add_1050 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1013",
                "add_1050"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_1012",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 210
        },
        "layernorm_1051": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1050 (port_0)",
                "Data: bert.encoder.layer.19.attention.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.19.attention.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_1050": "Data",
                "bert.encoder.layer.19.attention.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.19.attention.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_1050",
                "bert.encoder.layer.19.attention.output.LayerNorm.weight",
                "bert.encoder.layer.19.attention.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_1051",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1052 (port_0)",
                "Data: add_1064 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1052",
                "add_1064"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_1051",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 177
        },
        "layernorm_1065": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1064 (port_0)",
                "Data: bert.encoder.layer.19.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.19.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_1064": "Data",
                "bert.encoder.layer.19.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.19.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_1064",
                "bert.encoder.layer.19.output.LayerNorm.weight",
                "bert.encoder.layer.19.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_1065",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1066 (port_0)",
                "Data: add_1103 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1066",
                "add_1103"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_1065",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 147
        },
        "layernorm_1104": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1103 (port_0)",
                "Data: bert.encoder.layer.20.attention.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.20.attention.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_1103": "Data",
                "bert.encoder.layer.20.attention.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.20.attention.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_1103",
                "bert.encoder.layer.20.attention.output.LayerNorm.weight",
                "bert.encoder.layer.20.attention.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_1104",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1105 (port_0)",
                "Data: add_1117 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1105",
                "add_1117"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_1104",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 117
        },
        "layernorm_111": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_110 (port_0)",
                "Data: bert.encoder.layer.1.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.1.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_110": "Data",
                "bert.encoder.layer.1.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.1.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_110",
                "bert.encoder.layer.1.output.LayerNorm.weight",
                "bert.encoder.layer.1.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_111",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_112 (port_0)",
                "Data: add_149 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_112",
                "add_149"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_111",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 1399
        },
        "layernorm_1118": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1117 (port_0)",
                "Data: bert.encoder.layer.20.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.20.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_1117": "Data",
                "bert.encoder.layer.20.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.20.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_1117",
                "bert.encoder.layer.20.output.LayerNorm.weight",
                "bert.encoder.layer.20.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_1118",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1119 (port_0)",
                "Data: add_1156 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1119",
                "add_1156"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_1118",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 96
        },
        "layernorm_1157": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1156 (port_0)",
                "Data: bert.encoder.layer.21.attention.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.21.attention.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_1156": "Data",
                "bert.encoder.layer.21.attention.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.21.attention.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_1156",
                "bert.encoder.layer.21.attention.output.LayerNorm.weight",
                "bert.encoder.layer.21.attention.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_1157",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1158 (port_0)",
                "Data: add_1170 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1158",
                "add_1170"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_1157",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 75
        },
        "layernorm_1171": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1170 (port_0)",
                "Data: bert.encoder.layer.21.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.21.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_1170": "Data",
                "bert.encoder.layer.21.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.21.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_1170",
                "bert.encoder.layer.21.output.LayerNorm.weight",
                "bert.encoder.layer.21.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_1171",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1172 (port_0)",
                "Data: add_1209 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1172",
                "add_1209"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_1171",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 58
        },
        "layernorm_1210": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1209 (port_0)",
                "Data: bert.encoder.layer.22.attention.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.22.attention.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_1209": "Data",
                "bert.encoder.layer.22.attention.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.22.attention.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_1209",
                "bert.encoder.layer.22.attention.output.LayerNorm.weight",
                "bert.encoder.layer.22.attention.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_1210",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1211 (port_0)",
                "Data: add_1223 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1211",
                "add_1223"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_1210",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 44
        },
        "layernorm_1224": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1223 (port_0)",
                "Data: bert.encoder.layer.22.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.22.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_1223": "Data",
                "bert.encoder.layer.22.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.22.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_1223",
                "bert.encoder.layer.22.output.LayerNorm.weight",
                "bert.encoder.layer.22.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_1224",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1225 (port_0)",
                "Data: add_1262 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1225",
                "add_1262"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_1224",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 33
        },
        "layernorm_1263": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1262 (port_0)",
                "Data: bert.encoder.layer.23.attention.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.23.attention.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_1262": "Data",
                "bert.encoder.layer.23.attention.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.23.attention.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_1262",
                "bert.encoder.layer.23.attention.output.LayerNorm.weight",
                "bert.encoder.layer.23.attention.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_1263",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1264 (port_0)",
                "Data: add_1276 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1264",
                "add_1276"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_1263",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 25
        },
        "layernorm_1277": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1276 (port_0)",
                "Data: bert.encoder.layer.23.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.23.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_1276": "Data",
                "bert.encoder.layer.23.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.23.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_1276",
                "bert.encoder.layer.23.output.LayerNorm.weight",
                "bert.encoder.layer.23.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_1277",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1278 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1278"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_1277",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 17
        },
        "layernorm_150": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_149 (port_0)",
                "Data: bert.encoder.layer.2.attention.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.2.attention.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_149": "Data",
                "bert.encoder.layer.2.attention.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.2.attention.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_149",
                "bert.encoder.layer.2.attention.output.LayerNorm.weight",
                "bert.encoder.layer.2.attention.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_150",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_151 (port_0)",
                "Data: add_163 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_151",
                "add_163"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_150",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 1363
        },
        "layernorm_164": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_163 (port_0)",
                "Data: bert.encoder.layer.2.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.2.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_163": "Data",
                "bert.encoder.layer.2.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.2.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_163",
                "bert.encoder.layer.2.output.LayerNorm.weight",
                "bert.encoder.layer.2.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_164",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_165 (port_0)",
                "Data: add_202 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_165",
                "add_202"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_164",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 1329
        },
        "layernorm_203": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_202 (port_0)",
                "Data: bert.encoder.layer.3.attention.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.3.attention.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_202": "Data",
                "bert.encoder.layer.3.attention.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.3.attention.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_202",
                "bert.encoder.layer.3.attention.output.LayerNorm.weight",
                "bert.encoder.layer.3.attention.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_203",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_204 (port_0)",
                "Data: add_216 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_204",
                "add_216"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_203",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 1293
        },
        "layernorm_217": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_216 (port_0)",
                "Data: bert.encoder.layer.3.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.3.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_216": "Data",
                "bert.encoder.layer.3.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.3.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_216",
                "bert.encoder.layer.3.output.LayerNorm.weight",
                "bert.encoder.layer.3.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_217",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_218 (port_0)",
                "Data: add_255 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_218",
                "add_255"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_217",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 1259
        },
        "layernorm_256": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_255 (port_0)",
                "Data: bert.encoder.layer.4.attention.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.4.attention.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_255": "Data",
                "bert.encoder.layer.4.attention.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.4.attention.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_255",
                "bert.encoder.layer.4.attention.output.LayerNorm.weight",
                "bert.encoder.layer.4.attention.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_256",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_257 (port_0)",
                "Data: add_269 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_257",
                "add_269"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_256",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 1223
        },
        "layernorm_270": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_269 (port_0)",
                "Data: bert.encoder.layer.4.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.4.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_269": "Data",
                "bert.encoder.layer.4.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.4.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_269",
                "bert.encoder.layer.4.output.LayerNorm.weight",
                "bert.encoder.layer.4.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_270",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_271 (port_0)",
                "Data: add_308 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_271",
                "add_308"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_270",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 1189
        },
        "layernorm_309": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_308 (port_0)",
                "Data: bert.encoder.layer.5.attention.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.5.attention.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_308": "Data",
                "bert.encoder.layer.5.attention.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.5.attention.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_308",
                "bert.encoder.layer.5.attention.output.LayerNorm.weight",
                "bert.encoder.layer.5.attention.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_309",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_310 (port_0)",
                "Data: add_322 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_310",
                "add_322"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_309",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 1153
        },
        "layernorm_323": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_322 (port_0)",
                "Data: bert.encoder.layer.5.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.5.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_322": "Data",
                "bert.encoder.layer.5.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.5.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_322",
                "bert.encoder.layer.5.output.LayerNorm.weight",
                "bert.encoder.layer.5.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_323",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_324 (port_0)",
                "Data: add_361 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_324",
                "add_361"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_323",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 1119
        },
        "layernorm_362": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_361 (port_0)",
                "Data: bert.encoder.layer.6.attention.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.6.attention.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_361": "Data",
                "bert.encoder.layer.6.attention.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.6.attention.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_361",
                "bert.encoder.layer.6.attention.output.LayerNorm.weight",
                "bert.encoder.layer.6.attention.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_362",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_363 (port_0)",
                "Data: add_375 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_363",
                "add_375"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_362",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 1083
        },
        "layernorm_376": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_375 (port_0)",
                "Data: bert.encoder.layer.6.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.6.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_375": "Data",
                "bert.encoder.layer.6.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.6.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_375",
                "bert.encoder.layer.6.output.LayerNorm.weight",
                "bert.encoder.layer.6.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_376",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_377 (port_0)",
                "Data: add_414 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_377",
                "add_414"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_376",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 1049
        },
        "layernorm_415": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_414 (port_0)",
                "Data: bert.encoder.layer.7.attention.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.7.attention.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_414": "Data",
                "bert.encoder.layer.7.attention.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.7.attention.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_414",
                "bert.encoder.layer.7.attention.output.LayerNorm.weight",
                "bert.encoder.layer.7.attention.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_415",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_416 (port_0)",
                "Data: add_428 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_416",
                "add_428"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_415",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 1013
        },
        "layernorm_429": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_428 (port_0)",
                "Data: bert.encoder.layer.7.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.7.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_428": "Data",
                "bert.encoder.layer.7.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.7.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_428",
                "bert.encoder.layer.7.output.LayerNorm.weight",
                "bert.encoder.layer.7.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_429",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_430 (port_0)",
                "Data: add_467 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_430",
                "add_467"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_429",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 979
        },
        "layernorm_44": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_43 (port_0)",
                "Data: bert.encoder.layer.0.attention.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.0.attention.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_43": "Data",
                "bert.encoder.layer.0.attention.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.0.attention.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_43",
                "bert.encoder.layer.0.attention.output.LayerNorm.weight",
                "bert.encoder.layer.0.attention.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_44",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_45 (port_0)",
                "Data: add_57 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_45",
                "add_57"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_44",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 1503
        },
        "layernorm_468": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_467 (port_0)",
                "Data: bert.encoder.layer.8.attention.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.8.attention.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_467": "Data",
                "bert.encoder.layer.8.attention.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.8.attention.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_467",
                "bert.encoder.layer.8.attention.output.LayerNorm.weight",
                "bert.encoder.layer.8.attention.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_468",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_469 (port_0)",
                "Data: add_481 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_469",
                "add_481"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_468",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 943
        },
        "layernorm_482": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_481 (port_0)",
                "Data: bert.encoder.layer.8.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.8.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_481": "Data",
                "bert.encoder.layer.8.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.8.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_481",
                "bert.encoder.layer.8.output.LayerNorm.weight",
                "bert.encoder.layer.8.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_482",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_483 (port_0)",
                "Data: add_520 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_483",
                "add_520"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_482",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 909
        },
        "layernorm_521": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_520 (port_0)",
                "Data: bert.encoder.layer.9.attention.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.9.attention.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_520": "Data",
                "bert.encoder.layer.9.attention.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.9.attention.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_520",
                "bert.encoder.layer.9.attention.output.LayerNorm.weight",
                "bert.encoder.layer.9.attention.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_521",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_522 (port_0)",
                "Data: add_534 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_522",
                "add_534"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_521",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 873
        },
        "layernorm_535": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_534 (port_0)",
                "Data: bert.encoder.layer.9.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.9.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_534": "Data",
                "bert.encoder.layer.9.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.9.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_534",
                "bert.encoder.layer.9.output.LayerNorm.weight",
                "bert.encoder.layer.9.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_535",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_536 (port_0)",
                "Data: add_573 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_536",
                "add_573"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_535",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 839
        },
        "layernorm_574": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_573 (port_0)",
                "Data: bert.encoder.layer.10.attention.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.10.attention.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_573": "Data",
                "bert.encoder.layer.10.attention.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.10.attention.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_573",
                "bert.encoder.layer.10.attention.output.LayerNorm.weight",
                "bert.encoder.layer.10.attention.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_574",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_575 (port_0)",
                "Data: add_587 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_575",
                "add_587"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_574",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 803
        },
        "layernorm_58": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_57 (port_0)",
                "Data: bert.encoder.layer.0.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.0.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_57": "Data",
                "bert.encoder.layer.0.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.0.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_57",
                "bert.encoder.layer.0.output.LayerNorm.weight",
                "bert.encoder.layer.0.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_58",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_59 (port_0)",
                "Data: add_96 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_59",
                "add_96"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_58",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 1469
        },
        "layernorm_588": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_587 (port_0)",
                "Data: bert.encoder.layer.10.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.10.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_587": "Data",
                "bert.encoder.layer.10.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.10.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_587",
                "bert.encoder.layer.10.output.LayerNorm.weight",
                "bert.encoder.layer.10.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_588",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_589 (port_0)",
                "Data: add_626 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_589",
                "add_626"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_588",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 769
        },
        "layernorm_627": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_626 (port_0)",
                "Data: bert.encoder.layer.11.attention.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.11.attention.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_626": "Data",
                "bert.encoder.layer.11.attention.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.11.attention.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_626",
                "bert.encoder.layer.11.attention.output.LayerNorm.weight",
                "bert.encoder.layer.11.attention.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_627",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_628 (port_0)",
                "Data: add_640 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_628",
                "add_640"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_627",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 733
        },
        "layernorm_641": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_640 (port_0)",
                "Data: bert.encoder.layer.11.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.11.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_640": "Data",
                "bert.encoder.layer.11.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.11.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_640",
                "bert.encoder.layer.11.output.LayerNorm.weight",
                "bert.encoder.layer.11.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_641",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_642 (port_0)",
                "Data: add_679 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_642",
                "add_679"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_641",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 699
        },
        "layernorm_680": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_679 (port_0)",
                "Data: bert.encoder.layer.12.attention.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.12.attention.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_679": "Data",
                "bert.encoder.layer.12.attention.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.12.attention.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_679",
                "bert.encoder.layer.12.attention.output.LayerNorm.weight",
                "bert.encoder.layer.12.attention.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_680",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_681 (port_0)",
                "Data: add_693 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_681",
                "add_693"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_680",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 663
        },
        "layernorm_694": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_693 (port_0)",
                "Data: bert.encoder.layer.12.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.12.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_693": "Data",
                "bert.encoder.layer.12.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.12.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_693",
                "bert.encoder.layer.12.output.LayerNorm.weight",
                "bert.encoder.layer.12.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_694",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_695 (port_0)",
                "Data: add_732 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_695",
                "add_732"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_694",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 629
        },
        "layernorm_733": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_732 (port_0)",
                "Data: bert.encoder.layer.13.attention.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.13.attention.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_732": "Data",
                "bert.encoder.layer.13.attention.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.13.attention.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_732",
                "bert.encoder.layer.13.attention.output.LayerNorm.weight",
                "bert.encoder.layer.13.attention.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_733",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_734 (port_0)",
                "Data: add_746 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_734",
                "add_746"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_733",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 593
        },
        "layernorm_747": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_746 (port_0)",
                "Data: bert.encoder.layer.13.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.13.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_746": "Data",
                "bert.encoder.layer.13.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.13.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_746",
                "bert.encoder.layer.13.output.LayerNorm.weight",
                "bert.encoder.layer.13.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_747",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_748 (port_0)",
                "Data: add_785 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_748",
                "add_785"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_747",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 559
        },
        "layernorm_786": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_785 (port_0)",
                "Data: bert.encoder.layer.14.attention.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.14.attention.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_785": "Data",
                "bert.encoder.layer.14.attention.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.14.attention.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_785",
                "bert.encoder.layer.14.attention.output.LayerNorm.weight",
                "bert.encoder.layer.14.attention.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_786",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_787 (port_0)",
                "Data: add_799 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_787",
                "add_799"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_786",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 523
        },
        "layernorm_800": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_799 (port_0)",
                "Data: bert.encoder.layer.14.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.14.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_799": "Data",
                "bert.encoder.layer.14.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.14.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_799",
                "bert.encoder.layer.14.output.LayerNorm.weight",
                "bert.encoder.layer.14.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_800",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_801 (port_0)",
                "Data: add_838 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_801",
                "add_838"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_800",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 489
        },
        "layernorm_839": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_838 (port_0)",
                "Data: bert.encoder.layer.15.attention.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.15.attention.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_838": "Data",
                "bert.encoder.layer.15.attention.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.15.attention.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_838",
                "bert.encoder.layer.15.attention.output.LayerNorm.weight",
                "bert.encoder.layer.15.attention.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_839",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_840 (port_0)",
                "Data: add_852 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_840",
                "add_852"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_839",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 453
        },
        "layernorm_853": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_852 (port_0)",
                "Data: bert.encoder.layer.15.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.15.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_852": "Data",
                "bert.encoder.layer.15.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.15.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_852",
                "bert.encoder.layer.15.output.LayerNorm.weight",
                "bert.encoder.layer.15.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_853",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_854 (port_0)",
                "Data: add_891 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_854",
                "add_891"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_853",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 419
        },
        "layernorm_892": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_891 (port_0)",
                "Data: bert.encoder.layer.16.attention.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.16.attention.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_891": "Data",
                "bert.encoder.layer.16.attention.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.16.attention.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_891",
                "bert.encoder.layer.16.attention.output.LayerNorm.weight",
                "bert.encoder.layer.16.attention.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_892",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_893 (port_0)",
                "Data: add_905 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_893",
                "add_905"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_892",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 383
        },
        "layernorm_906": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_905 (port_0)",
                "Data: bert.encoder.layer.16.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.16.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_905": "Data",
                "bert.encoder.layer.16.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.16.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_905",
                "bert.encoder.layer.16.output.LayerNorm.weight",
                "bert.encoder.layer.16.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_906",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_907 (port_0)",
                "Data: add_944 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_907",
                "add_944"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_906",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 349
        },
        "layernorm_945": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_944 (port_0)",
                "Data: bert.encoder.layer.17.attention.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.17.attention.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_944": "Data",
                "bert.encoder.layer.17.attention.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.17.attention.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_944",
                "bert.encoder.layer.17.attention.output.LayerNorm.weight",
                "bert.encoder.layer.17.attention.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_945",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_946 (port_0)",
                "Data: add_958 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_946",
                "add_958"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_945",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 313
        },
        "layernorm_959": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_958 (port_0)",
                "Data: bert.encoder.layer.17.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.17.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_958": "Data",
                "bert.encoder.layer.17.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.17.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_958",
                "bert.encoder.layer.17.output.LayerNorm.weight",
                "bert.encoder.layer.17.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_959",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_960 (port_0)",
                "Data: add_997 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_960",
                "add_997"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_959",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 279
        },
        "layernorm_97": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_96 (port_0)",
                "Data: bert.encoder.layer.1.attention.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.1.attention.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_96": "Data",
                "bert.encoder.layer.1.attention.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.1.attention.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_96",
                "bert.encoder.layer.1.attention.output.LayerNorm.weight",
                "bert.encoder.layer.1.attention.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_97",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_98 (port_0)",
                "Data: add_110 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_98",
                "add_110"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_97",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 1433
        },
        "layernorm_998": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "layernorm(-1,0.000000,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_997 (port_0)",
                "Data: bert.encoder.layer.18.attention.output.LayerNorm.weight (port_1)",
                "Data: bert.encoder.layer.18.attention.output.LayerNorm.bias (port_2)"
            ],
            "input_node_to_edge_type": {
                "add_997": "Data",
                "bert.encoder.layer.18.attention.output.LayerNorm.bias": "Data",
                "bert.encoder.layer.18.attention.output.LayerNorm.weight": "Data"
            },
            "input_nodes": [
                "add_997",
                "bert.encoder.layer.18.attention.output.LayerNorm.weight",
                "bert.encoder.layer.18.attention.output.LayerNorm.bias"
            ],
            "input_tms": [
                [],
                [],
                []
            ],
            "ir": "pybuda",
            "name": "layernorm_998",
            "op_type": {
                "attrs": [
                    -1,
                    0.0
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "layernorm"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_999 (port_0)",
                "Data: add_1011 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_999",
                "add_1011"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm",
                "original_op_name": "layernorm_998",
                "original_op_type": "layernorm"
            },
            "type": "layernorm",
            "unique_id": 243
        },
        "matmul_10": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_2 (port_0)",
                "Data: bert.encoder.layer.0.attention.self.key.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.0.attention.self.key.weight": "Data",
                "reshape_2": "Data"
            },
            "input_nodes": [
                "reshape_2",
                "bert.encoder.layer.0.attention.self.key.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_10",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_11 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_11"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "matmul_10",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1708
        },
        "matmul_100": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_98 (port_0)",
                "Data: bert.encoder.layer.1.intermediate.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.1.intermediate.dense.weight": "Data",
                "reshape_98": "Data"
            },
            "input_nodes": [
                "reshape_98",
                "bert.encoder.layer.1.intermediate.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_100",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_101 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_101"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_100",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1564
        },
        "matmul_1001": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_999 (port_0)",
                "Data: bert.encoder.layer.18.intermediate.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.18.intermediate.dense.weight": "Data",
                "reshape_999": "Data"
            },
            "input_nodes": [
                "reshape_999",
                "bert.encoder.layer.18.intermediate.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1001",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1002 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1002"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_1001",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 376
        },
        "matmul_1007": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1005 (port_0)",
                "Data: bert.encoder.layer.18.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.18.output.dense.weight": "Data",
                "reshape_1005": "Data"
            },
            "input_nodes": [
                "reshape_1005",
                "bert.encoder.layer.18.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1007",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1008 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1008"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_1007",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 292
        },
        "matmul_1015": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1013 (port_0)",
                "Data: bert.encoder.layer.19.attention.self.query.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.19.attention.self.query.weight": "Data",
                "reshape_1013": "Data"
            },
            "input_nodes": [
                "reshape_1013",
                "bert.encoder.layer.19.attention.self.query.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1015",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1016 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1016"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "matmul_1015",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 507
        },
        "matmul_1021": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1013 (port_0)",
                "Data: bert.encoder.layer.19.attention.self.key.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.19.attention.self.key.weight": "Data",
                "reshape_1013": "Data"
            },
            "input_nodes": [
                "reshape_1013",
                "bert.encoder.layer.19.attention.self.key.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1021",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1022 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1022"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "matmul_1021",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 525
        },
        "matmul_1027": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1019 (port_0)",
                "Data: transpose_1026 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_1019": "Data",
                "transpose_1026": "Data"
            },
            "input_nodes": [
                "reshape_1019",
                "transpose_1026"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1027",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1028 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1028"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_1027",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 424
        },
        "matmul_1035": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1013 (port_0)",
                "Data: bert.encoder.layer.19.attention.self.value.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.19.attention.self.value.weight": "Data",
                "reshape_1013": "Data"
            },
            "input_nodes": [
                "reshape_1013",
                "bert.encoder.layer.19.attention.self.value.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1035",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1036 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1036"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "matmul_1035",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 425
        },
        "matmul_1042": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1033 (port_0)",
                "Data: transpose_1041 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_1033": "Data",
                "transpose_1041": "Data"
            },
            "input_nodes": [
                "reshape_1033",
                "transpose_1041"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1042",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1043 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1043"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_1042",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 307
        },
        "matmul_1046": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hstack_1044 (port_0)",
                "Data: bert.encoder.layer.19.attention.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.19.attention.output.dense.weight": "Data",
                "hstack_1044": "Data"
            },
            "input_nodes": [
                "hstack_1044",
                "bert.encoder.layer.19.attention.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1046",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1047 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1047"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_1046",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 257
        },
        "matmul_1054": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1052 (port_0)",
                "Data: bert.encoder.layer.19.intermediate.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.19.intermediate.dense.weight": "Data",
                "reshape_1052": "Data"
            },
            "input_nodes": [
                "reshape_1052",
                "bert.encoder.layer.19.intermediate.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1054",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1055 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1055"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_1054",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 306
        },
        "matmul_106": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_104 (port_0)",
                "Data: bert.encoder.layer.1.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.1.output.dense.weight": "Data",
                "reshape_104": "Data"
            },
            "input_nodes": [
                "reshape_104",
                "bert.encoder.layer.1.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_106",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_107 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_107"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_106",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1482
        },
        "matmul_1060": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1058 (port_0)",
                "Data: bert.encoder.layer.19.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.19.output.dense.weight": "Data",
                "reshape_1058": "Data"
            },
            "input_nodes": [
                "reshape_1058",
                "bert.encoder.layer.19.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1060",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1061 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1061"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_1060",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 222
        },
        "matmul_1068": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1066 (port_0)",
                "Data: bert.encoder.layer.20.attention.self.query.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.20.attention.self.query.weight": "Data",
                "reshape_1066": "Data"
            },
            "input_nodes": [
                "reshape_1066",
                "bert.encoder.layer.20.attention.self.query.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1068",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1069 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1069"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "matmul_1068",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 437
        },
        "matmul_1074": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1066 (port_0)",
                "Data: bert.encoder.layer.20.attention.self.key.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.20.attention.self.key.weight": "Data",
                "reshape_1066": "Data"
            },
            "input_nodes": [
                "reshape_1066",
                "bert.encoder.layer.20.attention.self.key.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1074",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1075 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1075"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "matmul_1074",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 455
        },
        "matmul_1080": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1072 (port_0)",
                "Data: transpose_1079 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_1072": "Data",
                "transpose_1079": "Data"
            },
            "input_nodes": [
                "reshape_1072",
                "transpose_1079"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1080",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1081 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1081"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_1080",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 354
        },
        "matmul_1088": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1066 (port_0)",
                "Data: bert.encoder.layer.20.attention.self.value.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.20.attention.self.value.weight": "Data",
                "reshape_1066": "Data"
            },
            "input_nodes": [
                "reshape_1066",
                "bert.encoder.layer.20.attention.self.value.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1088",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1089 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1089"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "matmul_1088",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 355
        },
        "matmul_1095": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1086 (port_0)",
                "Data: transpose_1094 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_1086": "Data",
                "transpose_1094": "Data"
            },
            "input_nodes": [
                "reshape_1086",
                "transpose_1094"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1095",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1096 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1096"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_1095",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 237
        },
        "matmul_1099": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hstack_1097 (port_0)",
                "Data: bert.encoder.layer.20.attention.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.20.attention.output.dense.weight": "Data",
                "hstack_1097": "Data"
            },
            "input_nodes": [
                "hstack_1097",
                "bert.encoder.layer.20.attention.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1099",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1100 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1100"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_1099",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 190
        },
        "matmul_1107": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1105 (port_0)",
                "Data: bert.encoder.layer.20.intermediate.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.20.intermediate.dense.weight": "Data",
                "reshape_1105": "Data"
            },
            "input_nodes": [
                "reshape_1105",
                "bert.encoder.layer.20.intermediate.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1107",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1108 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1108"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_1107",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 236
        },
        "matmul_1113": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1111 (port_0)",
                "Data: bert.encoder.layer.20.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.20.output.dense.weight": "Data",
                "reshape_1111": "Data"
            },
            "input_nodes": [
                "reshape_1111",
                "bert.encoder.layer.20.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1113",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1114 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1114"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_1113",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 158
        },
        "matmul_1121": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1119 (port_0)",
                "Data: bert.encoder.layer.21.attention.self.query.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.21.attention.self.query.weight": "Data",
                "reshape_1119": "Data"
            },
            "input_nodes": [
                "reshape_1119",
                "bert.encoder.layer.21.attention.self.query.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1121",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1122 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1122"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "matmul_1121",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 367
        },
        "matmul_1127": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1119 (port_0)",
                "Data: bert.encoder.layer.21.attention.self.key.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.21.attention.self.key.weight": "Data",
                "reshape_1119": "Data"
            },
            "input_nodes": [
                "reshape_1119",
                "bert.encoder.layer.21.attention.self.key.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1127",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1128 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1128"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "matmul_1127",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 385
        },
        "matmul_1133": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1125 (port_0)",
                "Data: transpose_1132 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_1125": "Data",
                "transpose_1132": "Data"
            },
            "input_nodes": [
                "reshape_1125",
                "transpose_1132"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1133",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1134 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1134"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_1133",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 284
        },
        "matmul_114": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_112 (port_0)",
                "Data: bert.encoder.layer.2.attention.self.query.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.2.attention.self.query.weight": "Data",
                "reshape_112": "Data"
            },
            "input_nodes": [
                "reshape_112",
                "bert.encoder.layer.2.attention.self.query.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_114",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_115 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_115"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "matmul_114",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1654
        },
        "matmul_1141": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1119 (port_0)",
                "Data: bert.encoder.layer.21.attention.self.value.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.21.attention.self.value.weight": "Data",
                "reshape_1119": "Data"
            },
            "input_nodes": [
                "reshape_1119",
                "bert.encoder.layer.21.attention.self.value.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1141",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1142 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1142"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "matmul_1141",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 285
        },
        "matmul_1148": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1139 (port_0)",
                "Data: transpose_1147 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_1139": "Data",
                "transpose_1147": "Data"
            },
            "input_nodes": [
                "reshape_1139",
                "transpose_1147"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1148",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1149 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1149"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_1148",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 171
        },
        "matmul_1152": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hstack_1150 (port_0)",
                "Data: bert.encoder.layer.21.attention.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.21.attention.output.dense.weight": "Data",
                "hstack_1150": "Data"
            },
            "input_nodes": [
                "hstack_1150",
                "bert.encoder.layer.21.attention.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1152",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1153 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1153"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_1152",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 127
        },
        "matmul_1160": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1158 (port_0)",
                "Data: bert.encoder.layer.21.intermediate.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.21.intermediate.dense.weight": "Data",
                "reshape_1158": "Data"
            },
            "input_nodes": [
                "reshape_1158",
                "bert.encoder.layer.21.intermediate.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1160",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1161 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1161"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_1160",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 170
        },
        "matmul_1166": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1164 (port_0)",
                "Data: bert.encoder.layer.21.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.21.output.dense.weight": "Data",
                "reshape_1164": "Data"
            },
            "input_nodes": [
                "reshape_1164",
                "bert.encoder.layer.21.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1166",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1167 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1167"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_1166",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 103
        },
        "matmul_1174": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1172 (port_0)",
                "Data: bert.encoder.layer.22.attention.self.query.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.22.attention.self.query.weight": "Data",
                "reshape_1172": "Data"
            },
            "input_nodes": [
                "reshape_1172",
                "bert.encoder.layer.22.attention.self.query.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1174",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1175 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1175"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "matmul_1174",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 297
        },
        "matmul_1180": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1172 (port_0)",
                "Data: bert.encoder.layer.22.attention.self.key.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.22.attention.self.key.weight": "Data",
                "reshape_1172": "Data"
            },
            "input_nodes": [
                "reshape_1172",
                "bert.encoder.layer.22.attention.self.key.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1180",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1181 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1181"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "matmul_1180",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 315
        },
        "matmul_1186": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1178 (port_0)",
                "Data: transpose_1185 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_1178": "Data",
                "transpose_1185": "Data"
            },
            "input_nodes": [
                "reshape_1178",
                "transpose_1185"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1186",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1187 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1187"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_1186",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 214
        },
        "matmul_1194": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1172 (port_0)",
                "Data: bert.encoder.layer.22.attention.self.value.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.22.attention.self.value.weight": "Data",
                "reshape_1172": "Data"
            },
            "input_nodes": [
                "reshape_1172",
                "bert.encoder.layer.22.attention.self.value.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1194",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1195 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1195"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "matmul_1194",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 215
        },
        "matmul_120": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_112 (port_0)",
                "Data: bert.encoder.layer.2.attention.self.key.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.2.attention.self.key.weight": "Data",
                "reshape_112": "Data"
            },
            "input_nodes": [
                "reshape_112",
                "bert.encoder.layer.2.attention.self.key.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_120",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_121 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_121"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "matmul_120",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1664
        },
        "matmul_1201": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1192 (port_0)",
                "Data: transpose_1200 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_1192": "Data",
                "transpose_1200": "Data"
            },
            "input_nodes": [
                "reshape_1192",
                "transpose_1200"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1201",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1202 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1202"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_1201",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 111
        },
        "matmul_1205": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hstack_1203 (port_0)",
                "Data: bert.encoder.layer.22.attention.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.22.attention.output.dense.weight": "Data",
                "hstack_1203": "Data"
            },
            "input_nodes": [
                "hstack_1203",
                "bert.encoder.layer.22.attention.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1205",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1206 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1206"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_1205",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 82
        },
        "matmul_1213": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1211 (port_0)",
                "Data: bert.encoder.layer.22.intermediate.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.22.intermediate.dense.weight": "Data",
                "reshape_1211": "Data"
            },
            "input_nodes": [
                "reshape_1211",
                "bert.encoder.layer.22.intermediate.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1213",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1214 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1214"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_1213",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 110
        },
        "matmul_1219": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1217 (port_0)",
                "Data: bert.encoder.layer.22.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.22.output.dense.weight": "Data",
                "reshape_1217": "Data"
            },
            "input_nodes": [
                "reshape_1217",
                "bert.encoder.layer.22.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1219",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1220 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1220"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_1219",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 63
        },
        "matmul_1227": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1225 (port_0)",
                "Data: bert.encoder.layer.23.attention.self.query.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.23.attention.self.query.weight": "Data",
                "reshape_1225": "Data"
            },
            "input_nodes": [
                "reshape_1225",
                "bert.encoder.layer.23.attention.self.query.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1227",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1228 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1228"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "matmul_1227",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 227
        },
        "matmul_1233": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1225 (port_0)",
                "Data: bert.encoder.layer.23.attention.self.key.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.23.attention.self.key.weight": "Data",
                "reshape_1225": "Data"
            },
            "input_nodes": [
                "reshape_1225",
                "bert.encoder.layer.23.attention.self.key.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1233",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1234 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1234"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "matmul_1233",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 245
        },
        "matmul_1239": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1231 (port_0)",
                "Data: transpose_1238 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_1231": "Data",
                "transpose_1238": "Data"
            },
            "input_nodes": [
                "reshape_1231",
                "transpose_1238"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1239",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1240 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1240"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_1239",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 148
        },
        "matmul_1247": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1225 (port_0)",
                "Data: bert.encoder.layer.23.attention.self.value.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.23.attention.self.value.weight": "Data",
                "reshape_1225": "Data"
            },
            "input_nodes": [
                "reshape_1225",
                "bert.encoder.layer.23.attention.self.value.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1247",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1248 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1248"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "matmul_1247",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 151
        },
        "matmul_1254": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1245 (port_0)",
                "Data: transpose_1253 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_1245": "Data",
                "transpose_1253": "Data"
            },
            "input_nodes": [
                "reshape_1245",
                "transpose_1253"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1254",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1255 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1255"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_1254",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 69
        },
        "matmul_1258": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hstack_1256 (port_0)",
                "Data: bert.encoder.layer.23.attention.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.23.attention.output.dense.weight": "Data",
                "hstack_1256": "Data"
            },
            "input_nodes": [
                "hstack_1256",
                "bert.encoder.layer.23.attention.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1258",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1259 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1259"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_1258",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 47
        },
        "matmul_126": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_118 (port_0)",
                "Data: transpose_125 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_118": "Data",
                "transpose_125": "Data"
            },
            "input_nodes": [
                "reshape_118",
                "transpose_125"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_126",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_127 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_127"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_126",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1605
        },
        "matmul_1266": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1264 (port_0)",
                "Data: bert.encoder.layer.23.intermediate.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.23.intermediate.dense.weight": "Data",
                "reshape_1264": "Data"
            },
            "input_nodes": [
                "reshape_1264",
                "bert.encoder.layer.23.intermediate.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1266",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1267 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1267"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_1266",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 68
        },
        "matmul_1272": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1270 (port_0)",
                "Data: bert.encoder.layer.23.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.23.output.dense.weight": "Data",
                "reshape_1270": "Data"
            },
            "input_nodes": [
                "reshape_1270",
                "bert.encoder.layer.23.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1272",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1273 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1273"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_1272",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 34
        },
        "matmul_1281": {
            "cache": {
                "shape": [
                    384,
                    1
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1278 (port_0)",
                "Data: qa_outputs.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "qa_outputs.weight": "Data",
                "reshape_1278": "Data"
            },
            "input_nodes": [
                "reshape_1278",
                "qa_outputs.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1281",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1283 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1283"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "matmul_1281",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 9
        },
        "matmul_1288": {
            "cache": {
                "shape": [
                    384,
                    1
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1278 (port_0)",
                "Data: qa_outputs.weight_fork_clone19 (port_1)"
            ],
            "input_node_to_edge_type": {
                "qa_outputs.weight_fork_clone19": "Data",
                "reshape_1278": "Data"
            },
            "input_nodes": [
                "reshape_1278",
                "qa_outputs.weight_fork_clone19"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_1288",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1290 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1290"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "matmul_1288",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 11
        },
        "matmul_134": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_112 (port_0)",
                "Data: bert.encoder.layer.2.attention.self.value.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.2.attention.self.value.weight": "Data",
                "reshape_112": "Data"
            },
            "input_nodes": [
                "reshape_112",
                "bert.encoder.layer.2.attention.self.value.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_134",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_135 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_135"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "matmul_134",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1606
        },
        "matmul_141": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_132 (port_0)",
                "Data: transpose_140 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_132": "Data",
                "transpose_140": "Data"
            },
            "input_nodes": [
                "reshape_132",
                "transpose_140"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_141",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_142 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_142"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_141",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1497
        },
        "matmul_145": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hstack_143 (port_0)",
                "Data: bert.encoder.layer.2.attention.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.2.attention.output.dense.weight": "Data",
                "hstack_143": "Data"
            },
            "input_nodes": [
                "hstack_143",
                "bert.encoder.layer.2.attention.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_145",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_146 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_146"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_145",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1447
        },
        "matmul_153": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_151 (port_0)",
                "Data: bert.encoder.layer.2.intermediate.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.2.intermediate.dense.weight": "Data",
                "reshape_151": "Data"
            },
            "input_nodes": [
                "reshape_151",
                "bert.encoder.layer.2.intermediate.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_153",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_154 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_154"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_153",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1496
        },
        "matmul_159": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_157 (port_0)",
                "Data: bert.encoder.layer.2.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.2.output.dense.weight": "Data",
                "reshape_157": "Data"
            },
            "input_nodes": [
                "reshape_157",
                "bert.encoder.layer.2.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_159",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_160 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_160"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_159",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1412
        },
        "matmul_16": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_8 (port_0)",
                "Data: transpose_15 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_8": "Data",
                "transpose_15": "Data"
            },
            "input_nodes": [
                "reshape_8",
                "transpose_15"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_16",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_17 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_17"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_16",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1682
        },
        "matmul_167": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_165 (port_0)",
                "Data: bert.encoder.layer.3.attention.self.query.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.3.attention.self.query.weight": "Data",
                "reshape_165": "Data"
            },
            "input_nodes": [
                "reshape_165",
                "bert.encoder.layer.3.attention.self.query.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_167",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_168 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_168"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "matmul_167",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1613
        },
        "matmul_173": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_165 (port_0)",
                "Data: bert.encoder.layer.3.attention.self.key.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.3.attention.self.key.weight": "Data",
                "reshape_165": "Data"
            },
            "input_nodes": [
                "reshape_165",
                "bert.encoder.layer.3.attention.self.key.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_173",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_174 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_174"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "matmul_173",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1625
        },
        "matmul_179": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_171 (port_0)",
                "Data: transpose_178 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_171": "Data",
                "transpose_178": "Data"
            },
            "input_nodes": [
                "reshape_171",
                "transpose_178"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_179",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_180 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_180"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_179",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1544
        },
        "matmul_187": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_165 (port_0)",
                "Data: bert.encoder.layer.3.attention.self.value.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.3.attention.self.value.weight": "Data",
                "reshape_165": "Data"
            },
            "input_nodes": [
                "reshape_165",
                "bert.encoder.layer.3.attention.self.value.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_187",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_188 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_188"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "matmul_187",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1545
        },
        "matmul_194": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_185 (port_0)",
                "Data: transpose_193 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_185": "Data",
                "transpose_193": "Data"
            },
            "input_nodes": [
                "reshape_185",
                "transpose_193"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_194",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_195 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_195"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_194",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1427
        },
        "matmul_198": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hstack_196 (port_0)",
                "Data: bert.encoder.layer.3.attention.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.3.attention.output.dense.weight": "Data",
                "hstack_196": "Data"
            },
            "input_nodes": [
                "hstack_196",
                "bert.encoder.layer.3.attention.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_198",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_199 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_199"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_198",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1377
        },
        "matmul_206": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_204 (port_0)",
                "Data: bert.encoder.layer.3.intermediate.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.3.intermediate.dense.weight": "Data",
                "reshape_204": "Data"
            },
            "input_nodes": [
                "reshape_204",
                "bert.encoder.layer.3.intermediate.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_206",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_207 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_207"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_206",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1426
        },
        "matmul_212": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_210 (port_0)",
                "Data: bert.encoder.layer.3.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.3.output.dense.weight": "Data",
                "reshape_210": "Data"
            },
            "input_nodes": [
                "reshape_210",
                "bert.encoder.layer.3.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_212",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_213 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_213"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_212",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1342
        },
        "matmul_220": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_218 (port_0)",
                "Data: bert.encoder.layer.4.attention.self.query.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.4.attention.self.query.weight": "Data",
                "reshape_218": "Data"
            },
            "input_nodes": [
                "reshape_218",
                "bert.encoder.layer.4.attention.self.query.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_220",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_221 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_221"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "matmul_220",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1555
        },
        "matmul_226": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_218 (port_0)",
                "Data: bert.encoder.layer.4.attention.self.key.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.4.attention.self.key.weight": "Data",
                "reshape_218": "Data"
            },
            "input_nodes": [
                "reshape_218",
                "bert.encoder.layer.4.attention.self.key.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_226",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_227 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_227"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "matmul_226",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1574
        },
        "matmul_232": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_224 (port_0)",
                "Data: transpose_231 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_224": "Data",
                "transpose_231": "Data"
            },
            "input_nodes": [
                "reshape_224",
                "transpose_231"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_232",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_233 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_233"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_232",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1474
        },
        "matmul_240": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_218 (port_0)",
                "Data: bert.encoder.layer.4.attention.self.value.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.4.attention.self.value.weight": "Data",
                "reshape_218": "Data"
            },
            "input_nodes": [
                "reshape_218",
                "bert.encoder.layer.4.attention.self.value.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_240",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_241 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_241"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "matmul_240",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1475
        },
        "matmul_247": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_238 (port_0)",
                "Data: transpose_246 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_238": "Data",
                "transpose_246": "Data"
            },
            "input_nodes": [
                "reshape_238",
                "transpose_246"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_247",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_248 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_248"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_247",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1357
        },
        "matmul_251": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hstack_249 (port_0)",
                "Data: bert.encoder.layer.4.attention.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.4.attention.output.dense.weight": "Data",
                "hstack_249": "Data"
            },
            "input_nodes": [
                "hstack_249",
                "bert.encoder.layer.4.attention.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_251",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_252 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_252"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_251",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1307
        },
        "matmul_259": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_257 (port_0)",
                "Data: bert.encoder.layer.4.intermediate.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.4.intermediate.dense.weight": "Data",
                "reshape_257": "Data"
            },
            "input_nodes": [
                "reshape_257",
                "bert.encoder.layer.4.intermediate.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_259",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_260 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_260"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_259",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1356
        },
        "matmul_265": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_263 (port_0)",
                "Data: bert.encoder.layer.4.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.4.output.dense.weight": "Data",
                "reshape_263": "Data"
            },
            "input_nodes": [
                "reshape_263",
                "bert.encoder.layer.4.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_265",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_266 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_266"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_265",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1272
        },
        "matmul_273": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_271 (port_0)",
                "Data: bert.encoder.layer.5.attention.self.query.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.5.attention.self.query.weight": "Data",
                "reshape_271": "Data"
            },
            "input_nodes": [
                "reshape_271",
                "bert.encoder.layer.5.attention.self.query.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_273",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_274 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_274"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "matmul_273",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1487
        },
        "matmul_279": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_271 (port_0)",
                "Data: bert.encoder.layer.5.attention.self.key.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.5.attention.self.key.weight": "Data",
                "reshape_271": "Data"
            },
            "input_nodes": [
                "reshape_271",
                "bert.encoder.layer.5.attention.self.key.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_279",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_280 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_280"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "matmul_279",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1505
        },
        "matmul_28": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_2 (port_0)",
                "Data: bert.encoder.layer.0.attention.self.value.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.0.attention.self.value.weight": "Data",
                "reshape_2": "Data"
            },
            "input_nodes": [
                "reshape_2",
                "bert.encoder.layer.0.attention.self.value.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_28",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_29 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_29"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "matmul_28",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1683
        },
        "matmul_285": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_277 (port_0)",
                "Data: transpose_284 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_277": "Data",
                "transpose_284": "Data"
            },
            "input_nodes": [
                "reshape_277",
                "transpose_284"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_285",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_286 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_286"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_285",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1404
        },
        "matmul_293": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_271 (port_0)",
                "Data: bert.encoder.layer.5.attention.self.value.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.5.attention.self.value.weight": "Data",
                "reshape_271": "Data"
            },
            "input_nodes": [
                "reshape_271",
                "bert.encoder.layer.5.attention.self.value.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_293",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_294 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_294"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "matmul_293",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1405
        },
        "matmul_300": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_291 (port_0)",
                "Data: transpose_299 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_291": "Data",
                "transpose_299": "Data"
            },
            "input_nodes": [
                "reshape_291",
                "transpose_299"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_300",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_301 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_301"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_300",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1287
        },
        "matmul_304": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hstack_302 (port_0)",
                "Data: bert.encoder.layer.5.attention.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.5.attention.output.dense.weight": "Data",
                "hstack_302": "Data"
            },
            "input_nodes": [
                "hstack_302",
                "bert.encoder.layer.5.attention.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_304",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_305 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_305"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_304",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1237
        },
        "matmul_312": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_310 (port_0)",
                "Data: bert.encoder.layer.5.intermediate.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.5.intermediate.dense.weight": "Data",
                "reshape_310": "Data"
            },
            "input_nodes": [
                "reshape_310",
                "bert.encoder.layer.5.intermediate.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_312",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_313 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_313"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_312",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1286
        },
        "matmul_318": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_316 (port_0)",
                "Data: bert.encoder.layer.5.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.5.output.dense.weight": "Data",
                "reshape_316": "Data"
            },
            "input_nodes": [
                "reshape_316",
                "bert.encoder.layer.5.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_318",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_319 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_319"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_318",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1202
        },
        "matmul_326": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_324 (port_0)",
                "Data: bert.encoder.layer.6.attention.self.query.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.6.attention.self.query.weight": "Data",
                "reshape_324": "Data"
            },
            "input_nodes": [
                "reshape_324",
                "bert.encoder.layer.6.attention.self.query.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_326",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_327 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_327"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "matmul_326",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1417
        },
        "matmul_332": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_324 (port_0)",
                "Data: bert.encoder.layer.6.attention.self.key.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.6.attention.self.key.weight": "Data",
                "reshape_324": "Data"
            },
            "input_nodes": [
                "reshape_324",
                "bert.encoder.layer.6.attention.self.key.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_332",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_333 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_333"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "matmul_332",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1435
        },
        "matmul_338": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_330 (port_0)",
                "Data: transpose_337 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_330": "Data",
                "transpose_337": "Data"
            },
            "input_nodes": [
                "reshape_330",
                "transpose_337"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_338",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_339 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_339"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_338",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1334
        },
        "matmul_346": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_324 (port_0)",
                "Data: bert.encoder.layer.6.attention.self.value.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.6.attention.self.value.weight": "Data",
                "reshape_324": "Data"
            },
            "input_nodes": [
                "reshape_324",
                "bert.encoder.layer.6.attention.self.value.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_346",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_347 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_347"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "matmul_346",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1335
        },
        "matmul_35": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_26 (port_0)",
                "Data: transpose_34 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_26": "Data",
                "transpose_34": "Data"
            },
            "input_nodes": [
                "reshape_26",
                "transpose_34"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_35",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_36 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_36"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_35",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1623
        },
        "matmul_353": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_344 (port_0)",
                "Data: transpose_352 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_344": "Data",
                "transpose_352": "Data"
            },
            "input_nodes": [
                "reshape_344",
                "transpose_352"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_353",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_354 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_354"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_353",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1217
        },
        "matmul_357": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hstack_355 (port_0)",
                "Data: bert.encoder.layer.6.attention.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.6.attention.output.dense.weight": "Data",
                "hstack_355": "Data"
            },
            "input_nodes": [
                "hstack_355",
                "bert.encoder.layer.6.attention.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_357",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_358 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_358"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_357",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1167
        },
        "matmul_365": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_363 (port_0)",
                "Data: bert.encoder.layer.6.intermediate.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.6.intermediate.dense.weight": "Data",
                "reshape_363": "Data"
            },
            "input_nodes": [
                "reshape_363",
                "bert.encoder.layer.6.intermediate.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_365",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_366 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_366"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_365",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1216
        },
        "matmul_371": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_369 (port_0)",
                "Data: bert.encoder.layer.6.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.6.output.dense.weight": "Data",
                "reshape_369": "Data"
            },
            "input_nodes": [
                "reshape_369",
                "bert.encoder.layer.6.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_371",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_372 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_372"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_371",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1132
        },
        "matmul_379": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_377 (port_0)",
                "Data: bert.encoder.layer.7.attention.self.query.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.7.attention.self.query.weight": "Data",
                "reshape_377": "Data"
            },
            "input_nodes": [
                "reshape_377",
                "bert.encoder.layer.7.attention.self.query.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_379",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_380 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_380"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "matmul_379",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1347
        },
        "matmul_385": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_377 (port_0)",
                "Data: bert.encoder.layer.7.attention.self.key.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.7.attention.self.key.weight": "Data",
                "reshape_377": "Data"
            },
            "input_nodes": [
                "reshape_377",
                "bert.encoder.layer.7.attention.self.key.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_385",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_386 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_386"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "matmul_385",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1365
        },
        "matmul_39": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hstack_37 (port_0)",
                "Data: bert.encoder.layer.0.attention.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.0.attention.output.dense.weight": "Data",
                "hstack_37": "Data"
            },
            "input_nodes": [
                "hstack_37",
                "bert.encoder.layer.0.attention.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_39",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_40 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_40"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_39",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1586
        },
        "matmul_391": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_383 (port_0)",
                "Data: transpose_390 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_383": "Data",
                "transpose_390": "Data"
            },
            "input_nodes": [
                "reshape_383",
                "transpose_390"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_391",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_392 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_392"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_391",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1264
        },
        "matmul_399": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_377 (port_0)",
                "Data: bert.encoder.layer.7.attention.self.value.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.7.attention.self.value.weight": "Data",
                "reshape_377": "Data"
            },
            "input_nodes": [
                "reshape_377",
                "bert.encoder.layer.7.attention.self.value.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_399",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_400 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_400"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "matmul_399",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1265
        },
        "matmul_4": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_2 (port_0)",
                "Data: bert.encoder.layer.0.attention.self.query.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.0.attention.self.query.weight": "Data",
                "reshape_2": "Data"
            },
            "input_nodes": [
                "reshape_2",
                "bert.encoder.layer.0.attention.self.query.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_4",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_5 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_5"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "matmul_4",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1704
        },
        "matmul_406": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_397 (port_0)",
                "Data: transpose_405 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_397": "Data",
                "transpose_405": "Data"
            },
            "input_nodes": [
                "reshape_397",
                "transpose_405"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_406",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_407 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_407"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_406",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1147
        },
        "matmul_410": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hstack_408 (port_0)",
                "Data: bert.encoder.layer.7.attention.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.7.attention.output.dense.weight": "Data",
                "hstack_408": "Data"
            },
            "input_nodes": [
                "hstack_408",
                "bert.encoder.layer.7.attention.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_410",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_411 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_411"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_410",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1097
        },
        "matmul_418": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_416 (port_0)",
                "Data: bert.encoder.layer.7.intermediate.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.7.intermediate.dense.weight": "Data",
                "reshape_416": "Data"
            },
            "input_nodes": [
                "reshape_416",
                "bert.encoder.layer.7.intermediate.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_418",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_419 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_419"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_418",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1146
        },
        "matmul_424": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_422 (port_0)",
                "Data: bert.encoder.layer.7.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.7.output.dense.weight": "Data",
                "reshape_422": "Data"
            },
            "input_nodes": [
                "reshape_422",
                "bert.encoder.layer.7.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_424",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_425 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_425"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_424",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1062
        },
        "matmul_432": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_430 (port_0)",
                "Data: bert.encoder.layer.8.attention.self.query.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.8.attention.self.query.weight": "Data",
                "reshape_430": "Data"
            },
            "input_nodes": [
                "reshape_430",
                "bert.encoder.layer.8.attention.self.query.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_432",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_433 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_433"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "matmul_432",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1277
        },
        "matmul_438": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_430 (port_0)",
                "Data: bert.encoder.layer.8.attention.self.key.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.8.attention.self.key.weight": "Data",
                "reshape_430": "Data"
            },
            "input_nodes": [
                "reshape_430",
                "bert.encoder.layer.8.attention.self.key.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_438",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_439 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_439"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "matmul_438",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1295
        },
        "matmul_444": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_436 (port_0)",
                "Data: transpose_443 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_436": "Data",
                "transpose_443": "Data"
            },
            "input_nodes": [
                "reshape_436",
                "transpose_443"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_444",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_445 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_445"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_444",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1194
        },
        "matmul_452": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_430 (port_0)",
                "Data: bert.encoder.layer.8.attention.self.value.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.8.attention.self.value.weight": "Data",
                "reshape_430": "Data"
            },
            "input_nodes": [
                "reshape_430",
                "bert.encoder.layer.8.attention.self.value.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_452",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_453 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_453"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "matmul_452",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1195
        },
        "matmul_459": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_450 (port_0)",
                "Data: transpose_458 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_450": "Data",
                "transpose_458": "Data"
            },
            "input_nodes": [
                "reshape_450",
                "transpose_458"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_459",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_460 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_460"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_459",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1077
        },
        "matmul_463": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hstack_461 (port_0)",
                "Data: bert.encoder.layer.8.attention.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.8.attention.output.dense.weight": "Data",
                "hstack_461": "Data"
            },
            "input_nodes": [
                "hstack_461",
                "bert.encoder.layer.8.attention.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_463",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_464 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_464"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_463",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1027
        },
        "matmul_47": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_45 (port_0)",
                "Data: bert.encoder.layer.0.intermediate.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.0.intermediate.dense.weight": "Data",
                "reshape_45": "Data"
            },
            "input_nodes": [
                "reshape_45",
                "bert.encoder.layer.0.intermediate.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_47",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_48 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_48"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_47",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1622
        },
        "matmul_471": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_469 (port_0)",
                "Data: bert.encoder.layer.8.intermediate.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.8.intermediate.dense.weight": "Data",
                "reshape_469": "Data"
            },
            "input_nodes": [
                "reshape_469",
                "bert.encoder.layer.8.intermediate.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_471",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_472 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_472"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_471",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1076
        },
        "matmul_477": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_475 (port_0)",
                "Data: bert.encoder.layer.8.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.8.output.dense.weight": "Data",
                "reshape_475": "Data"
            },
            "input_nodes": [
                "reshape_475",
                "bert.encoder.layer.8.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_477",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_478 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_478"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_477",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 992
        },
        "matmul_485": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_483 (port_0)",
                "Data: bert.encoder.layer.9.attention.self.query.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.9.attention.self.query.weight": "Data",
                "reshape_483": "Data"
            },
            "input_nodes": [
                "reshape_483",
                "bert.encoder.layer.9.attention.self.query.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_485",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_486 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_486"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "matmul_485",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1207
        },
        "matmul_491": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_483 (port_0)",
                "Data: bert.encoder.layer.9.attention.self.key.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.9.attention.self.key.weight": "Data",
                "reshape_483": "Data"
            },
            "input_nodes": [
                "reshape_483",
                "bert.encoder.layer.9.attention.self.key.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_491",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_492 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_492"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "matmul_491",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1225
        },
        "matmul_497": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_489 (port_0)",
                "Data: transpose_496 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_489": "Data",
                "transpose_496": "Data"
            },
            "input_nodes": [
                "reshape_489",
                "transpose_496"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_497",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_498 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_498"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_497",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1124
        },
        "matmul_505": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_483 (port_0)",
                "Data: bert.encoder.layer.9.attention.self.value.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.9.attention.self.value.weight": "Data",
                "reshape_483": "Data"
            },
            "input_nodes": [
                "reshape_483",
                "bert.encoder.layer.9.attention.self.value.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_505",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_506 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_506"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "matmul_505",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1125
        },
        "matmul_512": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_503 (port_0)",
                "Data: transpose_511 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_503": "Data",
                "transpose_511": "Data"
            },
            "input_nodes": [
                "reshape_503",
                "transpose_511"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_512",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_513 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_513"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_512",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1007
        },
        "matmul_516": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hstack_514 (port_0)",
                "Data: bert.encoder.layer.9.attention.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.9.attention.output.dense.weight": "Data",
                "hstack_514": "Data"
            },
            "input_nodes": [
                "hstack_514",
                "bert.encoder.layer.9.attention.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_516",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_517 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_517"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_516",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 957
        },
        "matmul_524": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_522 (port_0)",
                "Data: bert.encoder.layer.9.intermediate.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.9.intermediate.dense.weight": "Data",
                "reshape_522": "Data"
            },
            "input_nodes": [
                "reshape_522",
                "bert.encoder.layer.9.intermediate.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_524",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_525 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_525"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_524",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1006
        },
        "matmul_53": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_51 (port_0)",
                "Data: bert.encoder.layer.0.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.0.output.dense.weight": "Data",
                "reshape_51": "Data"
            },
            "input_nodes": [
                "reshape_51",
                "bert.encoder.layer.0.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_53",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_54 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_54"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_53",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1552
        },
        "matmul_530": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_528 (port_0)",
                "Data: bert.encoder.layer.9.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.9.output.dense.weight": "Data",
                "reshape_528": "Data"
            },
            "input_nodes": [
                "reshape_528",
                "bert.encoder.layer.9.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_530",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_531 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_531"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_530",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 922
        },
        "matmul_538": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_536 (port_0)",
                "Data: bert.encoder.layer.10.attention.self.query.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.10.attention.self.query.weight": "Data",
                "reshape_536": "Data"
            },
            "input_nodes": [
                "reshape_536",
                "bert.encoder.layer.10.attention.self.query.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_538",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_539 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_539"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "matmul_538",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1137
        },
        "matmul_544": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_536 (port_0)",
                "Data: bert.encoder.layer.10.attention.self.key.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.10.attention.self.key.weight": "Data",
                "reshape_536": "Data"
            },
            "input_nodes": [
                "reshape_536",
                "bert.encoder.layer.10.attention.self.key.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_544",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_545 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_545"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "matmul_544",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1155
        },
        "matmul_550": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_542 (port_0)",
                "Data: transpose_549 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_542": "Data",
                "transpose_549": "Data"
            },
            "input_nodes": [
                "reshape_542",
                "transpose_549"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_550",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_551 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_551"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_550",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1054
        },
        "matmul_558": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_536 (port_0)",
                "Data: bert.encoder.layer.10.attention.self.value.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.10.attention.self.value.weight": "Data",
                "reshape_536": "Data"
            },
            "input_nodes": [
                "reshape_536",
                "bert.encoder.layer.10.attention.self.value.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_558",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_559 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_559"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "matmul_558",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1055
        },
        "matmul_565": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_556 (port_0)",
                "Data: transpose_564 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_556": "Data",
                "transpose_564": "Data"
            },
            "input_nodes": [
                "reshape_556",
                "transpose_564"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_565",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_566 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_566"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_565",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 937
        },
        "matmul_569": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hstack_567 (port_0)",
                "Data: bert.encoder.layer.10.attention.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.10.attention.output.dense.weight": "Data",
                "hstack_567": "Data"
            },
            "input_nodes": [
                "hstack_567",
                "bert.encoder.layer.10.attention.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_569",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_570 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_570"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_569",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 887
        },
        "matmul_577": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_575 (port_0)",
                "Data: bert.encoder.layer.10.intermediate.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.10.intermediate.dense.weight": "Data",
                "reshape_575": "Data"
            },
            "input_nodes": [
                "reshape_575",
                "bert.encoder.layer.10.intermediate.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_577",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_578 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_578"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_577",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 936
        },
        "matmul_583": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_581 (port_0)",
                "Data: bert.encoder.layer.10.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.10.output.dense.weight": "Data",
                "reshape_581": "Data"
            },
            "input_nodes": [
                "reshape_581",
                "bert.encoder.layer.10.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_583",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_584 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_584"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_583",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 852
        },
        "matmul_591": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_589 (port_0)",
                "Data: bert.encoder.layer.11.attention.self.query.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.11.attention.self.query.weight": "Data",
                "reshape_589": "Data"
            },
            "input_nodes": [
                "reshape_589",
                "bert.encoder.layer.11.attention.self.query.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_591",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_592 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_592"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "matmul_591",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1067
        },
        "matmul_597": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_589 (port_0)",
                "Data: bert.encoder.layer.11.attention.self.key.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.11.attention.self.key.weight": "Data",
                "reshape_589": "Data"
            },
            "input_nodes": [
                "reshape_589",
                "bert.encoder.layer.11.attention.self.key.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_597",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_598 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_598"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "matmul_597",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1085
        },
        "matmul_603": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_595 (port_0)",
                "Data: transpose_602 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_595": "Data",
                "transpose_602": "Data"
            },
            "input_nodes": [
                "reshape_595",
                "transpose_602"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_603",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_604 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_604"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_603",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 984
        },
        "matmul_61": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_59 (port_0)",
                "Data: bert.encoder.layer.1.attention.self.query.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.1.attention.self.query.weight": "Data",
                "reshape_59": "Data"
            },
            "input_nodes": [
                "reshape_59",
                "bert.encoder.layer.1.attention.self.query.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_61",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_62 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_62"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "matmul_61",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1684
        },
        "matmul_611": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_589 (port_0)",
                "Data: bert.encoder.layer.11.attention.self.value.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.11.attention.self.value.weight": "Data",
                "reshape_589": "Data"
            },
            "input_nodes": [
                "reshape_589",
                "bert.encoder.layer.11.attention.self.value.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_611",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_612 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_612"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "matmul_611",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 985
        },
        "matmul_618": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_609 (port_0)",
                "Data: transpose_617 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_609": "Data",
                "transpose_617": "Data"
            },
            "input_nodes": [
                "reshape_609",
                "transpose_617"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_618",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_619 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_619"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_618",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 867
        },
        "matmul_622": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hstack_620 (port_0)",
                "Data: bert.encoder.layer.11.attention.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.11.attention.output.dense.weight": "Data",
                "hstack_620": "Data"
            },
            "input_nodes": [
                "hstack_620",
                "bert.encoder.layer.11.attention.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_622",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_623 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_623"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_622",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 817
        },
        "matmul_630": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_628 (port_0)",
                "Data: bert.encoder.layer.11.intermediate.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.11.intermediate.dense.weight": "Data",
                "reshape_628": "Data"
            },
            "input_nodes": [
                "reshape_628",
                "bert.encoder.layer.11.intermediate.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_630",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_631 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_631"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_630",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 866
        },
        "matmul_636": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_634 (port_0)",
                "Data: bert.encoder.layer.11.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.11.output.dense.weight": "Data",
                "reshape_634": "Data"
            },
            "input_nodes": [
                "reshape_634",
                "bert.encoder.layer.11.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_636",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_637 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_637"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_636",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 782
        },
        "matmul_644": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_642 (port_0)",
                "Data: bert.encoder.layer.12.attention.self.query.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.12.attention.self.query.weight": "Data",
                "reshape_642": "Data"
            },
            "input_nodes": [
                "reshape_642",
                "bert.encoder.layer.12.attention.self.query.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_644",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_645 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_645"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "matmul_644",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 997
        },
        "matmul_650": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_642 (port_0)",
                "Data: bert.encoder.layer.12.attention.self.key.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.12.attention.self.key.weight": "Data",
                "reshape_642": "Data"
            },
            "input_nodes": [
                "reshape_642",
                "bert.encoder.layer.12.attention.self.key.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_650",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_651 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_651"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "matmul_650",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1015
        },
        "matmul_656": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_648 (port_0)",
                "Data: transpose_655 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_648": "Data",
                "transpose_655": "Data"
            },
            "input_nodes": [
                "reshape_648",
                "transpose_655"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_656",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_657 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_657"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_656",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 914
        },
        "matmul_664": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_642 (port_0)",
                "Data: bert.encoder.layer.12.attention.self.value.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.12.attention.self.value.weight": "Data",
                "reshape_642": "Data"
            },
            "input_nodes": [
                "reshape_642",
                "bert.encoder.layer.12.attention.self.value.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_664",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_665 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_665"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "matmul_664",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 915
        },
        "matmul_67": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_59 (port_0)",
                "Data: bert.encoder.layer.1.attention.self.key.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.1.attention.self.key.weight": "Data",
                "reshape_59": "Data"
            },
            "input_nodes": [
                "reshape_59",
                "bert.encoder.layer.1.attention.self.key.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_67",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_68 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_68"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "matmul_67",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1692
        },
        "matmul_671": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_662 (port_0)",
                "Data: transpose_670 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_662": "Data",
                "transpose_670": "Data"
            },
            "input_nodes": [
                "reshape_662",
                "transpose_670"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_671",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_672 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_672"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_671",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 797
        },
        "matmul_675": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hstack_673 (port_0)",
                "Data: bert.encoder.layer.12.attention.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.12.attention.output.dense.weight": "Data",
                "hstack_673": "Data"
            },
            "input_nodes": [
                "hstack_673",
                "bert.encoder.layer.12.attention.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_675",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_676 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_676"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_675",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 747
        },
        "matmul_683": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_681 (port_0)",
                "Data: bert.encoder.layer.12.intermediate.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.12.intermediate.dense.weight": "Data",
                "reshape_681": "Data"
            },
            "input_nodes": [
                "reshape_681",
                "bert.encoder.layer.12.intermediate.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_683",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_684 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_684"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_683",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 796
        },
        "matmul_689": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_687 (port_0)",
                "Data: bert.encoder.layer.12.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.12.output.dense.weight": "Data",
                "reshape_687": "Data"
            },
            "input_nodes": [
                "reshape_687",
                "bert.encoder.layer.12.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_689",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_690 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_690"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_689",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 712
        },
        "matmul_697": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_695 (port_0)",
                "Data: bert.encoder.layer.13.attention.self.query.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.13.attention.self.query.weight": "Data",
                "reshape_695": "Data"
            },
            "input_nodes": [
                "reshape_695",
                "bert.encoder.layer.13.attention.self.query.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_697",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_698 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_698"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "matmul_697",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 927
        },
        "matmul_703": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_695 (port_0)",
                "Data: bert.encoder.layer.13.attention.self.key.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.13.attention.self.key.weight": "Data",
                "reshape_695": "Data"
            },
            "input_nodes": [
                "reshape_695",
                "bert.encoder.layer.13.attention.self.key.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_703",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_704 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_704"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "matmul_703",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 945
        },
        "matmul_709": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_701 (port_0)",
                "Data: transpose_708 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_701": "Data",
                "transpose_708": "Data"
            },
            "input_nodes": [
                "reshape_701",
                "transpose_708"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_709",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_710 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_710"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_709",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 844
        },
        "matmul_717": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_695 (port_0)",
                "Data: bert.encoder.layer.13.attention.self.value.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.13.attention.self.value.weight": "Data",
                "reshape_695": "Data"
            },
            "input_nodes": [
                "reshape_695",
                "bert.encoder.layer.13.attention.self.value.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_717",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_718 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_718"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "matmul_717",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 845
        },
        "matmul_724": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_715 (port_0)",
                "Data: transpose_723 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_715": "Data",
                "transpose_723": "Data"
            },
            "input_nodes": [
                "reshape_715",
                "transpose_723"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_724",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_725 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_725"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_724",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 727
        },
        "matmul_728": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hstack_726 (port_0)",
                "Data: bert.encoder.layer.13.attention.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.13.attention.output.dense.weight": "Data",
                "hstack_726": "Data"
            },
            "input_nodes": [
                "hstack_726",
                "bert.encoder.layer.13.attention.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_728",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_729 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_729"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_728",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 677
        },
        "matmul_73": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_65 (port_0)",
                "Data: transpose_72 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_65": "Data",
                "transpose_72": "Data"
            },
            "input_nodes": [
                "reshape_65",
                "transpose_72"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_73",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_74 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_74"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_73",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1650
        },
        "matmul_736": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_734 (port_0)",
                "Data: bert.encoder.layer.13.intermediate.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.13.intermediate.dense.weight": "Data",
                "reshape_734": "Data"
            },
            "input_nodes": [
                "reshape_734",
                "bert.encoder.layer.13.intermediate.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_736",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_737 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_737"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_736",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 726
        },
        "matmul_742": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_740 (port_0)",
                "Data: bert.encoder.layer.13.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.13.output.dense.weight": "Data",
                "reshape_740": "Data"
            },
            "input_nodes": [
                "reshape_740",
                "bert.encoder.layer.13.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_742",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_743 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_743"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_742",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 642
        },
        "matmul_750": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_748 (port_0)",
                "Data: bert.encoder.layer.14.attention.self.query.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.14.attention.self.query.weight": "Data",
                "reshape_748": "Data"
            },
            "input_nodes": [
                "reshape_748",
                "bert.encoder.layer.14.attention.self.query.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_750",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_751 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_751"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "matmul_750",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 857
        },
        "matmul_756": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_748 (port_0)",
                "Data: bert.encoder.layer.14.attention.self.key.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.14.attention.self.key.weight": "Data",
                "reshape_748": "Data"
            },
            "input_nodes": [
                "reshape_748",
                "bert.encoder.layer.14.attention.self.key.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_756",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_757 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_757"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "matmul_756",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 875
        },
        "matmul_762": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_754 (port_0)",
                "Data: transpose_761 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_754": "Data",
                "transpose_761": "Data"
            },
            "input_nodes": [
                "reshape_754",
                "transpose_761"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_762",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_763 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_763"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_762",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 774
        },
        "matmul_770": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_748 (port_0)",
                "Data: bert.encoder.layer.14.attention.self.value.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.14.attention.self.value.weight": "Data",
                "reshape_748": "Data"
            },
            "input_nodes": [
                "reshape_748",
                "bert.encoder.layer.14.attention.self.value.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_770",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_771 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_771"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "matmul_770",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 775
        },
        "matmul_777": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_768 (port_0)",
                "Data: transpose_776 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_768": "Data",
                "transpose_776": "Data"
            },
            "input_nodes": [
                "reshape_768",
                "transpose_776"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_777",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_778 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_778"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_777",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 657
        },
        "matmul_781": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hstack_779 (port_0)",
                "Data: bert.encoder.layer.14.attention.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.14.attention.output.dense.weight": "Data",
                "hstack_779": "Data"
            },
            "input_nodes": [
                "hstack_779",
                "bert.encoder.layer.14.attention.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_781",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_782 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_782"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_781",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 607
        },
        "matmul_789": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_787 (port_0)",
                "Data: bert.encoder.layer.14.intermediate.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.14.intermediate.dense.weight": "Data",
                "reshape_787": "Data"
            },
            "input_nodes": [
                "reshape_787",
                "bert.encoder.layer.14.intermediate.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_789",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_790 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_790"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_789",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 656
        },
        "matmul_795": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_793 (port_0)",
                "Data: bert.encoder.layer.14.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.14.output.dense.weight": "Data",
                "reshape_793": "Data"
            },
            "input_nodes": [
                "reshape_793",
                "bert.encoder.layer.14.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_795",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_796 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_796"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_795",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 572
        },
        "matmul_803": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_801 (port_0)",
                "Data: bert.encoder.layer.15.attention.self.query.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.15.attention.self.query.weight": "Data",
                "reshape_801": "Data"
            },
            "input_nodes": [
                "reshape_801",
                "bert.encoder.layer.15.attention.self.query.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_803",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_804 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_804"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "matmul_803",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 787
        },
        "matmul_809": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_801 (port_0)",
                "Data: bert.encoder.layer.15.attention.self.key.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.15.attention.self.key.weight": "Data",
                "reshape_801": "Data"
            },
            "input_nodes": [
                "reshape_801",
                "bert.encoder.layer.15.attention.self.key.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_809",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_810 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_810"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "matmul_809",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 805
        },
        "matmul_81": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_59 (port_0)",
                "Data: bert.encoder.layer.1.attention.self.value.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.1.attention.self.value.weight": "Data",
                "reshape_59": "Data"
            },
            "input_nodes": [
                "reshape_59",
                "bert.encoder.layer.1.attention.self.value.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_81",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_82 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_82"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "matmul_81",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1651
        },
        "matmul_815": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_807 (port_0)",
                "Data: transpose_814 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_807": "Data",
                "transpose_814": "Data"
            },
            "input_nodes": [
                "reshape_807",
                "transpose_814"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_815",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_816 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_816"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_815",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 704
        },
        "matmul_823": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_801 (port_0)",
                "Data: bert.encoder.layer.15.attention.self.value.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.15.attention.self.value.weight": "Data",
                "reshape_801": "Data"
            },
            "input_nodes": [
                "reshape_801",
                "bert.encoder.layer.15.attention.self.value.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_823",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_824 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_824"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "matmul_823",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 705
        },
        "matmul_830": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_821 (port_0)",
                "Data: transpose_829 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_821": "Data",
                "transpose_829": "Data"
            },
            "input_nodes": [
                "reshape_821",
                "transpose_829"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_830",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_831 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_831"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_830",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 587
        },
        "matmul_834": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hstack_832 (port_0)",
                "Data: bert.encoder.layer.15.attention.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.15.attention.output.dense.weight": "Data",
                "hstack_832": "Data"
            },
            "input_nodes": [
                "hstack_832",
                "bert.encoder.layer.15.attention.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_834",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_835 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_835"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_834",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 537
        },
        "matmul_842": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_840 (port_0)",
                "Data: bert.encoder.layer.15.intermediate.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.15.intermediate.dense.weight": "Data",
                "reshape_840": "Data"
            },
            "input_nodes": [
                "reshape_840",
                "bert.encoder.layer.15.intermediate.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_842",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_843 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_843"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_842",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 586
        },
        "matmul_848": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_846 (port_0)",
                "Data: bert.encoder.layer.15.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.15.output.dense.weight": "Data",
                "reshape_846": "Data"
            },
            "input_nodes": [
                "reshape_846",
                "bert.encoder.layer.15.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_848",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_849 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_849"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_848",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 502
        },
        "matmul_856": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_854 (port_0)",
                "Data: bert.encoder.layer.16.attention.self.query.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.16.attention.self.query.weight": "Data",
                "reshape_854": "Data"
            },
            "input_nodes": [
                "reshape_854",
                "bert.encoder.layer.16.attention.self.query.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_856",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_857 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_857"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "matmul_856",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 717
        },
        "matmul_862": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_854 (port_0)",
                "Data: bert.encoder.layer.16.attention.self.key.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.16.attention.self.key.weight": "Data",
                "reshape_854": "Data"
            },
            "input_nodes": [
                "reshape_854",
                "bert.encoder.layer.16.attention.self.key.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_862",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_863 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_863"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "matmul_862",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 735
        },
        "matmul_868": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_860 (port_0)",
                "Data: transpose_867 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_860": "Data",
                "transpose_867": "Data"
            },
            "input_nodes": [
                "reshape_860",
                "transpose_867"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_868",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_869 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_869"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_868",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 634
        },
        "matmul_876": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_854 (port_0)",
                "Data: bert.encoder.layer.16.attention.self.value.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.16.attention.self.value.weight": "Data",
                "reshape_854": "Data"
            },
            "input_nodes": [
                "reshape_854",
                "bert.encoder.layer.16.attention.self.value.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_876",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_877 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_877"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "matmul_876",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 635
        },
        "matmul_88": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_79 (port_0)",
                "Data: transpose_87 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_79": "Data",
                "transpose_87": "Data"
            },
            "input_nodes": [
                "reshape_79",
                "transpose_87"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_88",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_89 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_89"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_88",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1565
        },
        "matmul_883": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_874 (port_0)",
                "Data: transpose_882 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_874": "Data",
                "transpose_882": "Data"
            },
            "input_nodes": [
                "reshape_874",
                "transpose_882"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_883",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_884 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_884"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_883",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 517
        },
        "matmul_887": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hstack_885 (port_0)",
                "Data: bert.encoder.layer.16.attention.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.16.attention.output.dense.weight": "Data",
                "hstack_885": "Data"
            },
            "input_nodes": [
                "hstack_885",
                "bert.encoder.layer.16.attention.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_887",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_888 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_888"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_887",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 467
        },
        "matmul_895": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_893 (port_0)",
                "Data: bert.encoder.layer.16.intermediate.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.16.intermediate.dense.weight": "Data",
                "reshape_893": "Data"
            },
            "input_nodes": [
                "reshape_893",
                "bert.encoder.layer.16.intermediate.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_895",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_896 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_896"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_895",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 516
        },
        "matmul_901": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_899 (port_0)",
                "Data: bert.encoder.layer.16.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.16.output.dense.weight": "Data",
                "reshape_899": "Data"
            },
            "input_nodes": [
                "reshape_899",
                "bert.encoder.layer.16.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_901",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_902 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_902"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_901",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 432
        },
        "matmul_909": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_907 (port_0)",
                "Data: bert.encoder.layer.17.attention.self.query.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.17.attention.self.query.weight": "Data",
                "reshape_907": "Data"
            },
            "input_nodes": [
                "reshape_907",
                "bert.encoder.layer.17.attention.self.query.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_909",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_910 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_910"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "matmul_909",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 647
        },
        "matmul_915": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_907 (port_0)",
                "Data: bert.encoder.layer.17.attention.self.key.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.17.attention.self.key.weight": "Data",
                "reshape_907": "Data"
            },
            "input_nodes": [
                "reshape_907",
                "bert.encoder.layer.17.attention.self.key.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_915",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_916 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_916"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "matmul_915",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 665
        },
        "matmul_92": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hstack_90 (port_0)",
                "Data: bert.encoder.layer.1.attention.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.1.attention.output.dense.weight": "Data",
                "hstack_90": "Data"
            },
            "input_nodes": [
                "hstack_90",
                "bert.encoder.layer.1.attention.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_92",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_93 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_93"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_92",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 1517
        },
        "matmul_921": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_913 (port_0)",
                "Data: transpose_920 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_913": "Data",
                "transpose_920": "Data"
            },
            "input_nodes": [
                "reshape_913",
                "transpose_920"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_921",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_922 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_922"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_921",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 564
        },
        "matmul_929": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_907 (port_0)",
                "Data: bert.encoder.layer.17.attention.self.value.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.17.attention.self.value.weight": "Data",
                "reshape_907": "Data"
            },
            "input_nodes": [
                "reshape_907",
                "bert.encoder.layer.17.attention.self.value.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_929",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_930 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_930"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "matmul_929",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 565
        },
        "matmul_936": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_927 (port_0)",
                "Data: transpose_935 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_927": "Data",
                "transpose_935": "Data"
            },
            "input_nodes": [
                "reshape_927",
                "transpose_935"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_936",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_937 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_937"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_936",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 447
        },
        "matmul_940": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hstack_938 (port_0)",
                "Data: bert.encoder.layer.17.attention.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.17.attention.output.dense.weight": "Data",
                "hstack_938": "Data"
            },
            "input_nodes": [
                "hstack_938",
                "bert.encoder.layer.17.attention.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_940",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_941 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_941"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_940",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 397
        },
        "matmul_948": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_946 (port_0)",
                "Data: bert.encoder.layer.17.intermediate.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.17.intermediate.dense.weight": "Data",
                "reshape_946": "Data"
            },
            "input_nodes": [
                "reshape_946",
                "bert.encoder.layer.17.intermediate.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_948",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_949 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_949"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_948",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 446
        },
        "matmul_954": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_952 (port_0)",
                "Data: bert.encoder.layer.17.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.17.output.dense.weight": "Data",
                "reshape_952": "Data"
            },
            "input_nodes": [
                "reshape_952",
                "bert.encoder.layer.17.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_954",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_955 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_955"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_954",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 362
        },
        "matmul_962": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_960 (port_0)",
                "Data: bert.encoder.layer.18.attention.self.query.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.18.attention.self.query.weight": "Data",
                "reshape_960": "Data"
            },
            "input_nodes": [
                "reshape_960",
                "bert.encoder.layer.18.attention.self.query.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_962",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_963 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_963"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "matmul_962",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 577
        },
        "matmul_968": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_960 (port_0)",
                "Data: bert.encoder.layer.18.attention.self.key.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.18.attention.self.key.weight": "Data",
                "reshape_960": "Data"
            },
            "input_nodes": [
                "reshape_960",
                "bert.encoder.layer.18.attention.self.key.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_968",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_969 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_969"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "matmul_968",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 595
        },
        "matmul_974": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_966 (port_0)",
                "Data: transpose_973 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_966": "Data",
                "transpose_973": "Data"
            },
            "input_nodes": [
                "reshape_966",
                "transpose_973"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_974",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_975 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_975"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_974",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 494
        },
        "matmul_982": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_960 (port_0)",
                "Data: bert.encoder.layer.18.attention.self.value.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.18.attention.self.value.weight": "Data",
                "reshape_960": "Data"
            },
            "input_nodes": [
                "reshape_960",
                "bert.encoder.layer.18.attention.self.value.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_982",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_983 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_983"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "matmul_982",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 495
        },
        "matmul_989": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_980 (port_0)",
                "Data: transpose_988 (port_1)"
            ],
            "input_node_to_edge_type": {
                "reshape_980": "Data",
                "transpose_988": "Data"
            },
            "input_nodes": [
                "reshape_980",
                "transpose_988"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_989",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_990 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_990"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "matmul_989",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 377
        },
        "matmul_993": {
            "cache": {
                "shape": [
                    1,
                    1,
                    384,
                    1024
                ]
            },
            "class": "matmul",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hstack_991 (port_0)",
                "Data: bert.encoder.layer.18.attention.output.dense.weight (port_1)"
            ],
            "input_node_to_edge_type": {
                "bert.encoder.layer.18.attention.output.dense.weight": "Data",
                "hstack_991": "Data"
            },
            "input_nodes": [
                "hstack_991",
                "bert.encoder.layer.18.attention.output.dense.weight"
            ],
            "input_tms": [
                [],
                []
            ],
            "ir": "pybuda",
            "name": "matmul_993",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "matmul"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_994 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_994"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "matmul_993",
                "original_op_type": "matmul"
            },
            "type": "matmul",
            "unique_id": 327
        },
        "multiply_1029": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "multiply",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1028 (port_0)",
                "Data: input_1_multiply_1029 (port_1)"
            ],
            "input_node_to_edge_type": {
                "input_1_multiply_1029": "Data",
                "reshape_1028": "Data"
            },
            "input_nodes": [
                "reshape_1028",
                "input_1_multiply_1029"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -1,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "multiply_1029",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "multiply"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1030 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1030"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "multiply_1029",
                "original_op_type": "multiply"
            },
            "type": "multiply",
            "unique_id": 389
        },
        "multiply_1082": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "multiply",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1081 (port_0)",
                "Data: input_1_multiply_1082 (port_1)"
            ],
            "input_node_to_edge_type": {
                "input_1_multiply_1082": "Data",
                "reshape_1081": "Data"
            },
            "input_nodes": [
                "reshape_1081",
                "input_1_multiply_1082"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -1,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "multiply_1082",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "multiply"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1083 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1083"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "multiply_1082",
                "original_op_type": "multiply"
            },
            "type": "multiply",
            "unique_id": 319
        },
        "multiply_1135": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "multiply",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1134 (port_0)",
                "Data: input_1_multiply_1135 (port_1)"
            ],
            "input_node_to_edge_type": {
                "input_1_multiply_1135": "Data",
                "reshape_1134": "Data"
            },
            "input_nodes": [
                "reshape_1134",
                "input_1_multiply_1135"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -1,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "multiply_1135",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "multiply"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1136 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1136"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "multiply_1135",
                "original_op_type": "multiply"
            },
            "type": "multiply",
            "unique_id": 249
        },
        "multiply_1188": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "multiply",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1187 (port_0)",
                "Data: input_1_multiply_1188 (port_1)"
            ],
            "input_node_to_edge_type": {
                "input_1_multiply_1188": "Data",
                "reshape_1187": "Data"
            },
            "input_nodes": [
                "reshape_1187",
                "input_1_multiply_1188"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -1,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "multiply_1188",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "multiply"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1189 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1189"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "multiply_1188",
                "original_op_type": "multiply"
            },
            "type": "multiply",
            "unique_id": 182
        },
        "multiply_1241": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "multiply",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1240 (port_0)",
                "Data: input_1_multiply_1241 (port_1)"
            ],
            "input_node_to_edge_type": {
                "input_1_multiply_1241": "Data",
                "reshape_1240": "Data"
            },
            "input_nodes": [
                "reshape_1240",
                "input_1_multiply_1241"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -1,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "multiply_1241",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "multiply"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1242 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1242"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "multiply_1241",
                "original_op_type": "multiply"
            },
            "type": "multiply",
            "unique_id": 118
        },
        "multiply_128": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "multiply",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_127 (port_0)",
                "Data: input_1_multiply_128 (port_1)"
            ],
            "input_node_to_edge_type": {
                "input_1_multiply_128": "Data",
                "reshape_127": "Data"
            },
            "input_nodes": [
                "reshape_127",
                "input_1_multiply_128"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -1,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "multiply_128",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "multiply"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_129 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_129"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "multiply_128",
                "original_op_type": "multiply"
            },
            "type": "multiply",
            "unique_id": 1578
        },
        "multiply_18": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "multiply",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_17 (port_0)",
                "Data: input_1_multiply_18 (port_1)"
            ],
            "input_node_to_edge_type": {
                "input_1_multiply_18": "Data",
                "reshape_17": "Data"
            },
            "input_nodes": [
                "reshape_17",
                "input_1_multiply_18"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -1,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "multiply_18",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "multiply"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_23 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_23"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "multiply_18",
                "original_op_type": "multiply"
            },
            "type": "multiply",
            "unique_id": 1668
        },
        "multiply_181": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "multiply",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_180 (port_0)",
                "Data: input_1_multiply_181 (port_1)"
            ],
            "input_node_to_edge_type": {
                "input_1_multiply_181": "Data",
                "reshape_180": "Data"
            },
            "input_nodes": [
                "reshape_180",
                "input_1_multiply_181"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -1,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "multiply_181",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "multiply"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_182 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_182"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "multiply_181",
                "original_op_type": "multiply"
            },
            "type": "multiply",
            "unique_id": 1509
        },
        "multiply_22": {
            "cache": {
                "shape": [
                    1,
                    1,
                    1,
                    384
                ]
            },
            "class": "multiply",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: subtract_21 (port_0)",
                "Data: input_1_multiply_22 (port_1)"
            ],
            "input_node_to_edge_type": {
                "input_1_multiply_22": "Data",
                "subtract_21": "Data"
            },
            "input_nodes": [
                "subtract_21",
                "input_1_multiply_22"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -1,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "multiply_22",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "multiply"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1242 (port_0)",
                "Data: add_1189 (port_0)",
                "Data: add_1136 (port_0)",
                "Data: add_1083 (port_0)",
                "Data: add_1030 (port_0)",
                "Data: add_977 (port_0)",
                "Data: add_924 (port_0)",
                "Data: add_871 (port_0)",
                "Data: add_818 (port_0)",
                "Data: add_765 (port_0)",
                "Data: add_712 (port_0)",
                "Data: add_659 (port_0)",
                "Data: add_606 (port_0)",
                "Data: add_553 (port_0)",
                "Data: add_500 (port_0)",
                "Data: add_447 (port_0)",
                "Data: add_394 (port_0)",
                "Data: add_341 (port_0)",
                "Data: add_288 (port_0)",
                "Data: add_235 (port_0)",
                "Data: add_182 (port_0)",
                "Data: add_129 (port_0)",
                "Data: add_76 (port_0)",
                "Data: add_23 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1242",
                "add_1189",
                "add_1136",
                "add_1083",
                "add_1030",
                "add_977",
                "add_924",
                "add_871",
                "add_818",
                "add_765",
                "add_712",
                "add_659",
                "add_606",
                "add_553",
                "add_500",
                "add_447",
                "add_394",
                "add_341",
                "add_288",
                "add_235",
                "add_182",
                "add_129",
                "add_76",
                "add_23"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert",
                "original_op_name": "multiply_22",
                "original_op_type": "multiply"
            },
            "type": "multiply",
            "unique_id": 119
        },
        "multiply_234": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "multiply",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_233 (port_0)",
                "Data: input_1_multiply_234 (port_1)"
            ],
            "input_node_to_edge_type": {
                "input_1_multiply_234": "Data",
                "reshape_233": "Data"
            },
            "input_nodes": [
                "reshape_233",
                "input_1_multiply_234"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -1,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "multiply_234",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "multiply"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_235 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_235"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "multiply_234",
                "original_op_type": "multiply"
            },
            "type": "multiply",
            "unique_id": 1439
        },
        "multiply_287": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "multiply",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_286 (port_0)",
                "Data: input_1_multiply_287 (port_1)"
            ],
            "input_node_to_edge_type": {
                "input_1_multiply_287": "Data",
                "reshape_286": "Data"
            },
            "input_nodes": [
                "reshape_286",
                "input_1_multiply_287"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -1,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "multiply_287",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "multiply"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_288 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_288"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "multiply_287",
                "original_op_type": "multiply"
            },
            "type": "multiply",
            "unique_id": 1369
        },
        "multiply_340": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "multiply",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_339 (port_0)",
                "Data: input_1_multiply_340 (port_1)"
            ],
            "input_node_to_edge_type": {
                "input_1_multiply_340": "Data",
                "reshape_339": "Data"
            },
            "input_nodes": [
                "reshape_339",
                "input_1_multiply_340"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -1,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "multiply_340",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "multiply"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_341 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_341"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "multiply_340",
                "original_op_type": "multiply"
            },
            "type": "multiply",
            "unique_id": 1299
        },
        "multiply_393": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "multiply",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_392 (port_0)",
                "Data: input_1_multiply_393 (port_1)"
            ],
            "input_node_to_edge_type": {
                "input_1_multiply_393": "Data",
                "reshape_392": "Data"
            },
            "input_nodes": [
                "reshape_392",
                "input_1_multiply_393"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -1,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "multiply_393",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "multiply"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_394 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_394"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "multiply_393",
                "original_op_type": "multiply"
            },
            "type": "multiply",
            "unique_id": 1229
        },
        "multiply_446": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "multiply",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_445 (port_0)",
                "Data: input_1_multiply_446 (port_1)"
            ],
            "input_node_to_edge_type": {
                "input_1_multiply_446": "Data",
                "reshape_445": "Data"
            },
            "input_nodes": [
                "reshape_445",
                "input_1_multiply_446"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -1,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "multiply_446",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "multiply"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_447 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_447"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "multiply_446",
                "original_op_type": "multiply"
            },
            "type": "multiply",
            "unique_id": 1159
        },
        "multiply_499": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "multiply",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_498 (port_0)",
                "Data: input_1_multiply_499 (port_1)"
            ],
            "input_node_to_edge_type": {
                "input_1_multiply_499": "Data",
                "reshape_498": "Data"
            },
            "input_nodes": [
                "reshape_498",
                "input_1_multiply_499"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -1,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "multiply_499",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "multiply"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_500 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_500"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "multiply_499",
                "original_op_type": "multiply"
            },
            "type": "multiply",
            "unique_id": 1089
        },
        "multiply_552": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "multiply",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_551 (port_0)",
                "Data: input_1_multiply_552 (port_1)"
            ],
            "input_node_to_edge_type": {
                "input_1_multiply_552": "Data",
                "reshape_551": "Data"
            },
            "input_nodes": [
                "reshape_551",
                "input_1_multiply_552"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -1,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "multiply_552",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "multiply"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_553 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_553"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "multiply_552",
                "original_op_type": "multiply"
            },
            "type": "multiply",
            "unique_id": 1019
        },
        "multiply_605": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "multiply",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_604 (port_0)",
                "Data: input_1_multiply_605 (port_1)"
            ],
            "input_node_to_edge_type": {
                "input_1_multiply_605": "Data",
                "reshape_604": "Data"
            },
            "input_nodes": [
                "reshape_604",
                "input_1_multiply_605"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -1,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "multiply_605",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "multiply"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_606 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_606"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "multiply_605",
                "original_op_type": "multiply"
            },
            "type": "multiply",
            "unique_id": 949
        },
        "multiply_658": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "multiply",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_657 (port_0)",
                "Data: input_1_multiply_658 (port_1)"
            ],
            "input_node_to_edge_type": {
                "input_1_multiply_658": "Data",
                "reshape_657": "Data"
            },
            "input_nodes": [
                "reshape_657",
                "input_1_multiply_658"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -1,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "multiply_658",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "multiply"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_659 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_659"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "multiply_658",
                "original_op_type": "multiply"
            },
            "type": "multiply",
            "unique_id": 879
        },
        "multiply_711": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "multiply",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_710 (port_0)",
                "Data: input_1_multiply_711 (port_1)"
            ],
            "input_node_to_edge_type": {
                "input_1_multiply_711": "Data",
                "reshape_710": "Data"
            },
            "input_nodes": [
                "reshape_710",
                "input_1_multiply_711"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -1,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "multiply_711",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "multiply"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_712 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_712"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "multiply_711",
                "original_op_type": "multiply"
            },
            "type": "multiply",
            "unique_id": 809
        },
        "multiply_75": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "multiply",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_74 (port_0)",
                "Data: input_1_multiply_75 (port_1)"
            ],
            "input_node_to_edge_type": {
                "input_1_multiply_75": "Data",
                "reshape_74": "Data"
            },
            "input_nodes": [
                "reshape_74",
                "input_1_multiply_75"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -1,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "multiply_75",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "multiply"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_76 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_76"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "multiply_75",
                "original_op_type": "multiply"
            },
            "type": "multiply",
            "unique_id": 1629
        },
        "multiply_764": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "multiply",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_763 (port_0)",
                "Data: input_1_multiply_764 (port_1)"
            ],
            "input_node_to_edge_type": {
                "input_1_multiply_764": "Data",
                "reshape_763": "Data"
            },
            "input_nodes": [
                "reshape_763",
                "input_1_multiply_764"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -1,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "multiply_764",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "multiply"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_765 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_765"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "multiply_764",
                "original_op_type": "multiply"
            },
            "type": "multiply",
            "unique_id": 739
        },
        "multiply_817": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "multiply",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_816 (port_0)",
                "Data: input_1_multiply_817 (port_1)"
            ],
            "input_node_to_edge_type": {
                "input_1_multiply_817": "Data",
                "reshape_816": "Data"
            },
            "input_nodes": [
                "reshape_816",
                "input_1_multiply_817"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -1,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "multiply_817",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "multiply"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_818 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_818"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "multiply_817",
                "original_op_type": "multiply"
            },
            "type": "multiply",
            "unique_id": 669
        },
        "multiply_870": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "multiply",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_869 (port_0)",
                "Data: input_1_multiply_870 (port_1)"
            ],
            "input_node_to_edge_type": {
                "input_1_multiply_870": "Data",
                "reshape_869": "Data"
            },
            "input_nodes": [
                "reshape_869",
                "input_1_multiply_870"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -1,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "multiply_870",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "multiply"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_871 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_871"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "multiply_870",
                "original_op_type": "multiply"
            },
            "type": "multiply",
            "unique_id": 599
        },
        "multiply_923": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "multiply",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_922 (port_0)",
                "Data: input_1_multiply_923 (port_1)"
            ],
            "input_node_to_edge_type": {
                "input_1_multiply_923": "Data",
                "reshape_922": "Data"
            },
            "input_nodes": [
                "reshape_922",
                "input_1_multiply_923"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -1,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "multiply_923",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "multiply"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_924 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_924"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "multiply_923",
                "original_op_type": "multiply"
            },
            "type": "multiply",
            "unique_id": 529
        },
        "multiply_976": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "multiply",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_975 (port_0)",
                "Data: input_1_multiply_976 (port_1)"
            ],
            "input_node_to_edge_type": {
                "input_1_multiply_976": "Data",
                "reshape_975": "Data"
            },
            "input_nodes": [
                "reshape_975",
                "input_1_multiply_976"
            ],
            "input_tms": [
                [],
                [
                    {
                        "op_type": {
                            "attrs": [
                                -3,
                                16,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -2,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    },
                    {
                        "op_type": {
                            "attrs": [
                                -1,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ]
            ],
            "ir": "pybuda",
            "name": "multiply_976",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "multiply"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_977 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_977"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "multiply_976",
                "original_op_type": "multiply"
            },
            "type": "multiply",
            "unique_id": 459
        },
        "nop_1": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_0 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_0": "Data"
            },
            "input_nodes": [
                "layernorm_0"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_1",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_2 (port_0)",
                "Data: add_43 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_2",
                "add_43"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEmbeddings::embeddings/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_1",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 1539
        },
        "nop_1010": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1009 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1009": "Data"
            },
            "input_nodes": [
                "add_1009"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_1010",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1011 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1011"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_1010",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 242
        },
        "nop_1032": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: softmax_1031 (port_0)"
            ],
            "input_node_to_edge_type": {
                "softmax_1031": "Data"
            },
            "input_nodes": [
                "softmax_1031"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_1032",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1033 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1033"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_1032",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 341
        },
        "nop_1049": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1048 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1048": "Data"
            },
            "input_nodes": [
                "add_1048"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_1049",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1050 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1050"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_1049",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 209
        },
        "nop_1063": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1062 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1062": "Data"
            },
            "input_nodes": [
                "add_1062"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_1063",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1064 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1064"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_1063",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 176
        },
        "nop_1085": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: softmax_1084 (port_0)"
            ],
            "input_node_to_edge_type": {
                "softmax_1084": "Data"
            },
            "input_nodes": [
                "softmax_1084"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_1085",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1086 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1086"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_1085",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 271
        },
        "nop_109": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_108 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_108": "Data"
            },
            "input_nodes": [
                "add_108"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_109",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_110 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_110"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_109",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 1432
        },
        "nop_1102": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1101 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1101": "Data"
            },
            "input_nodes": [
                "add_1101"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_1102",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1103 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1103"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_1102",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 146
        },
        "nop_1116": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1115 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1115": "Data"
            },
            "input_nodes": [
                "add_1115"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_1116",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1117 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1117"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_1116",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 116
        },
        "nop_1138": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: softmax_1137 (port_0)"
            ],
            "input_node_to_edge_type": {
                "softmax_1137": "Data"
            },
            "input_nodes": [
                "softmax_1137"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_1138",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1139 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1139"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_1138",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 202
        },
        "nop_1155": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1154 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1154": "Data"
            },
            "input_nodes": [
                "add_1154"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_1155",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1156 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1156"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_1155",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 95
        },
        "nop_1169": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1168 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1168": "Data"
            },
            "input_nodes": [
                "add_1168"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_1169",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1170 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1170"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_1169",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 74
        },
        "nop_1191": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: softmax_1190 (port_0)"
            ],
            "input_node_to_edge_type": {
                "softmax_1190": "Data"
            },
            "input_nodes": [
                "softmax_1190"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_1191",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1192 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1192"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_1191",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 139
        },
        "nop_1208": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1207 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1207": "Data"
            },
            "input_nodes": [
                "add_1207"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_1208",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1209 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1209"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_1208",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 57
        },
        "nop_1222": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1221 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1221": "Data"
            },
            "input_nodes": [
                "add_1221"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_1222",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1223 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1223"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_1222",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 43
        },
        "nop_1244": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: softmax_1243 (port_0)"
            ],
            "input_node_to_edge_type": {
                "softmax_1243": "Data"
            },
            "input_nodes": [
                "softmax_1243"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_1244",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1245 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1245"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_1244",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 88
        },
        "nop_1261": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1260 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1260": "Data"
            },
            "input_nodes": [
                "add_1260"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_1261",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1262 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1262"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_1261",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 32
        },
        "nop_1275": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1274 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1274": "Data"
            },
            "input_nodes": [
                "add_1274"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_1275",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1276 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1276"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_1275",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 24
        },
        "nop_131": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: softmax_130 (port_0)"
            ],
            "input_node_to_edge_type": {
                "softmax_130": "Data"
            },
            "input_nodes": [
                "softmax_130"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_131",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_132 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_132"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_131",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 1531
        },
        "nop_148": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_147 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_147": "Data"
            },
            "input_nodes": [
                "add_147"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_148",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_149 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_149"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_148",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 1398
        },
        "nop_162": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_161 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_161": "Data"
            },
            "input_nodes": [
                "add_161"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_162",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_163 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_163"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_162",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 1362
        },
        "nop_184": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: softmax_183 (port_0)"
            ],
            "input_node_to_edge_type": {
                "softmax_183": "Data"
            },
            "input_nodes": [
                "softmax_183"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_184",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_185 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_185"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_184",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 1461
        },
        "nop_20": {
            "cache": {
                "shape": [
                    1,
                    1,
                    1,
                    384
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_19 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_19": "Data"
            },
            "input_nodes": [
                "reshape_19"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_20",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: subtract_21 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "subtract_21"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert",
                "original_op_name": "nop_20",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 150
        },
        "nop_201": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_200 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_200": "Data"
            },
            "input_nodes": [
                "add_200"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_201",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_202 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_202"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_201",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 1328
        },
        "nop_215": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_214 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_214": "Data"
            },
            "input_nodes": [
                "add_214"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_215",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_216 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_216"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_215",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 1292
        },
        "nop_237": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: softmax_236 (port_0)"
            ],
            "input_node_to_edge_type": {
                "softmax_236": "Data"
            },
            "input_nodes": [
                "softmax_236"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_237",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_238 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_238"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_237",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 1391
        },
        "nop_25": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: softmax_24 (port_0)"
            ],
            "input_node_to_edge_type": {
                "softmax_24": "Data"
            },
            "input_nodes": [
                "softmax_24"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_25",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_26 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_26"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_25",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 1644
        },
        "nop_254": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_253 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_253": "Data"
            },
            "input_nodes": [
                "add_253"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_254",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_255 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_255"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_254",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 1258
        },
        "nop_268": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_267 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_267": "Data"
            },
            "input_nodes": [
                "add_267"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_268",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_269 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_269"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_268",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 1222
        },
        "nop_290": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: softmax_289 (port_0)"
            ],
            "input_node_to_edge_type": {
                "softmax_289": "Data"
            },
            "input_nodes": [
                "softmax_289"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_290",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_291 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_291"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_290",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 1321
        },
        "nop_307": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_306 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_306": "Data"
            },
            "input_nodes": [
                "add_306"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_307",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_308 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_308"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_307",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 1188
        },
        "nop_321": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_320 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_320": "Data"
            },
            "input_nodes": [
                "add_320"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_321",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_322 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_322"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_321",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 1152
        },
        "nop_343": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: softmax_342 (port_0)"
            ],
            "input_node_to_edge_type": {
                "softmax_342": "Data"
            },
            "input_nodes": [
                "softmax_342"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_343",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_344 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_344"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_343",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 1251
        },
        "nop_360": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_359 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_359": "Data"
            },
            "input_nodes": [
                "add_359"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_360",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_361 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_361"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_360",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 1118
        },
        "nop_374": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_373 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_373": "Data"
            },
            "input_nodes": [
                "add_373"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_374",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_375 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_375"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_374",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 1082
        },
        "nop_396": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: softmax_395 (port_0)"
            ],
            "input_node_to_edge_type": {
                "softmax_395": "Data"
            },
            "input_nodes": [
                "softmax_395"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_396",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_397 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_397"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_396",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 1181
        },
        "nop_413": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_412 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_412": "Data"
            },
            "input_nodes": [
                "add_412"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_413",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_414 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_414"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_413",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 1048
        },
        "nop_42": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_41 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_41": "Data"
            },
            "input_nodes": [
                "add_41"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_42",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_43 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_43"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_42",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 1538
        },
        "nop_427": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_426 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_426": "Data"
            },
            "input_nodes": [
                "add_426"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_427",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_428 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_428"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_427",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 1012
        },
        "nop_449": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: softmax_448 (port_0)"
            ],
            "input_node_to_edge_type": {
                "softmax_448": "Data"
            },
            "input_nodes": [
                "softmax_448"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_449",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_450 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_450"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_449",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 1111
        },
        "nop_466": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_465 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_465": "Data"
            },
            "input_nodes": [
                "add_465"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_466",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_467 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_467"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_466",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 978
        },
        "nop_480": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_479 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_479": "Data"
            },
            "input_nodes": [
                "add_479"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_480",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_481 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_481"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_480",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 942
        },
        "nop_502": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: softmax_501 (port_0)"
            ],
            "input_node_to_edge_type": {
                "softmax_501": "Data"
            },
            "input_nodes": [
                "softmax_501"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_502",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_503 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_503"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_502",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 1041
        },
        "nop_519": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_518 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_518": "Data"
            },
            "input_nodes": [
                "add_518"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_519",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_520 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_520"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_519",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 908
        },
        "nop_533": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_532 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_532": "Data"
            },
            "input_nodes": [
                "add_532"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_533",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_534 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_534"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_533",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 872
        },
        "nop_555": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: softmax_554 (port_0)"
            ],
            "input_node_to_edge_type": {
                "softmax_554": "Data"
            },
            "input_nodes": [
                "softmax_554"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_555",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_556 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_556"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_555",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 971
        },
        "nop_56": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_55 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_55": "Data"
            },
            "input_nodes": [
                "add_55"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_56",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_57 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_57"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_56",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 1502
        },
        "nop_572": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_571 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_571": "Data"
            },
            "input_nodes": [
                "add_571"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_572",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_573 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_573"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_572",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 838
        },
        "nop_586": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_585 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_585": "Data"
            },
            "input_nodes": [
                "add_585"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_586",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_587 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_587"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_586",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 802
        },
        "nop_608": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: softmax_607 (port_0)"
            ],
            "input_node_to_edge_type": {
                "softmax_607": "Data"
            },
            "input_nodes": [
                "softmax_607"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_608",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_609 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_609"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_608",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 901
        },
        "nop_625": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_624 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_624": "Data"
            },
            "input_nodes": [
                "add_624"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_625",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_626 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_626"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_625",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 768
        },
        "nop_639": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_638 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_638": "Data"
            },
            "input_nodes": [
                "add_638"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_639",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_640 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_640"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_639",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 732
        },
        "nop_661": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: softmax_660 (port_0)"
            ],
            "input_node_to_edge_type": {
                "softmax_660": "Data"
            },
            "input_nodes": [
                "softmax_660"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_661",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_662 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_662"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_661",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 831
        },
        "nop_678": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_677 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_677": "Data"
            },
            "input_nodes": [
                "add_677"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_678",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_679 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_679"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_678",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 698
        },
        "nop_692": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_691 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_691": "Data"
            },
            "input_nodes": [
                "add_691"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_692",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_693 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_693"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_692",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 662
        },
        "nop_714": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: softmax_713 (port_0)"
            ],
            "input_node_to_edge_type": {
                "softmax_713": "Data"
            },
            "input_nodes": [
                "softmax_713"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_714",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_715 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_715"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_714",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 761
        },
        "nop_731": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_730 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_730": "Data"
            },
            "input_nodes": [
                "add_730"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_731",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_732 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_732"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_731",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 628
        },
        "nop_745": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_744 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_744": "Data"
            },
            "input_nodes": [
                "add_744"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_745",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_746 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_746"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_745",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 592
        },
        "nop_767": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: softmax_766 (port_0)"
            ],
            "input_node_to_edge_type": {
                "softmax_766": "Data"
            },
            "input_nodes": [
                "softmax_766"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_767",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_768 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_768"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_767",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 691
        },
        "nop_78": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: softmax_77 (port_0)"
            ],
            "input_node_to_edge_type": {
                "softmax_77": "Data"
            },
            "input_nodes": [
                "softmax_77"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_78",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_79 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_79"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_78",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 1596
        },
        "nop_784": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_783 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_783": "Data"
            },
            "input_nodes": [
                "add_783"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_784",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_785 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_785"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_784",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 558
        },
        "nop_798": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_797 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_797": "Data"
            },
            "input_nodes": [
                "add_797"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_798",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_799 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_799"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_798",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 522
        },
        "nop_820": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: softmax_819 (port_0)"
            ],
            "input_node_to_edge_type": {
                "softmax_819": "Data"
            },
            "input_nodes": [
                "softmax_819"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_820",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_821 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_821"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_820",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 621
        },
        "nop_837": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_836 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_836": "Data"
            },
            "input_nodes": [
                "add_836"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_837",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_838 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_838"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_837",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 488
        },
        "nop_851": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_850 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_850": "Data"
            },
            "input_nodes": [
                "add_850"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_851",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_852 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_852"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_851",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 452
        },
        "nop_873": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: softmax_872 (port_0)"
            ],
            "input_node_to_edge_type": {
                "softmax_872": "Data"
            },
            "input_nodes": [
                "softmax_872"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_873",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_874 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_874"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_873",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 551
        },
        "nop_890": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_889 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_889": "Data"
            },
            "input_nodes": [
                "add_889"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_890",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_891 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_891"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_890",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 418
        },
        "nop_904": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_903 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_903": "Data"
            },
            "input_nodes": [
                "add_903"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_904",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_905 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_905"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_904",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 382
        },
        "nop_926": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: softmax_925 (port_0)"
            ],
            "input_node_to_edge_type": {
                "softmax_925": "Data"
            },
            "input_nodes": [
                "softmax_925"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_926",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_927 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_927"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_926",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 481
        },
        "nop_943": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_942 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_942": "Data"
            },
            "input_nodes": [
                "add_942"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_943",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_944 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_944"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_943",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 348
        },
        "nop_95": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_94 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_94": "Data"
            },
            "input_nodes": [
                "add_94"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_95",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_96 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_96"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_95",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 1468
        },
        "nop_957": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_956 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_956": "Data"
            },
            "input_nodes": [
                "add_956"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_957",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_958 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_958"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_957",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 312
        },
        "nop_979": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: softmax_978 (port_0)"
            ],
            "input_node_to_edge_type": {
                "softmax_978": "Data"
            },
            "input_nodes": [
                "softmax_978"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_979",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_980 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_980"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_979",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 411
        },
        "nop_996": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "nop",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_995 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_995": "Data"
            },
            "input_nodes": [
                "add_995"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "nop_996",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "nop"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_997 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_997"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.dropout.Dropout::dropout",
                "original_op_name": "nop_996",
                "original_op_type": "nop"
            },
            "type": "nop",
            "unique_id": 278
        },
        "pybuda_6_i0": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "pybuda_6_i0",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: layernorm_0 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "layernorm_0"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "pybuda_6_i0"
            },
            "tile_broadcast": [],
            "type": "Input::input",
            "unique_id": 1570
        },
        "qa_outputs.bias": {
            "cache": {
                "shape": [
                    1
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "qa_outputs.bias",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_1283 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1283"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "qa_outputs.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1714
        },
        "qa_outputs.bias_fork_clone12": {
            "cache": {
                "shape": [
                    1
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "qa_outputs.bias_fork_clone12",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: add_1290 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1290"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "qa_outputs.bias"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1715
        },
        "qa_outputs.weight": {
            "cache": {
                "shape": [
                    1024,
                    1
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "qa_outputs.weight",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_1281 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1281"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "qa_outputs.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1720
        },
        "qa_outputs.weight_fork_clone19": {
            "cache": {
                "shape": [
                    1024,
                    1
                ]
            },
            "class": "Input::",
            "epoch": 0,
            "epoch_type": "Forward",
            "incoming_edge_port_info": [],
            "input_node_to_edge_type": {},
            "input_nodes": [],
            "is_cross_epoch_type": false,
            "memory_access": "FIFO",
            "name": "qa_outputs.weight_fork_clone19",
            "opcode": "Input",
            "outgoing_edge_port_info": [
                "Data: matmul_1288 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1288"
            ],
            "pybuda": 1,
            "queue_type": "input",
            "requires_grad": true,
            "tags": {
                "original_op_name": "qa_outputs.weight"
            },
            "tile_broadcast": [],
            "type": "Input::parameter",
            "unique_id": 1721
        },
        "reshape_1002": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "reshape(1,384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1001 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1001": "Data"
            },
            "input_nodes": [
                "matmul_1001"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1002",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1003 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1003"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_1002",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 358
        },
        "reshape_1005": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "reshape(384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: gelu_1004 (port_0)"
            ],
            "input_node_to_edge_type": {
                "gelu_1004": "Data"
            },
            "input_nodes": [
                "gelu_1004"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1005",
            "op_type": {
                "attrs": [
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1007 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1007"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_1005",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 308
        },
        "reshape_1008": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1007 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1007": "Data"
            },
            "input_nodes": [
                "matmul_1007"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1008",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1009 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1009"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_1008",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 276
        },
        "reshape_101": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "reshape(1,384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_100 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_100": "Data"
            },
            "input_nodes": [
                "matmul_100"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_101",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_102 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_102"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_101",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1548
        },
        "reshape_1013": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_1012 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_1012": "Data"
            },
            "input_nodes": [
                "layernorm_1012"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1013",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1035 (port_0)",
                "Data: matmul_1015 (port_0)",
                "Data: matmul_1021 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1035",
                "matmul_1015",
                "matmul_1021"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_1013",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 442
        },
        "reshape_1016": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1015 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1015": "Data"
            },
            "input_nodes": [
                "matmul_1015"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1016",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1017 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1017"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_1016",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 491
        },
        "reshape_1019": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_1018 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_1018": "Data"
            },
            "input_nodes": [
                "hslice_1018"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1019",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1027 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1027"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_1019",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 440
        },
        "reshape_1022": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1021 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1021": "Data"
            },
            "input_nodes": [
                "matmul_1021"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1022",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1023 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1023"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "reshape_1022",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 508
        },
        "reshape_1025": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_1024 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_1024": "Data"
            },
            "input_nodes": [
                "hslice_1024"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1025",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_1026 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_1026"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_1025",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 457
        },
        "reshape_1028": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(1,16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1027 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1027": "Data"
            },
            "input_nodes": [
                "matmul_1027"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1028",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: multiply_1029 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_1029"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_1028",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 406
        },
        "reshape_1033": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_1032 (port_0)"
            ],
            "input_node_to_edge_type": {
                "nop_1032": "Data"
            },
            "input_nodes": [
                "nop_1032"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1033",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1042 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1042"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_1033",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 323
        },
        "reshape_1036": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1035 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1035": "Data"
            },
            "input_nodes": [
                "matmul_1035"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1036",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1037 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1037"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "reshape_1036",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 408
        },
        "reshape_104": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "reshape(384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: gelu_103 (port_0)"
            ],
            "input_node_to_edge_type": {
                "gelu_103": "Data"
            },
            "input_nodes": [
                "gelu_103"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_104",
            "op_type": {
                "attrs": [
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_106 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_106"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_104",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1498
        },
        "reshape_1040": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "reshape(16,64,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: transpose_1039 (port_0)"
            ],
            "input_node_to_edge_type": {
                "transpose_1039": "Data"
            },
            "input_nodes": [
                "transpose_1039"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1040",
            "op_type": {
                "attrs": [
                    16,
                    64,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_1041 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_1041"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_1040",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 342
        },
        "reshape_1043": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(1,16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1042 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1042": "Data"
            },
            "input_nodes": [
                "matmul_1042"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1043",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hstack_1044 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hstack_1044"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_1043",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 290
        },
        "reshape_1047": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1046 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1046": "Data"
            },
            "input_nodes": [
                "matmul_1046"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1047",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1048 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1048"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_1047",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 240
        },
        "reshape_1052": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_1051 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_1051": "Data"
            },
            "input_nodes": [
                "layernorm_1051"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1052",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1054 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1054"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_1052",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 321
        },
        "reshape_1055": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "reshape(1,384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1054 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1054": "Data"
            },
            "input_nodes": [
                "matmul_1054"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1055",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1056 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1056"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_1055",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 288
        },
        "reshape_1058": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "reshape(384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: gelu_1057 (port_0)"
            ],
            "input_node_to_edge_type": {
                "gelu_1057": "Data"
            },
            "input_nodes": [
                "gelu_1057"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1058",
            "op_type": {
                "attrs": [
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1060 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1060"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_1058",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 238
        },
        "reshape_1061": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1060 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1060": "Data"
            },
            "input_nodes": [
                "matmul_1060"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1061",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1062 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1062"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_1061",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 207
        },
        "reshape_1066": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_1065 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_1065": "Data"
            },
            "input_nodes": [
                "layernorm_1065"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1066",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1088 (port_0)",
                "Data: matmul_1068 (port_0)",
                "Data: matmul_1074 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1088",
                "matmul_1068",
                "matmul_1074"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_1066",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 372
        },
        "reshape_1069": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1068 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1068": "Data"
            },
            "input_nodes": [
                "matmul_1068"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1069",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1070 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1070"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_1069",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 421
        },
        "reshape_107": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_106 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_106": "Data"
            },
            "input_nodes": [
                "matmul_106"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_107",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_108 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_108"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_107",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1466
        },
        "reshape_1072": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_1071 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_1071": "Data"
            },
            "input_nodes": [
                "hslice_1071"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1072",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1080 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1080"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_1072",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 370
        },
        "reshape_1075": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1074 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1074": "Data"
            },
            "input_nodes": [
                "matmul_1074"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1075",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1076 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1076"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "reshape_1075",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 438
        },
        "reshape_1078": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_1077 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_1077": "Data"
            },
            "input_nodes": [
                "hslice_1077"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1078",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_1079 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_1079"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_1078",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 387
        },
        "reshape_1081": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(1,16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1080 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1080": "Data"
            },
            "input_nodes": [
                "matmul_1080"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1081",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: multiply_1082 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_1082"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_1081",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 336
        },
        "reshape_1086": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_1085 (port_0)"
            ],
            "input_node_to_edge_type": {
                "nop_1085": "Data"
            },
            "input_nodes": [
                "nop_1085"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1086",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1095 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1095"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_1086",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 253
        },
        "reshape_1089": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1088 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1088": "Data"
            },
            "input_nodes": [
                "matmul_1088"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1089",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1090 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1090"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "reshape_1089",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 338
        },
        "reshape_1093": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "reshape(16,64,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: transpose_1092 (port_0)"
            ],
            "input_node_to_edge_type": {
                "transpose_1092": "Data"
            },
            "input_nodes": [
                "transpose_1092"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1093",
            "op_type": {
                "attrs": [
                    16,
                    64,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_1094 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_1094"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_1093",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 272
        },
        "reshape_1096": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(1,16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1095 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1095": "Data"
            },
            "input_nodes": [
                "matmul_1095"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1096",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hstack_1097 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hstack_1097"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_1096",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 220
        },
        "reshape_11": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_10 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_10": "Data"
            },
            "input_nodes": [
                "matmul_10"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_11",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_12 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_12"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "reshape_11",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1705
        },
        "reshape_1100": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1099 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1099": "Data"
            },
            "input_nodes": [
                "matmul_1099"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1100",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1101 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1101"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_1100",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 174
        },
        "reshape_1105": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_1104 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_1104": "Data"
            },
            "input_nodes": [
                "layernorm_1104"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1105",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1107 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1107"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_1105",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 251
        },
        "reshape_1108": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "reshape(1,384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1107 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1107": "Data"
            },
            "input_nodes": [
                "matmul_1107"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1108",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1109 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1109"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_1108",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 218
        },
        "reshape_1111": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "reshape(384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: gelu_1110 (port_0)"
            ],
            "input_node_to_edge_type": {
                "gelu_1110": "Data"
            },
            "input_nodes": [
                "gelu_1110"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1111",
            "op_type": {
                "attrs": [
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1113 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1113"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_1111",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 172
        },
        "reshape_1114": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1113 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1113": "Data"
            },
            "input_nodes": [
                "matmul_1113"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1114",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1115 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1115"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_1114",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 144
        },
        "reshape_1119": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_1118 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_1118": "Data"
            },
            "input_nodes": [
                "layernorm_1118"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1119",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1141 (port_0)",
                "Data: matmul_1121 (port_0)",
                "Data: matmul_1127 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1141",
                "matmul_1121",
                "matmul_1127"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_1119",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 302
        },
        "reshape_112": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_111 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_111": "Data"
            },
            "input_nodes": [
                "layernorm_111"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_112",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_134 (port_0)",
                "Data: matmul_114 (port_0)",
                "Data: matmul_120 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_134",
                "matmul_114",
                "matmul_120"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_112",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1618
        },
        "reshape_1122": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1121 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1121": "Data"
            },
            "input_nodes": [
                "matmul_1121"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1122",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1123 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1123"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_1122",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 351
        },
        "reshape_1125": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_1124 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_1124": "Data"
            },
            "input_nodes": [
                "hslice_1124"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1125",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1133 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1133"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_1125",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 300
        },
        "reshape_1128": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1127 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1127": "Data"
            },
            "input_nodes": [
                "matmul_1127"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1128",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1129 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1129"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "reshape_1128",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 368
        },
        "reshape_1131": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_1130 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_1130": "Data"
            },
            "input_nodes": [
                "hslice_1130"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1131",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_1132 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_1132"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_1131",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 317
        },
        "reshape_1134": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(1,16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1133 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1133": "Data"
            },
            "input_nodes": [
                "matmul_1133"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1134",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: multiply_1135 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_1135"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_1134",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 266
        },
        "reshape_1139": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_1138 (port_0)"
            ],
            "input_node_to_edge_type": {
                "nop_1138": "Data"
            },
            "input_nodes": [
                "nop_1138"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1139",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1148 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1148"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_1139",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 186
        },
        "reshape_1142": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1141 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1141": "Data"
            },
            "input_nodes": [
                "matmul_1141"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1142",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1143 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1143"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "reshape_1142",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 268
        },
        "reshape_1146": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "reshape(16,64,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: transpose_1145 (port_0)"
            ],
            "input_node_to_edge_type": {
                "transpose_1145": "Data"
            },
            "input_nodes": [
                "transpose_1145"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1146",
            "op_type": {
                "attrs": [
                    16,
                    64,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_1147 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_1147"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_1146",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 203
        },
        "reshape_1149": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(1,16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1148 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1148": "Data"
            },
            "input_nodes": [
                "matmul_1148"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1149",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hstack_1150 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hstack_1150"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_1149",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 156
        },
        "reshape_115": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_114 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_114": "Data"
            },
            "input_nodes": [
                "matmul_114"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_115",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_116 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_116"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_115",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1647
        },
        "reshape_1153": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1152 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1152": "Data"
            },
            "input_nodes": [
                "matmul_1152"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1153",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1154 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1154"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_1153",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 114
        },
        "reshape_1158": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_1157 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_1157": "Data"
            },
            "input_nodes": [
                "layernorm_1157"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1158",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1160 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1160"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_1158",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 184
        },
        "reshape_1161": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "reshape(1,384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1160 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1160": "Data"
            },
            "input_nodes": [
                "matmul_1160"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1161",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1162 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1162"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_1161",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 154
        },
        "reshape_1164": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "reshape(384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: gelu_1163 (port_0)"
            ],
            "input_node_to_edge_type": {
                "gelu_1163": "Data"
            },
            "input_nodes": [
                "gelu_1163"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1164",
            "op_type": {
                "attrs": [
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1166 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1166"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_1164",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 112
        },
        "reshape_1167": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1166 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1166": "Data"
            },
            "input_nodes": [
                "matmul_1166"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1167",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1168 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1168"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_1167",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 93
        },
        "reshape_1172": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_1171 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_1171": "Data"
            },
            "input_nodes": [
                "layernorm_1171"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1172",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1194 (port_0)",
                "Data: matmul_1174 (port_0)",
                "Data: matmul_1180 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1194",
                "matmul_1174",
                "matmul_1180"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_1172",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 232
        },
        "reshape_1175": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1174 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1174": "Data"
            },
            "input_nodes": [
                "matmul_1174"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1175",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1176 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1176"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_1175",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 281
        },
        "reshape_1178": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_1177 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_1177": "Data"
            },
            "input_nodes": [
                "hslice_1177"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1178",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1186 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1186"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_1178",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 230
        },
        "reshape_118": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_117 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_117": "Data"
            },
            "input_nodes": [
                "hslice_117"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_118",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_126 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_126"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_118",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1616
        },
        "reshape_1181": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1180 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1180": "Data"
            },
            "input_nodes": [
                "matmul_1180"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1181",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1182 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1182"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "reshape_1181",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 298
        },
        "reshape_1184": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_1183 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_1183": "Data"
            },
            "input_nodes": [
                "hslice_1183"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1184",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_1185 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_1185"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_1184",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 247
        },
        "reshape_1187": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(1,16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1186 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1186": "Data"
            },
            "input_nodes": [
                "matmul_1186"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1187",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: multiply_1188 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_1188"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_1187",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 197
        },
        "reshape_1192": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_1191 (port_0)"
            ],
            "input_node_to_edge_type": {
                "nop_1191": "Data"
            },
            "input_nodes": [
                "nop_1191"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1192",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1201 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1201"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_1192",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 123
        },
        "reshape_1195": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1194 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1194": "Data"
            },
            "input_nodes": [
                "matmul_1194"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1195",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1196 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1196"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "reshape_1195",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 199
        },
        "reshape_1199": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "reshape(16,64,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: transpose_1198 (port_0)"
            ],
            "input_node_to_edge_type": {
                "transpose_1198": "Data"
            },
            "input_nodes": [
                "transpose_1198"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1199",
            "op_type": {
                "attrs": [
                    16,
                    64,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_1200 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_1200"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_1199",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 140
        },
        "reshape_1202": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(1,16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1201 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1201": "Data"
            },
            "input_nodes": [
                "matmul_1201"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1202",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hstack_1203 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hstack_1203"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_1202",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 101
        },
        "reshape_1206": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1205 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1205": "Data"
            },
            "input_nodes": [
                "matmul_1205"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1206",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1207 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1207"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_1206",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 72
        },
        "reshape_121": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_120 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_120": "Data"
            },
            "input_nodes": [
                "matmul_120"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_121",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_122 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_122"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "reshape_121",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1655
        },
        "reshape_1211": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_1210 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_1210": "Data"
            },
            "input_nodes": [
                "layernorm_1210"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1211",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1213 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1213"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_1211",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 121
        },
        "reshape_1214": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "reshape(1,384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1213 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1213": "Data"
            },
            "input_nodes": [
                "matmul_1213"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1214",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1215 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1215"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_1214",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 99
        },
        "reshape_1217": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "reshape(384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: gelu_1216 (port_0)"
            ],
            "input_node_to_edge_type": {
                "gelu_1216": "Data"
            },
            "input_nodes": [
                "gelu_1216"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1217",
            "op_type": {
                "attrs": [
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1219 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1219"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_1217",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 70
        },
        "reshape_1220": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1219 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1219": "Data"
            },
            "input_nodes": [
                "matmul_1219"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1220",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1221 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1221"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_1220",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 55
        },
        "reshape_1225": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_1224 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_1224": "Data"
            },
            "input_nodes": [
                "layernorm_1224"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1225",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1247 (port_0)",
                "Data: matmul_1227 (port_0)",
                "Data: matmul_1233 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1247",
                "matmul_1227",
                "matmul_1233"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_1225",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 166
        },
        "reshape_1228": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1227 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1227": "Data"
            },
            "input_nodes": [
                "matmul_1227"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1228",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1229 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1229"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_1228",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 211
        },
        "reshape_1231": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_1230 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_1230": "Data"
            },
            "input_nodes": [
                "hslice_1230"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1231",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1239 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1239"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_1231",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 163
        },
        "reshape_1234": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1233 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1233": "Data"
            },
            "input_nodes": [
                "matmul_1233"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1234",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1235 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1235"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "reshape_1234",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 228
        },
        "reshape_1237": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_1236 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_1236": "Data"
            },
            "input_nodes": [
                "hslice_1236"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1237",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_1238 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_1238"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_1237",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 179
        },
        "reshape_124": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_123 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_123": "Data"
            },
            "input_nodes": [
                "hslice_123"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_124",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_125 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_125"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_124",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1627
        },
        "reshape_1240": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(1,16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1239 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1239": "Data"
            },
            "input_nodes": [
                "matmul_1239"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1240",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: multiply_1241 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_1241"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_1240",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 132
        },
        "reshape_1245": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_1244 (port_0)"
            ],
            "input_node_to_edge_type": {
                "nop_1244": "Data"
            },
            "input_nodes": [
                "nop_1244"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1245",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1254 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1254"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_1245",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 78
        },
        "reshape_1248": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1247 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1247": "Data"
            },
            "input_nodes": [
                "matmul_1247"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1248",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1249 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1249"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "reshape_1248",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 136
        },
        "reshape_1252": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "reshape(16,64,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: transpose_1251 (port_0)"
            ],
            "input_node_to_edge_type": {
                "transpose_1251": "Data"
            },
            "input_nodes": [
                "transpose_1251"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1252",
            "op_type": {
                "attrs": [
                    16,
                    64,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_1253 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_1253"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_1252",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 89
        },
        "reshape_1255": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(1,16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1254 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1254": "Data"
            },
            "input_nodes": [
                "matmul_1254"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1255",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hstack_1256 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hstack_1256"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_1255",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 61
        },
        "reshape_1259": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1258 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1258": "Data"
            },
            "input_nodes": [
                "matmul_1258"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1259",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1260 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1260"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_1259",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 41
        },
        "reshape_1264": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_1263 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_1263": "Data"
            },
            "input_nodes": [
                "layernorm_1263"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1264",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1266 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1266"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_1264",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 76
        },
        "reshape_1267": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "reshape(1,384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1266 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1266": "Data"
            },
            "input_nodes": [
                "matmul_1266"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1267",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1268 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1268"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_1267",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 59
        },
        "reshape_127": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(1,16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_126 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_126": "Data"
            },
            "input_nodes": [
                "matmul_126"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_127",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: multiply_128 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_128"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_127",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1591
        },
        "reshape_1270": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "reshape(384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: gelu_1269 (port_0)"
            ],
            "input_node_to_edge_type": {
                "gelu_1269": "Data"
            },
            "input_nodes": [
                "gelu_1269"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1270",
            "op_type": {
                "attrs": [
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1272 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1272"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_1270",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 39
        },
        "reshape_1273": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_1272 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_1272": "Data"
            },
            "input_nodes": [
                "matmul_1272"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1273",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_1274 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_1274"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_1273",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 30
        },
        "reshape_1278": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_1277 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_1277": "Data"
            },
            "input_nodes": [
                "layernorm_1277"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1278",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1281 (port_0)",
                "Data: matmul_1288 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1281",
                "matmul_1288"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/torch.nn.modules.linear.Linear::qa_outputs",
                "original_op_name": "reshape_1278",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 13
        },
        "reshape_1284": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1
                ]
            },
            "class": "reshape(1,384,1,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1283 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1283": "Data"
            },
            "input_nodes": [
                "add_1283"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1284",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1285 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1285"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "reshape_1284",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 5
        },
        "reshape_1285": {
            "cache": {
                "shape": [
                    1,
                    384
                ]
            },
            "class": "reshape(1,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1284 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_1284": "Data"
            },
            "input_nodes": [
                "reshape_1284"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1285",
            "op_type": {
                "attrs": [
                    1,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: bert_large_tt_1.output_reshape_1285 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "bert_large_tt_1.output_reshape_1285"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::",
                "original_op_name": "reshape_1285",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 3
        },
        "reshape_1291": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1
                ]
            },
            "class": "reshape(1,384,1,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1290 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1290": "Data"
            },
            "input_nodes": [
                "add_1290"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1291",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1292 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1292"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "reshape_1291",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 6
        },
        "reshape_1292": {
            "cache": {
                "shape": [
                    1,
                    384
                ]
            },
            "class": "reshape(1,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1291 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_1291": "Data"
            },
            "input_nodes": [
                "reshape_1291"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_1292",
            "op_type": {
                "attrs": [
                    1,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: bert_large_tt_1.output_reshape_1292 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "bert_large_tt_1.output_reshape_1292"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::",
                "original_op_name": "reshape_1292",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 4
        },
        "reshape_132": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_131 (port_0)"
            ],
            "input_node_to_edge_type": {
                "nop_131": "Data"
            },
            "input_nodes": [
                "nop_131"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_132",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_141 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_141"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_132",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1513
        },
        "reshape_135": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_134 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_134": "Data"
            },
            "input_nodes": [
                "matmul_134"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_135",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_136 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_136"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "reshape_135",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1593
        },
        "reshape_139": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "reshape(16,64,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: transpose_138 (port_0)"
            ],
            "input_node_to_edge_type": {
                "transpose_138": "Data"
            },
            "input_nodes": [
                "transpose_138"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_139",
            "op_type": {
                "attrs": [
                    16,
                    64,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_140 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_140"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_139",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1532
        },
        "reshape_14": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_13 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_13": "Data"
            },
            "input_nodes": [
                "hslice_13"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_14",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_15 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_15"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_14",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1694
        },
        "reshape_142": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(1,16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_141 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_141": "Data"
            },
            "input_nodes": [
                "matmul_141"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_142",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hstack_143 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hstack_143"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_142",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1480
        },
        "reshape_146": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_145 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_145": "Data"
            },
            "input_nodes": [
                "matmul_145"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_146",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_147 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_147"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_146",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1430
        },
        "reshape_151": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_150 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_150": "Data"
            },
            "input_nodes": [
                "layernorm_150"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_151",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_153 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_153"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_151",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1511
        },
        "reshape_154": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "reshape(1,384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_153 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_153": "Data"
            },
            "input_nodes": [
                "matmul_153"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_154",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_155 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_155"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_154",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1478
        },
        "reshape_157": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "reshape(384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: gelu_156 (port_0)"
            ],
            "input_node_to_edge_type": {
                "gelu_156": "Data"
            },
            "input_nodes": [
                "gelu_156"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_157",
            "op_type": {
                "attrs": [
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_159 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_159"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_157",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1428
        },
        "reshape_160": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_159 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_159": "Data"
            },
            "input_nodes": [
                "matmul_159"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_160",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_161 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_161"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_160",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1396
        },
        "reshape_165": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_164 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_164": "Data"
            },
            "input_nodes": [
                "layernorm_164"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_165",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_187 (port_0)",
                "Data: matmul_167 (port_0)",
                "Data: matmul_173 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_187",
                "matmul_167",
                "matmul_173"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_165",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1560
        },
        "reshape_168": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_167 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_167": "Data"
            },
            "input_nodes": [
                "matmul_167"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_168",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_169 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_169"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_168",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1602
        },
        "reshape_17": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(1,16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_16 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_16": "Data"
            },
            "input_nodes": [
                "matmul_16"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_17",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: multiply_18 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_18"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_17",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1674
        },
        "reshape_171": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_170 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_170": "Data"
            },
            "input_nodes": [
                "hslice_170"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_171",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_179 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_179"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_171",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1558
        },
        "reshape_174": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_173 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_173": "Data"
            },
            "input_nodes": [
                "matmul_173"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_174",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_175 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_175"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "reshape_174",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1614
        },
        "reshape_177": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_176 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_176": "Data"
            },
            "input_nodes": [
                "hslice_176"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_177",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_178 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_178"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_177",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1576
        },
        "reshape_180": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(1,16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_179 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_179": "Data"
            },
            "input_nodes": [
                "matmul_179"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_180",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: multiply_181 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_181"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_180",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1526
        },
        "reshape_185": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_184 (port_0)"
            ],
            "input_node_to_edge_type": {
                "nop_184": "Data"
            },
            "input_nodes": [
                "nop_184"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_185",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_194 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_194"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_185",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1443
        },
        "reshape_188": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_187 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_187": "Data"
            },
            "input_nodes": [
                "matmul_187"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_188",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_189 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_189"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "reshape_188",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1528
        },
        "reshape_19": {
            "cache": {
                "shape": [
                    1,
                    1,
                    1,
                    384
                ]
            },
            "class": "reshape(1,1,1,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: attention_mask_1 (port_0)"
            ],
            "input_node_to_edge_type": {
                "attention_mask_1": "Data"
            },
            "input_nodes": [
                "attention_mask_1"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_19",
            "op_type": {
                "attrs": [
                    1,
                    1,
                    1,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_20 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_20"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert",
                "original_op_name": "reshape_19",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 165
        },
        "reshape_192": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "reshape(16,64,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: transpose_191 (port_0)"
            ],
            "input_node_to_edge_type": {
                "transpose_191": "Data"
            },
            "input_nodes": [
                "transpose_191"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_192",
            "op_type": {
                "attrs": [
                    16,
                    64,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_193 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_193"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_192",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1462
        },
        "reshape_195": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(1,16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_194 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_194": "Data"
            },
            "input_nodes": [
                "matmul_194"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_195",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hstack_196 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hstack_196"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_195",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1410
        },
        "reshape_199": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_198 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_198": "Data"
            },
            "input_nodes": [
                "matmul_198"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_199",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_200 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_200"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_199",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1360
        },
        "reshape_2": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_1 (port_0)"
            ],
            "input_node_to_edge_type": {
                "nop_1": "Data"
            },
            "input_nodes": [
                "nop_1"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_2",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_28 (port_0)",
                "Data: matmul_4 (port_0)",
                "Data: matmul_10 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_28",
                "matmul_4",
                "matmul_10"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_2",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1689
        },
        "reshape_204": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_203 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_203": "Data"
            },
            "input_nodes": [
                "layernorm_203"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_204",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_206 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_206"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_204",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1441
        },
        "reshape_207": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "reshape(1,384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_206 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_206": "Data"
            },
            "input_nodes": [
                "matmul_206"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_207",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_208 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_208"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_207",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1408
        },
        "reshape_210": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "reshape(384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: gelu_209 (port_0)"
            ],
            "input_node_to_edge_type": {
                "gelu_209": "Data"
            },
            "input_nodes": [
                "gelu_209"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_210",
            "op_type": {
                "attrs": [
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_212 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_212"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_210",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1358
        },
        "reshape_213": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_212 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_212": "Data"
            },
            "input_nodes": [
                "matmul_212"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_213",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_214 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_214"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_213",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1326
        },
        "reshape_218": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_217 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_217": "Data"
            },
            "input_nodes": [
                "layernorm_217"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_218",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_240 (port_0)",
                "Data: matmul_220 (port_0)",
                "Data: matmul_226 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_240",
                "matmul_220",
                "matmul_226"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_218",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1492
        },
        "reshape_221": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_220 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_220": "Data"
            },
            "input_nodes": [
                "matmul_220"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_221",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_222 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_222"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_221",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1541
        },
        "reshape_224": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_223 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_223": "Data"
            },
            "input_nodes": [
                "hslice_223"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_224",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_232 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_232"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_224",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1490
        },
        "reshape_227": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_226 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_226": "Data"
            },
            "input_nodes": [
                "matmul_226"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_227",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_228 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_228"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "reshape_227",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1556
        },
        "reshape_230": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_229 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_229": "Data"
            },
            "input_nodes": [
                "hslice_229"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_230",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_231 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_231"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_230",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1507
        },
        "reshape_233": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(1,16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_232 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_232": "Data"
            },
            "input_nodes": [
                "matmul_232"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_233",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: multiply_234 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_234"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_233",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1456
        },
        "reshape_238": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_237 (port_0)"
            ],
            "input_node_to_edge_type": {
                "nop_237": "Data"
            },
            "input_nodes": [
                "nop_237"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_238",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_247 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_247"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_238",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1373
        },
        "reshape_241": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_240 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_240": "Data"
            },
            "input_nodes": [
                "matmul_240"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_241",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_242 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_242"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "reshape_241",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1458
        },
        "reshape_245": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "reshape(16,64,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: transpose_244 (port_0)"
            ],
            "input_node_to_edge_type": {
                "transpose_244": "Data"
            },
            "input_nodes": [
                "transpose_244"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_245",
            "op_type": {
                "attrs": [
                    16,
                    64,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_246 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_246"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_245",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1392
        },
        "reshape_248": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(1,16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_247 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_247": "Data"
            },
            "input_nodes": [
                "matmul_247"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_248",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hstack_249 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hstack_249"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_248",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1340
        },
        "reshape_252": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_251 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_251": "Data"
            },
            "input_nodes": [
                "matmul_251"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_252",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_253 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_253"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_252",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1290
        },
        "reshape_257": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_256 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_256": "Data"
            },
            "input_nodes": [
                "layernorm_256"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_257",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_259 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_259"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_257",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1371
        },
        "reshape_26": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_25 (port_0)"
            ],
            "input_node_to_edge_type": {
                "nop_25": "Data"
            },
            "input_nodes": [
                "nop_25"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_26",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_35 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_35"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_26",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1633
        },
        "reshape_260": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "reshape(1,384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_259 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_259": "Data"
            },
            "input_nodes": [
                "matmul_259"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_260",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_261 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_261"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_260",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1338
        },
        "reshape_263": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "reshape(384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: gelu_262 (port_0)"
            ],
            "input_node_to_edge_type": {
                "gelu_262": "Data"
            },
            "input_nodes": [
                "gelu_262"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_263",
            "op_type": {
                "attrs": [
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_265 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_265"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_263",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1288
        },
        "reshape_266": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_265 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_265": "Data"
            },
            "input_nodes": [
                "matmul_265"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_266",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_267 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_267"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_266",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1256
        },
        "reshape_271": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_270 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_270": "Data"
            },
            "input_nodes": [
                "layernorm_270"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_271",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_293 (port_0)",
                "Data: matmul_273 (port_0)",
                "Data: matmul_279 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_293",
                "matmul_273",
                "matmul_279"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_271",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1422
        },
        "reshape_274": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_273 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_273": "Data"
            },
            "input_nodes": [
                "matmul_273"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_274",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_275 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_275"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_274",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1471
        },
        "reshape_277": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_276 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_276": "Data"
            },
            "input_nodes": [
                "hslice_276"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_277",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_285 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_285"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_277",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1420
        },
        "reshape_280": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_279 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_279": "Data"
            },
            "input_nodes": [
                "matmul_279"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_280",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_281 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_281"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "reshape_280",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1488
        },
        "reshape_283": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_282 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_282": "Data"
            },
            "input_nodes": [
                "hslice_282"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_283",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_284 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_284"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_283",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1437
        },
        "reshape_286": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(1,16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_285 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_285": "Data"
            },
            "input_nodes": [
                "matmul_285"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_286",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: multiply_287 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_287"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_286",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1386
        },
        "reshape_29": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_28 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_28": "Data"
            },
            "input_nodes": [
                "matmul_28"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_29",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_30 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_30"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "reshape_29",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1676
        },
        "reshape_291": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_290 (port_0)"
            ],
            "input_node_to_edge_type": {
                "nop_290": "Data"
            },
            "input_nodes": [
                "nop_290"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_291",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_300 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_300"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_291",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1303
        },
        "reshape_294": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_293 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_293": "Data"
            },
            "input_nodes": [
                "matmul_293"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_294",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_295 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_295"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "reshape_294",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1388
        },
        "reshape_298": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "reshape(16,64,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: transpose_297 (port_0)"
            ],
            "input_node_to_edge_type": {
                "transpose_297": "Data"
            },
            "input_nodes": [
                "transpose_297"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_298",
            "op_type": {
                "attrs": [
                    16,
                    64,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_299 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_299"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_298",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1322
        },
        "reshape_301": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(1,16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_300 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_300": "Data"
            },
            "input_nodes": [
                "matmul_300"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_301",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hstack_302 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hstack_302"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_301",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1270
        },
        "reshape_305": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_304 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_304": "Data"
            },
            "input_nodes": [
                "matmul_304"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_305",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_306 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_306"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_305",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1220
        },
        "reshape_310": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_309 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_309": "Data"
            },
            "input_nodes": [
                "layernorm_309"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_310",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_312 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_312"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_310",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1301
        },
        "reshape_313": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "reshape(1,384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_312 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_312": "Data"
            },
            "input_nodes": [
                "matmul_312"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_313",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_314 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_314"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_313",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1268
        },
        "reshape_316": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "reshape(384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: gelu_315 (port_0)"
            ],
            "input_node_to_edge_type": {
                "gelu_315": "Data"
            },
            "input_nodes": [
                "gelu_315"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_316",
            "op_type": {
                "attrs": [
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_318 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_318"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_316",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1218
        },
        "reshape_319": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_318 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_318": "Data"
            },
            "input_nodes": [
                "matmul_318"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_319",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_320 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_320"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_319",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1186
        },
        "reshape_324": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_323 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_323": "Data"
            },
            "input_nodes": [
                "layernorm_323"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_324",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_346 (port_0)",
                "Data: matmul_326 (port_0)",
                "Data: matmul_332 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_346",
                "matmul_326",
                "matmul_332"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_324",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1352
        },
        "reshape_327": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_326 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_326": "Data"
            },
            "input_nodes": [
                "matmul_326"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_327",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_328 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_328"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_327",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1401
        },
        "reshape_33": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "reshape(16,64,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: transpose_32 (port_0)"
            ],
            "input_node_to_edge_type": {
                "transpose_32": "Data"
            },
            "input_nodes": [
                "transpose_32"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_33",
            "op_type": {
                "attrs": [
                    16,
                    64,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_34 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_34"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_33",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1645
        },
        "reshape_330": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_329 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_329": "Data"
            },
            "input_nodes": [
                "hslice_329"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_330",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_338 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_338"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_330",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1350
        },
        "reshape_333": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_332 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_332": "Data"
            },
            "input_nodes": [
                "matmul_332"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_333",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_334 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_334"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "reshape_333",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1418
        },
        "reshape_336": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_335 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_335": "Data"
            },
            "input_nodes": [
                "hslice_335"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_336",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_337 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_337"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_336",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1367
        },
        "reshape_339": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(1,16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_338 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_338": "Data"
            },
            "input_nodes": [
                "matmul_338"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_339",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: multiply_340 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_340"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_339",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1316
        },
        "reshape_344": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_343 (port_0)"
            ],
            "input_node_to_edge_type": {
                "nop_343": "Data"
            },
            "input_nodes": [
                "nop_343"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_344",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_353 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_353"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_344",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1233
        },
        "reshape_347": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_346 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_346": "Data"
            },
            "input_nodes": [
                "matmul_346"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_347",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_348 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_348"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "reshape_347",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1318
        },
        "reshape_351": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "reshape(16,64,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: transpose_350 (port_0)"
            ],
            "input_node_to_edge_type": {
                "transpose_350": "Data"
            },
            "input_nodes": [
                "transpose_350"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_351",
            "op_type": {
                "attrs": [
                    16,
                    64,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_352 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_352"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_351",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1252
        },
        "reshape_354": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(1,16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_353 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_353": "Data"
            },
            "input_nodes": [
                "matmul_353"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_354",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hstack_355 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hstack_355"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_354",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1200
        },
        "reshape_358": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_357 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_357": "Data"
            },
            "input_nodes": [
                "matmul_357"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_358",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_359 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_359"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_358",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1150
        },
        "reshape_36": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(1,16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_35 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_35": "Data"
            },
            "input_nodes": [
                "matmul_35"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_36",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hstack_37 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hstack_37"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_36",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1611
        },
        "reshape_363": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_362 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_362": "Data"
            },
            "input_nodes": [
                "layernorm_362"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_363",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_365 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_365"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_363",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1231
        },
        "reshape_366": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "reshape(1,384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_365 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_365": "Data"
            },
            "input_nodes": [
                "matmul_365"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_366",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_367 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_367"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_366",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1198
        },
        "reshape_369": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "reshape(384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: gelu_368 (port_0)"
            ],
            "input_node_to_edge_type": {
                "gelu_368": "Data"
            },
            "input_nodes": [
                "gelu_368"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_369",
            "op_type": {
                "attrs": [
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_371 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_371"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_369",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1148
        },
        "reshape_372": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_371 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_371": "Data"
            },
            "input_nodes": [
                "matmul_371"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_372",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_373 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_373"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_372",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1116
        },
        "reshape_377": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_376 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_376": "Data"
            },
            "input_nodes": [
                "layernorm_376"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_377",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_399 (port_0)",
                "Data: matmul_379 (port_0)",
                "Data: matmul_385 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_399",
                "matmul_379",
                "matmul_385"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_377",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1282
        },
        "reshape_380": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_379 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_379": "Data"
            },
            "input_nodes": [
                "matmul_379"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_380",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_381 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_381"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_380",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1331
        },
        "reshape_383": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_382 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_382": "Data"
            },
            "input_nodes": [
                "hslice_382"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_383",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_391 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_391"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_383",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1280
        },
        "reshape_386": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_385 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_385": "Data"
            },
            "input_nodes": [
                "matmul_385"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_386",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_387 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_387"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "reshape_386",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1348
        },
        "reshape_389": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_388 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_388": "Data"
            },
            "input_nodes": [
                "hslice_388"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_389",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_390 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_390"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_389",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1297
        },
        "reshape_392": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(1,16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_391 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_391": "Data"
            },
            "input_nodes": [
                "matmul_391"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_392",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: multiply_393 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_393"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_392",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1246
        },
        "reshape_397": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_396 (port_0)"
            ],
            "input_node_to_edge_type": {
                "nop_396": "Data"
            },
            "input_nodes": [
                "nop_396"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_397",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_406 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_406"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_397",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1163
        },
        "reshape_40": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_39 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_39": "Data"
            },
            "input_nodes": [
                "matmul_39"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_40",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_41 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_41"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_40",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1568
        },
        "reshape_400": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_399 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_399": "Data"
            },
            "input_nodes": [
                "matmul_399"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_400",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_401 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_401"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "reshape_400",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1248
        },
        "reshape_404": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "reshape(16,64,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: transpose_403 (port_0)"
            ],
            "input_node_to_edge_type": {
                "transpose_403": "Data"
            },
            "input_nodes": [
                "transpose_403"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_404",
            "op_type": {
                "attrs": [
                    16,
                    64,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_405 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_405"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_404",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1182
        },
        "reshape_407": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(1,16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_406 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_406": "Data"
            },
            "input_nodes": [
                "matmul_406"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_407",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hstack_408 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hstack_408"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_407",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1130
        },
        "reshape_411": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_410 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_410": "Data"
            },
            "input_nodes": [
                "matmul_410"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_411",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_412 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_412"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_411",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1080
        },
        "reshape_416": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_415 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_415": "Data"
            },
            "input_nodes": [
                "layernorm_415"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_416",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_418 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_418"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_416",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1161
        },
        "reshape_419": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "reshape(1,384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_418 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_418": "Data"
            },
            "input_nodes": [
                "matmul_418"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_419",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_420 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_420"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_419",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1128
        },
        "reshape_422": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "reshape(384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: gelu_421 (port_0)"
            ],
            "input_node_to_edge_type": {
                "gelu_421": "Data"
            },
            "input_nodes": [
                "gelu_421"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_422",
            "op_type": {
                "attrs": [
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_424 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_424"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_422",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1078
        },
        "reshape_425": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_424 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_424": "Data"
            },
            "input_nodes": [
                "matmul_424"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_425",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_426 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_426"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_425",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1046
        },
        "reshape_430": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_429 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_429": "Data"
            },
            "input_nodes": [
                "layernorm_429"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_430",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_452 (port_0)",
                "Data: matmul_432 (port_0)",
                "Data: matmul_438 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_452",
                "matmul_432",
                "matmul_438"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_430",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1212
        },
        "reshape_433": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_432 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_432": "Data"
            },
            "input_nodes": [
                "matmul_432"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_433",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_434 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_434"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_433",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1261
        },
        "reshape_436": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_435 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_435": "Data"
            },
            "input_nodes": [
                "hslice_435"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_436",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_444 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_444"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_436",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1210
        },
        "reshape_439": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_438 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_438": "Data"
            },
            "input_nodes": [
                "matmul_438"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_439",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_440 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_440"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "reshape_439",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1278
        },
        "reshape_442": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_441 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_441": "Data"
            },
            "input_nodes": [
                "hslice_441"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_442",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_443 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_443"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_442",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1227
        },
        "reshape_445": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(1,16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_444 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_444": "Data"
            },
            "input_nodes": [
                "matmul_444"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_445",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: multiply_446 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_446"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_445",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1176
        },
        "reshape_45": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_44 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_44": "Data"
            },
            "input_nodes": [
                "layernorm_44"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_45",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_47 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_47"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_45",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1631
        },
        "reshape_450": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_449 (port_0)"
            ],
            "input_node_to_edge_type": {
                "nop_449": "Data"
            },
            "input_nodes": [
                "nop_449"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_450",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_459 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_459"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_450",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1093
        },
        "reshape_453": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_452 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_452": "Data"
            },
            "input_nodes": [
                "matmul_452"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_453",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_454 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_454"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "reshape_453",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1178
        },
        "reshape_457": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "reshape(16,64,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: transpose_456 (port_0)"
            ],
            "input_node_to_edge_type": {
                "transpose_456": "Data"
            },
            "input_nodes": [
                "transpose_456"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_457",
            "op_type": {
                "attrs": [
                    16,
                    64,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_458 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_458"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_457",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1112
        },
        "reshape_460": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(1,16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_459 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_459": "Data"
            },
            "input_nodes": [
                "matmul_459"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_460",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hstack_461 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hstack_461"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_460",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1060
        },
        "reshape_464": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_463 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_463": "Data"
            },
            "input_nodes": [
                "matmul_463"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_464",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_465 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_465"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_464",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1010
        },
        "reshape_469": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_468 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_468": "Data"
            },
            "input_nodes": [
                "layernorm_468"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_469",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_471 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_471"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_469",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1091
        },
        "reshape_472": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "reshape(1,384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_471 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_471": "Data"
            },
            "input_nodes": [
                "matmul_471"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_472",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_473 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_473"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_472",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1058
        },
        "reshape_475": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "reshape(384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: gelu_474 (port_0)"
            ],
            "input_node_to_edge_type": {
                "gelu_474": "Data"
            },
            "input_nodes": [
                "gelu_474"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_475",
            "op_type": {
                "attrs": [
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_477 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_477"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_475",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1008
        },
        "reshape_478": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_477 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_477": "Data"
            },
            "input_nodes": [
                "matmul_477"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_478",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_479 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_479"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_478",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 976
        },
        "reshape_48": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "reshape(1,384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_47 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_47": "Data"
            },
            "input_nodes": [
                "matmul_47"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_48",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_49 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_49"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_48",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1609
        },
        "reshape_483": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_482 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_482": "Data"
            },
            "input_nodes": [
                "layernorm_482"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_483",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_505 (port_0)",
                "Data: matmul_485 (port_0)",
                "Data: matmul_491 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_505",
                "matmul_485",
                "matmul_491"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_483",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1142
        },
        "reshape_486": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_485 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_485": "Data"
            },
            "input_nodes": [
                "matmul_485"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_486",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_487 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_487"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_486",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1191
        },
        "reshape_489": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_488 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_488": "Data"
            },
            "input_nodes": [
                "hslice_488"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_489",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_497 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_497"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_489",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1140
        },
        "reshape_492": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_491 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_491": "Data"
            },
            "input_nodes": [
                "matmul_491"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_492",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_493 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_493"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "reshape_492",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1208
        },
        "reshape_495": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_494 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_494": "Data"
            },
            "input_nodes": [
                "hslice_494"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_495",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_496 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_496"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_495",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1157
        },
        "reshape_498": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(1,16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_497 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_497": "Data"
            },
            "input_nodes": [
                "matmul_497"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_498",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: multiply_499 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_499"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_498",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1106
        },
        "reshape_5": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_4 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_4": "Data"
            },
            "input_nodes": [
                "matmul_4"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_5",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_6 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_6"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_5",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1701
        },
        "reshape_503": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_502 (port_0)"
            ],
            "input_node_to_edge_type": {
                "nop_502": "Data"
            },
            "input_nodes": [
                "nop_502"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_503",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_512 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_512"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_503",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1023
        },
        "reshape_506": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_505 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_505": "Data"
            },
            "input_nodes": [
                "matmul_505"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_506",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_507 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_507"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "reshape_506",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1108
        },
        "reshape_51": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "reshape(384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: gelu_50 (port_0)"
            ],
            "input_node_to_edge_type": {
                "gelu_50": "Data"
            },
            "input_nodes": [
                "gelu_50"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_51",
            "op_type": {
                "attrs": [
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_53 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_53"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_51",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1566
        },
        "reshape_510": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "reshape(16,64,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: transpose_509 (port_0)"
            ],
            "input_node_to_edge_type": {
                "transpose_509": "Data"
            },
            "input_nodes": [
                "transpose_509"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_510",
            "op_type": {
                "attrs": [
                    16,
                    64,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_511 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_511"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_510",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1042
        },
        "reshape_513": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(1,16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_512 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_512": "Data"
            },
            "input_nodes": [
                "matmul_512"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_513",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hstack_514 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hstack_514"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_513",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 990
        },
        "reshape_517": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_516 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_516": "Data"
            },
            "input_nodes": [
                "matmul_516"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_517",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_518 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_518"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_517",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 940
        },
        "reshape_522": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_521 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_521": "Data"
            },
            "input_nodes": [
                "layernorm_521"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_522",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_524 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_524"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_522",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1021
        },
        "reshape_525": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "reshape(1,384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_524 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_524": "Data"
            },
            "input_nodes": [
                "matmul_524"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_525",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_526 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_526"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_525",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 988
        },
        "reshape_528": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "reshape(384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: gelu_527 (port_0)"
            ],
            "input_node_to_edge_type": {
                "gelu_527": "Data"
            },
            "input_nodes": [
                "gelu_527"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_528",
            "op_type": {
                "attrs": [
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_530 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_530"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_528",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 938
        },
        "reshape_531": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_530 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_530": "Data"
            },
            "input_nodes": [
                "matmul_530"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_531",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_532 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_532"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_531",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 906
        },
        "reshape_536": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_535 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_535": "Data"
            },
            "input_nodes": [
                "layernorm_535"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_536",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_558 (port_0)",
                "Data: matmul_538 (port_0)",
                "Data: matmul_544 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_558",
                "matmul_538",
                "matmul_544"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_536",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1072
        },
        "reshape_539": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_538 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_538": "Data"
            },
            "input_nodes": [
                "matmul_538"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_539",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_540 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_540"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_539",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1121
        },
        "reshape_54": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_53 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_53": "Data"
            },
            "input_nodes": [
                "matmul_53"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_54",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_55 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_55"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_54",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1536
        },
        "reshape_542": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_541 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_541": "Data"
            },
            "input_nodes": [
                "hslice_541"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_542",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_550 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_550"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_542",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1070
        },
        "reshape_545": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_544 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_544": "Data"
            },
            "input_nodes": [
                "matmul_544"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_545",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_546 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_546"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "reshape_545",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1138
        },
        "reshape_548": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_547 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_547": "Data"
            },
            "input_nodes": [
                "hslice_547"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_548",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_549 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_549"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_548",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1087
        },
        "reshape_551": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(1,16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_550 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_550": "Data"
            },
            "input_nodes": [
                "matmul_550"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_551",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: multiply_552 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_552"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_551",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1036
        },
        "reshape_556": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_555 (port_0)"
            ],
            "input_node_to_edge_type": {
                "nop_555": "Data"
            },
            "input_nodes": [
                "nop_555"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_556",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_565 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_565"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_556",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 953
        },
        "reshape_559": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_558 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_558": "Data"
            },
            "input_nodes": [
                "matmul_558"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_559",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_560 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_560"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "reshape_559",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1038
        },
        "reshape_563": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "reshape(16,64,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: transpose_562 (port_0)"
            ],
            "input_node_to_edge_type": {
                "transpose_562": "Data"
            },
            "input_nodes": [
                "transpose_562"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_563",
            "op_type": {
                "attrs": [
                    16,
                    64,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_564 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_564"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_563",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 972
        },
        "reshape_566": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(1,16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_565 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_565": "Data"
            },
            "input_nodes": [
                "matmul_565"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_566",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hstack_567 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hstack_567"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_566",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 920
        },
        "reshape_570": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_569 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_569": "Data"
            },
            "input_nodes": [
                "matmul_569"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_570",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_571 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_571"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_570",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 870
        },
        "reshape_575": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_574 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_574": "Data"
            },
            "input_nodes": [
                "layernorm_574"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_575",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_577 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_577"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_575",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 951
        },
        "reshape_578": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "reshape(1,384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_577 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_577": "Data"
            },
            "input_nodes": [
                "matmul_577"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_578",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_579 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_579"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_578",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 918
        },
        "reshape_581": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "reshape(384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: gelu_580 (port_0)"
            ],
            "input_node_to_edge_type": {
                "gelu_580": "Data"
            },
            "input_nodes": [
                "gelu_580"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_581",
            "op_type": {
                "attrs": [
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_583 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_583"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_581",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 868
        },
        "reshape_584": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_583 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_583": "Data"
            },
            "input_nodes": [
                "matmul_583"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_584",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_585 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_585"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_584",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 836
        },
        "reshape_589": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_588 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_588": "Data"
            },
            "input_nodes": [
                "layernorm_588"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_589",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_611 (port_0)",
                "Data: matmul_591 (port_0)",
                "Data: matmul_597 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_611",
                "matmul_591",
                "matmul_597"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_589",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1002
        },
        "reshape_59": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_58 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_58": "Data"
            },
            "input_nodes": [
                "layernorm_58"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_59",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_81 (port_0)",
                "Data: matmul_61 (port_0)",
                "Data: matmul_67 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_81",
                "matmul_61",
                "matmul_67"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_59",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1659
        },
        "reshape_592": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_591 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_591": "Data"
            },
            "input_nodes": [
                "matmul_591"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_592",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_593 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_593"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_592",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1051
        },
        "reshape_595": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_594 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_594": "Data"
            },
            "input_nodes": [
                "hslice_594"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_595",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_603 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_603"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_595",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1000
        },
        "reshape_598": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_597 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_597": "Data"
            },
            "input_nodes": [
                "matmul_597"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_598",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_599 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_599"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "reshape_598",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1068
        },
        "reshape_601": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_600 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_600": "Data"
            },
            "input_nodes": [
                "hslice_600"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_601",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_602 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_602"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_601",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1017
        },
        "reshape_604": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(1,16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_603 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_603": "Data"
            },
            "input_nodes": [
                "matmul_603"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_604",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: multiply_605 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_605"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_604",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 966
        },
        "reshape_609": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_608 (port_0)"
            ],
            "input_node_to_edge_type": {
                "nop_608": "Data"
            },
            "input_nodes": [
                "nop_608"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_609",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_618 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_618"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_609",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 883
        },
        "reshape_612": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_611 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_611": "Data"
            },
            "input_nodes": [
                "matmul_611"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_612",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_613 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_613"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "reshape_612",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 968
        },
        "reshape_616": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "reshape(16,64,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: transpose_615 (port_0)"
            ],
            "input_node_to_edge_type": {
                "transpose_615": "Data"
            },
            "input_nodes": [
                "transpose_615"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_616",
            "op_type": {
                "attrs": [
                    16,
                    64,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_617 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_617"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_616",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 902
        },
        "reshape_619": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(1,16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_618 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_618": "Data"
            },
            "input_nodes": [
                "matmul_618"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_619",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hstack_620 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hstack_620"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_619",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 850
        },
        "reshape_62": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_61 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_61": "Data"
            },
            "input_nodes": [
                "matmul_61"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_62",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_63 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_63"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_62",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1679
        },
        "reshape_623": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_622 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_622": "Data"
            },
            "input_nodes": [
                "matmul_622"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_623",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_624 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_624"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_623",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 800
        },
        "reshape_628": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_627 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_627": "Data"
            },
            "input_nodes": [
                "layernorm_627"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_628",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_630 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_630"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_628",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 881
        },
        "reshape_631": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "reshape(1,384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_630 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_630": "Data"
            },
            "input_nodes": [
                "matmul_630"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_631",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_632 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_632"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_631",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 848
        },
        "reshape_634": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "reshape(384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: gelu_633 (port_0)"
            ],
            "input_node_to_edge_type": {
                "gelu_633": "Data"
            },
            "input_nodes": [
                "gelu_633"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_634",
            "op_type": {
                "attrs": [
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_636 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_636"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_634",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 798
        },
        "reshape_637": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_636 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_636": "Data"
            },
            "input_nodes": [
                "matmul_636"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_637",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_638 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_638"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_637",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 766
        },
        "reshape_642": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_641 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_641": "Data"
            },
            "input_nodes": [
                "layernorm_641"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_642",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_664 (port_0)",
                "Data: matmul_644 (port_0)",
                "Data: matmul_650 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_664",
                "matmul_644",
                "matmul_650"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_642",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 932
        },
        "reshape_645": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_644 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_644": "Data"
            },
            "input_nodes": [
                "matmul_644"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_645",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_646 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_646"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_645",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 981
        },
        "reshape_648": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_647 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_647": "Data"
            },
            "input_nodes": [
                "hslice_647"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_648",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_656 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_656"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_648",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 930
        },
        "reshape_65": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_64 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_64": "Data"
            },
            "input_nodes": [
                "hslice_64"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_65",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_73 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_73"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_65",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1657
        },
        "reshape_651": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_650 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_650": "Data"
            },
            "input_nodes": [
                "matmul_650"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_651",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_652 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_652"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "reshape_651",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 998
        },
        "reshape_654": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_653 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_653": "Data"
            },
            "input_nodes": [
                "hslice_653"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_654",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_655 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_655"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_654",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 947
        },
        "reshape_657": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(1,16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_656 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_656": "Data"
            },
            "input_nodes": [
                "matmul_656"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_657",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: multiply_658 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_658"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_657",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 896
        },
        "reshape_662": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_661 (port_0)"
            ],
            "input_node_to_edge_type": {
                "nop_661": "Data"
            },
            "input_nodes": [
                "nop_661"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_662",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_671 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_671"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_662",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 813
        },
        "reshape_665": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_664 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_664": "Data"
            },
            "input_nodes": [
                "matmul_664"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_665",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_666 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_666"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "reshape_665",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 898
        },
        "reshape_669": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "reshape(16,64,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: transpose_668 (port_0)"
            ],
            "input_node_to_edge_type": {
                "transpose_668": "Data"
            },
            "input_nodes": [
                "transpose_668"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_669",
            "op_type": {
                "attrs": [
                    16,
                    64,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_670 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_670"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_669",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 832
        },
        "reshape_672": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(1,16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_671 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_671": "Data"
            },
            "input_nodes": [
                "matmul_671"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_672",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hstack_673 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hstack_673"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_672",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 780
        },
        "reshape_676": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_675 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_675": "Data"
            },
            "input_nodes": [
                "matmul_675"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_676",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_677 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_677"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_676",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 730
        },
        "reshape_68": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_67 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_67": "Data"
            },
            "input_nodes": [
                "matmul_67"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_68",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_69 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_69"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "reshape_68",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1685
        },
        "reshape_681": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_680 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_680": "Data"
            },
            "input_nodes": [
                "layernorm_680"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_681",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_683 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_683"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_681",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 811
        },
        "reshape_684": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "reshape(1,384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_683 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_683": "Data"
            },
            "input_nodes": [
                "matmul_683"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_684",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_685 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_685"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_684",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 778
        },
        "reshape_687": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "reshape(384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: gelu_686 (port_0)"
            ],
            "input_node_to_edge_type": {
                "gelu_686": "Data"
            },
            "input_nodes": [
                "gelu_686"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_687",
            "op_type": {
                "attrs": [
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_689 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_689"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_687",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 728
        },
        "reshape_690": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_689 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_689": "Data"
            },
            "input_nodes": [
                "matmul_689"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_690",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_691 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_691"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_690",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 696
        },
        "reshape_695": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_694 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_694": "Data"
            },
            "input_nodes": [
                "layernorm_694"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_695",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_717 (port_0)",
                "Data: matmul_697 (port_0)",
                "Data: matmul_703 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_717",
                "matmul_697",
                "matmul_703"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_695",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 862
        },
        "reshape_698": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_697 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_697": "Data"
            },
            "input_nodes": [
                "matmul_697"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_698",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_699 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_699"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_698",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 911
        },
        "reshape_701": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_700 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_700": "Data"
            },
            "input_nodes": [
                "hslice_700"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_701",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_709 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_709"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_701",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 860
        },
        "reshape_704": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_703 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_703": "Data"
            },
            "input_nodes": [
                "matmul_703"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_704",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_705 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_705"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "reshape_704",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 928
        },
        "reshape_707": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_706 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_706": "Data"
            },
            "input_nodes": [
                "hslice_706"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_707",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_708 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_708"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_707",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 877
        },
        "reshape_71": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_70 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_70": "Data"
            },
            "input_nodes": [
                "hslice_70"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_71",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_72 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_72"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_71",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1666
        },
        "reshape_710": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(1,16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_709 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_709": "Data"
            },
            "input_nodes": [
                "matmul_709"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_710",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: multiply_711 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_711"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_710",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 826
        },
        "reshape_715": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_714 (port_0)"
            ],
            "input_node_to_edge_type": {
                "nop_714": "Data"
            },
            "input_nodes": [
                "nop_714"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_715",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_724 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_724"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_715",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 743
        },
        "reshape_718": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_717 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_717": "Data"
            },
            "input_nodes": [
                "matmul_717"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_718",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_719 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_719"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "reshape_718",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 828
        },
        "reshape_722": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "reshape(16,64,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: transpose_721 (port_0)"
            ],
            "input_node_to_edge_type": {
                "transpose_721": "Data"
            },
            "input_nodes": [
                "transpose_721"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_722",
            "op_type": {
                "attrs": [
                    16,
                    64,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_723 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_723"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_722",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 762
        },
        "reshape_725": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(1,16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_724 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_724": "Data"
            },
            "input_nodes": [
                "matmul_724"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_725",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hstack_726 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hstack_726"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_725",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 710
        },
        "reshape_729": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_728 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_728": "Data"
            },
            "input_nodes": [
                "matmul_728"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_729",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_730 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_730"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_729",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 660
        },
        "reshape_734": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_733 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_733": "Data"
            },
            "input_nodes": [
                "layernorm_733"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_734",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_736 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_736"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_734",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 741
        },
        "reshape_737": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "reshape(1,384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_736 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_736": "Data"
            },
            "input_nodes": [
                "matmul_736"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_737",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_738 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_738"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_737",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 708
        },
        "reshape_74": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(1,16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_73 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_73": "Data"
            },
            "input_nodes": [
                "matmul_73"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_74",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: multiply_75 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_75"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_74",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1639
        },
        "reshape_740": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "reshape(384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: gelu_739 (port_0)"
            ],
            "input_node_to_edge_type": {
                "gelu_739": "Data"
            },
            "input_nodes": [
                "gelu_739"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_740",
            "op_type": {
                "attrs": [
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_742 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_742"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_740",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 658
        },
        "reshape_743": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_742 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_742": "Data"
            },
            "input_nodes": [
                "matmul_742"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_743",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_744 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_744"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_743",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 626
        },
        "reshape_748": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_747 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_747": "Data"
            },
            "input_nodes": [
                "layernorm_747"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_748",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_770 (port_0)",
                "Data: matmul_750 (port_0)",
                "Data: matmul_756 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_770",
                "matmul_750",
                "matmul_756"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_748",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 792
        },
        "reshape_751": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_750 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_750": "Data"
            },
            "input_nodes": [
                "matmul_750"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_751",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_752 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_752"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_751",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 841
        },
        "reshape_754": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_753 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_753": "Data"
            },
            "input_nodes": [
                "hslice_753"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_754",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_762 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_762"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_754",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 790
        },
        "reshape_757": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_756 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_756": "Data"
            },
            "input_nodes": [
                "matmul_756"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_757",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_758 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_758"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "reshape_757",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 858
        },
        "reshape_760": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_759 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_759": "Data"
            },
            "input_nodes": [
                "hslice_759"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_760",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_761 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_761"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_760",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 807
        },
        "reshape_763": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(1,16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_762 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_762": "Data"
            },
            "input_nodes": [
                "matmul_762"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_763",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: multiply_764 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_764"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_763",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 756
        },
        "reshape_768": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_767 (port_0)"
            ],
            "input_node_to_edge_type": {
                "nop_767": "Data"
            },
            "input_nodes": [
                "nop_767"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_768",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_777 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_777"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_768",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 673
        },
        "reshape_771": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_770 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_770": "Data"
            },
            "input_nodes": [
                "matmul_770"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_771",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_772 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_772"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "reshape_771",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 758
        },
        "reshape_775": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "reshape(16,64,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: transpose_774 (port_0)"
            ],
            "input_node_to_edge_type": {
                "transpose_774": "Data"
            },
            "input_nodes": [
                "transpose_774"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_775",
            "op_type": {
                "attrs": [
                    16,
                    64,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_776 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_776"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_775",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 692
        },
        "reshape_778": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(1,16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_777 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_777": "Data"
            },
            "input_nodes": [
                "matmul_777"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_778",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hstack_779 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hstack_779"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_778",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 640
        },
        "reshape_782": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_781 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_781": "Data"
            },
            "input_nodes": [
                "matmul_781"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_782",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_783 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_783"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_782",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 590
        },
        "reshape_787": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_786 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_786": "Data"
            },
            "input_nodes": [
                "layernorm_786"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_787",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_789 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_789"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_787",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 671
        },
        "reshape_79": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_78 (port_0)"
            ],
            "input_node_to_edge_type": {
                "nop_78": "Data"
            },
            "input_nodes": [
                "nop_78"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_79",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_88 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_88"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_79",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1582
        },
        "reshape_790": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "reshape(1,384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_789 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_789": "Data"
            },
            "input_nodes": [
                "matmul_789"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_790",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_791 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_791"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_790",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 638
        },
        "reshape_793": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "reshape(384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: gelu_792 (port_0)"
            ],
            "input_node_to_edge_type": {
                "gelu_792": "Data"
            },
            "input_nodes": [
                "gelu_792"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_793",
            "op_type": {
                "attrs": [
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_795 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_795"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_793",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 588
        },
        "reshape_796": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_795 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_795": "Data"
            },
            "input_nodes": [
                "matmul_795"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_796",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_797 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_797"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_796",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 556
        },
        "reshape_8": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_7 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_7": "Data"
            },
            "input_nodes": [
                "hslice_7"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_8",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_16 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_16"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_8",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1687
        },
        "reshape_801": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_800 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_800": "Data"
            },
            "input_nodes": [
                "layernorm_800"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_801",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_823 (port_0)",
                "Data: matmul_803 (port_0)",
                "Data: matmul_809 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_823",
                "matmul_803",
                "matmul_809"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_801",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 722
        },
        "reshape_804": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_803 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_803": "Data"
            },
            "input_nodes": [
                "matmul_803"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_804",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_805 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_805"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_804",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 771
        },
        "reshape_807": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_806 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_806": "Data"
            },
            "input_nodes": [
                "hslice_806"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_807",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_815 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_815"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_807",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 720
        },
        "reshape_810": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_809 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_809": "Data"
            },
            "input_nodes": [
                "matmul_809"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_810",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_811 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_811"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "reshape_810",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 788
        },
        "reshape_813": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_812 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_812": "Data"
            },
            "input_nodes": [
                "hslice_812"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_813",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_814 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_814"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_813",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 737
        },
        "reshape_816": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(1,16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_815 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_815": "Data"
            },
            "input_nodes": [
                "matmul_815"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_816",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: multiply_817 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_817"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_816",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 686
        },
        "reshape_82": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_81 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_81": "Data"
            },
            "input_nodes": [
                "matmul_81"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_82",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_83 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_83"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "reshape_82",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1641
        },
        "reshape_821": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_820 (port_0)"
            ],
            "input_node_to_edge_type": {
                "nop_820": "Data"
            },
            "input_nodes": [
                "nop_820"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_821",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_830 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_830"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_821",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 603
        },
        "reshape_824": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_823 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_823": "Data"
            },
            "input_nodes": [
                "matmul_823"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_824",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_825 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_825"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "reshape_824",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 688
        },
        "reshape_828": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "reshape(16,64,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: transpose_827 (port_0)"
            ],
            "input_node_to_edge_type": {
                "transpose_827": "Data"
            },
            "input_nodes": [
                "transpose_827"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_828",
            "op_type": {
                "attrs": [
                    16,
                    64,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_829 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_829"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_828",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 622
        },
        "reshape_831": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(1,16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_830 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_830": "Data"
            },
            "input_nodes": [
                "matmul_830"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_831",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hstack_832 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hstack_832"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_831",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 570
        },
        "reshape_835": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_834 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_834": "Data"
            },
            "input_nodes": [
                "matmul_834"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_835",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_836 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_836"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_835",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 520
        },
        "reshape_840": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_839 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_839": "Data"
            },
            "input_nodes": [
                "layernorm_839"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_840",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_842 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_842"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_840",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 601
        },
        "reshape_843": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "reshape(1,384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_842 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_842": "Data"
            },
            "input_nodes": [
                "matmul_842"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_843",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_844 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_844"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_843",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 568
        },
        "reshape_846": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "reshape(384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: gelu_845 (port_0)"
            ],
            "input_node_to_edge_type": {
                "gelu_845": "Data"
            },
            "input_nodes": [
                "gelu_845"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_846",
            "op_type": {
                "attrs": [
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_848 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_848"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_846",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 518
        },
        "reshape_849": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_848 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_848": "Data"
            },
            "input_nodes": [
                "matmul_848"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_849",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_850 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_850"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_849",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 486
        },
        "reshape_854": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_853 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_853": "Data"
            },
            "input_nodes": [
                "layernorm_853"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_854",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_876 (port_0)",
                "Data: matmul_856 (port_0)",
                "Data: matmul_862 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_876",
                "matmul_856",
                "matmul_862"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_854",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 652
        },
        "reshape_857": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_856 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_856": "Data"
            },
            "input_nodes": [
                "matmul_856"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_857",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_858 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_858"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_857",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 701
        },
        "reshape_86": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "reshape(16,64,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: transpose_85 (port_0)"
            ],
            "input_node_to_edge_type": {
                "transpose_85": "Data"
            },
            "input_nodes": [
                "transpose_85"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_86",
            "op_type": {
                "attrs": [
                    16,
                    64,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_87 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_87"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_86",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1597
        },
        "reshape_860": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_859 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_859": "Data"
            },
            "input_nodes": [
                "hslice_859"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_860",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_868 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_868"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_860",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 650
        },
        "reshape_863": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_862 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_862": "Data"
            },
            "input_nodes": [
                "matmul_862"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_863",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_864 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_864"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "reshape_863",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 718
        },
        "reshape_866": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_865 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_865": "Data"
            },
            "input_nodes": [
                "hslice_865"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_866",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_867 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_867"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_866",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 667
        },
        "reshape_869": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(1,16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_868 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_868": "Data"
            },
            "input_nodes": [
                "matmul_868"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_869",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: multiply_870 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_870"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_869",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 616
        },
        "reshape_874": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_873 (port_0)"
            ],
            "input_node_to_edge_type": {
                "nop_873": "Data"
            },
            "input_nodes": [
                "nop_873"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_874",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_883 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_883"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_874",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 533
        },
        "reshape_877": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_876 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_876": "Data"
            },
            "input_nodes": [
                "matmul_876"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_877",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_878 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_878"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "reshape_877",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 618
        },
        "reshape_881": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "reshape(16,64,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: transpose_880 (port_0)"
            ],
            "input_node_to_edge_type": {
                "transpose_880": "Data"
            },
            "input_nodes": [
                "transpose_880"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_881",
            "op_type": {
                "attrs": [
                    16,
                    64,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_882 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_882"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_881",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 552
        },
        "reshape_884": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(1,16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_883 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_883": "Data"
            },
            "input_nodes": [
                "matmul_883"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_884",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hstack_885 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hstack_885"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_884",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 500
        },
        "reshape_888": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_887 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_887": "Data"
            },
            "input_nodes": [
                "matmul_887"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_888",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_889 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_889"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_888",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 450
        },
        "reshape_89": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(1,16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_88 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_88": "Data"
            },
            "input_nodes": [
                "matmul_88"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_89",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hstack_90 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hstack_90"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_89",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1550
        },
        "reshape_893": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_892 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_892": "Data"
            },
            "input_nodes": [
                "layernorm_892"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_893",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_895 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_895"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_893",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 531
        },
        "reshape_896": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "reshape(1,384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_895 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_895": "Data"
            },
            "input_nodes": [
                "matmul_895"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_896",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_897 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_897"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_896",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 498
        },
        "reshape_899": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "reshape(384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: gelu_898 (port_0)"
            ],
            "input_node_to_edge_type": {
                "gelu_898": "Data"
            },
            "input_nodes": [
                "gelu_898"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_899",
            "op_type": {
                "attrs": [
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_901 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_901"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_899",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 448
        },
        "reshape_902": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_901 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_901": "Data"
            },
            "input_nodes": [
                "matmul_901"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_902",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_903 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_903"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_902",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 416
        },
        "reshape_907": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_906 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_906": "Data"
            },
            "input_nodes": [
                "layernorm_906"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_907",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_929 (port_0)",
                "Data: matmul_909 (port_0)",
                "Data: matmul_915 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_929",
                "matmul_909",
                "matmul_915"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_907",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 582
        },
        "reshape_910": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_909 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_909": "Data"
            },
            "input_nodes": [
                "matmul_909"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_910",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_911 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_911"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_910",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 631
        },
        "reshape_913": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_912 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_912": "Data"
            },
            "input_nodes": [
                "hslice_912"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_913",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_921 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_921"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_913",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 580
        },
        "reshape_916": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_915 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_915": "Data"
            },
            "input_nodes": [
                "matmul_915"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_916",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_917 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_917"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "reshape_916",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 648
        },
        "reshape_919": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_918 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_918": "Data"
            },
            "input_nodes": [
                "hslice_918"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_919",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_920 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_920"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_919",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 597
        },
        "reshape_922": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(1,16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_921 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_921": "Data"
            },
            "input_nodes": [
                "matmul_921"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_922",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: multiply_923 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_923"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_922",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 546
        },
        "reshape_927": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_926 (port_0)"
            ],
            "input_node_to_edge_type": {
                "nop_926": "Data"
            },
            "input_nodes": [
                "nop_926"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_927",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_936 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_936"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_927",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 463
        },
        "reshape_93": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_92 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_92": "Data"
            },
            "input_nodes": [
                "matmul_92"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_93",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_94 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_94"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_93",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1500
        },
        "reshape_930": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_929 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_929": "Data"
            },
            "input_nodes": [
                "matmul_929"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_930",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_931 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_931"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "reshape_930",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 548
        },
        "reshape_934": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "reshape(16,64,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: transpose_933 (port_0)"
            ],
            "input_node_to_edge_type": {
                "transpose_933": "Data"
            },
            "input_nodes": [
                "transpose_933"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_934",
            "op_type": {
                "attrs": [
                    16,
                    64,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_935 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_935"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_934",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 482
        },
        "reshape_937": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(1,16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_936 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_936": "Data"
            },
            "input_nodes": [
                "matmul_936"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_937",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hstack_938 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hstack_938"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_937",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 430
        },
        "reshape_941": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_940 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_940": "Data"
            },
            "input_nodes": [
                "matmul_940"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_941",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_942 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_942"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_941",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 380
        },
        "reshape_946": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_945 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_945": "Data"
            },
            "input_nodes": [
                "layernorm_945"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_946",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_948 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_948"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_946",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 461
        },
        "reshape_949": {
            "cache": {
                "shape": [
                    1,
                    384,
                    4096
                ]
            },
            "class": "reshape(1,384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_948 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_948": "Data"
            },
            "input_nodes": [
                "matmul_948"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_949",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_950 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_950"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_949",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 428
        },
        "reshape_952": {
            "cache": {
                "shape": [
                    384,
                    4096
                ]
            },
            "class": "reshape(384,4096,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: gelu_951 (port_0)"
            ],
            "input_node_to_edge_type": {
                "gelu_951": "Data"
            },
            "input_nodes": [
                "gelu_951"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_952",
            "op_type": {
                "attrs": [
                    384,
                    4096
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_954 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_954"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_952",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 378
        },
        "reshape_955": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_954 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_954": "Data"
            },
            "input_nodes": [
                "matmul_954"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_955",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_956 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_956"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_955",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 346
        },
        "reshape_960": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_959 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_959": "Data"
            },
            "input_nodes": [
                "layernorm_959"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_960",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_982 (port_0)",
                "Data: matmul_962 (port_0)",
                "Data: matmul_968 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_982",
                "matmul_962",
                "matmul_968"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_960",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 512
        },
        "reshape_963": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_962 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_962": "Data"
            },
            "input_nodes": [
                "matmul_962"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_963",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_964 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_964"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query",
                "original_op_name": "reshape_963",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 561
        },
        "reshape_966": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_965 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_965": "Data"
            },
            "input_nodes": [
                "hslice_965"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_966",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_974 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_974"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_966",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 510
        },
        "reshape_969": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_968 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_968": "Data"
            },
            "input_nodes": [
                "matmul_968"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_969",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_970 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_970"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key",
                "original_op_name": "reshape_969",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 578
        },
        "reshape_972": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_971 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_971": "Data"
            },
            "input_nodes": [
                "hslice_971"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_972",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_973 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_973"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_972",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 527
        },
        "reshape_975": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(1,16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_974 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_974": "Data"
            },
            "input_nodes": [
                "matmul_974"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_975",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: multiply_976 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_976"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_975",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 476
        },
        "reshape_98": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_97 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_97": "Data"
            },
            "input_nodes": [
                "layernorm_97"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_98",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_100 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_100"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_98",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 1580
        },
        "reshape_980": {
            "cache": {
                "shape": [
                    16,
                    384,
                    384
                ]
            },
            "class": "reshape(16,384,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: nop_979 (port_0)"
            ],
            "input_node_to_edge_type": {
                "nop_979": "Data"
            },
            "input_nodes": [
                "nop_979"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_980",
            "op_type": {
                "attrs": [
                    16,
                    384,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_989 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_989"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_980",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 393
        },
        "reshape_983": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_982 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_982": "Data"
            },
            "input_nodes": [
                "matmul_982"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_983",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_984 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_984"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value",
                "original_op_name": "reshape_983",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 478
        },
        "reshape_987": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "reshape(16,64,384,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: transpose_986 (port_0)"
            ],
            "input_node_to_edge_type": {
                "transpose_986": "Data"
            },
            "input_nodes": [
                "transpose_986"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_987",
            "op_type": {
                "attrs": [
                    16,
                    64,
                    384
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: transpose_988 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "transpose_988"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_987",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 412
        },
        "reshape_990": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    64
                ]
            },
            "class": "reshape(1,16,384,64,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_989 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_989": "Data"
            },
            "input_nodes": [
                "matmul_989"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_990",
            "op_type": {
                "attrs": [
                    1,
                    16,
                    384,
                    64
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: hstack_991 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "hstack_991"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "reshape_990",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 360
        },
        "reshape_994": {
            "cache": {
                "shape": [
                    1,
                    384,
                    1024
                ]
            },
            "class": "reshape(1,384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: matmul_993 (port_0)"
            ],
            "input_node_to_edge_type": {
                "matmul_993": "Data"
            },
            "input_nodes": [
                "matmul_993"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_994",
            "op_type": {
                "attrs": [
                    1,
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: add_995 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "add_995"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_994",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 310
        },
        "reshape_999": {
            "cache": {
                "shape": [
                    384,
                    1024
                ]
            },
            "class": "reshape(384,1024,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: layernorm_998 (port_0)"
            ],
            "input_node_to_edge_type": {
                "layernorm_998": "Data"
            },
            "input_nodes": [
                "layernorm_998"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "reshape_999",
            "op_type": {
                "attrs": [
                    384,
                    1024
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "reshape"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1001 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1001"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense",
                "original_op_name": "reshape_999",
                "original_op_type": "reshape"
            },
            "type": "reshape",
            "unique_id": 391
        },
        "softmax_1031": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "softmax(-1,1,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1030 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1030": "Data"
            },
            "input_nodes": [
                "add_1030"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "softmax_1031",
            "op_type": {
                "attrs": [
                    -1,
                    true
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "softmax"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_1032 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_1032"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "softmax_1031",
                "original_op_type": "softmax"
            },
            "type": "softmax",
            "unique_id": 356
        },
        "softmax_1084": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "softmax(-1,1,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1083 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1083": "Data"
            },
            "input_nodes": [
                "add_1083"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "softmax_1084",
            "op_type": {
                "attrs": [
                    -1,
                    true
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "softmax"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_1085 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_1085"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "softmax_1084",
                "original_op_type": "softmax"
            },
            "type": "softmax",
            "unique_id": 286
        },
        "softmax_1137": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "softmax(-1,1,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1136 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1136": "Data"
            },
            "input_nodes": [
                "add_1136"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "softmax_1137",
            "op_type": {
                "attrs": [
                    -1,
                    true
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "softmax"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_1138 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_1138"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "softmax_1137",
                "original_op_type": "softmax"
            },
            "type": "softmax",
            "unique_id": 216
        },
        "softmax_1190": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "softmax(-1,1,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1189 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1189": "Data"
            },
            "input_nodes": [
                "add_1189"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "softmax_1190",
            "op_type": {
                "attrs": [
                    -1,
                    true
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "softmax"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_1191 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_1191"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "softmax_1190",
                "original_op_type": "softmax"
            },
            "type": "softmax",
            "unique_id": 152
        },
        "softmax_1243": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "softmax(-1,1,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_1242 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_1242": "Data"
            },
            "input_nodes": [
                "add_1242"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "softmax_1243",
            "op_type": {
                "attrs": [
                    -1,
                    true
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "softmax"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_1244 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_1244"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "softmax_1243",
                "original_op_type": "softmax"
            },
            "type": "softmax",
            "unique_id": 97
        },
        "softmax_130": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "softmax(-1,1,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_129 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_129": "Data"
            },
            "input_nodes": [
                "add_129"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "softmax_130",
            "op_type": {
                "attrs": [
                    -1,
                    true
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "softmax"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_131 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_131"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "softmax_130",
                "original_op_type": "softmax"
            },
            "type": "softmax",
            "unique_id": 1546
        },
        "softmax_183": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "softmax(-1,1,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_182 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_182": "Data"
            },
            "input_nodes": [
                "add_182"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "softmax_183",
            "op_type": {
                "attrs": [
                    -1,
                    true
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "softmax"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_184 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_184"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "softmax_183",
                "original_op_type": "softmax"
            },
            "type": "softmax",
            "unique_id": 1476
        },
        "softmax_236": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "softmax(-1,1,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_235 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_235": "Data"
            },
            "input_nodes": [
                "add_235"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "softmax_236",
            "op_type": {
                "attrs": [
                    -1,
                    true
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "softmax"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_237 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_237"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "softmax_236",
                "original_op_type": "softmax"
            },
            "type": "softmax",
            "unique_id": 1406
        },
        "softmax_24": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "softmax(-1,1,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_23 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_23": "Data"
            },
            "input_nodes": [
                "add_23"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "softmax_24",
            "op_type": {
                "attrs": [
                    -1,
                    true
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "softmax"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_25 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_25"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "softmax_24",
                "original_op_type": "softmax"
            },
            "type": "softmax",
            "unique_id": 1652
        },
        "softmax_289": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "softmax(-1,1,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_288 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_288": "Data"
            },
            "input_nodes": [
                "add_288"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "softmax_289",
            "op_type": {
                "attrs": [
                    -1,
                    true
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "softmax"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_290 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_290"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "softmax_289",
                "original_op_type": "softmax"
            },
            "type": "softmax",
            "unique_id": 1336
        },
        "softmax_342": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "softmax(-1,1,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_341 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_341": "Data"
            },
            "input_nodes": [
                "add_341"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "softmax_342",
            "op_type": {
                "attrs": [
                    -1,
                    true
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "softmax"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_343 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_343"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "softmax_342",
                "original_op_type": "softmax"
            },
            "type": "softmax",
            "unique_id": 1266
        },
        "softmax_395": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "softmax(-1,1,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_394 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_394": "Data"
            },
            "input_nodes": [
                "add_394"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "softmax_395",
            "op_type": {
                "attrs": [
                    -1,
                    true
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "softmax"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_396 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_396"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "softmax_395",
                "original_op_type": "softmax"
            },
            "type": "softmax",
            "unique_id": 1196
        },
        "softmax_448": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "softmax(-1,1,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_447 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_447": "Data"
            },
            "input_nodes": [
                "add_447"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "softmax_448",
            "op_type": {
                "attrs": [
                    -1,
                    true
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "softmax"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_449 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_449"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "softmax_448",
                "original_op_type": "softmax"
            },
            "type": "softmax",
            "unique_id": 1126
        },
        "softmax_501": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "softmax(-1,1,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_500 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_500": "Data"
            },
            "input_nodes": [
                "add_500"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "softmax_501",
            "op_type": {
                "attrs": [
                    -1,
                    true
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "softmax"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_502 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_502"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "softmax_501",
                "original_op_type": "softmax"
            },
            "type": "softmax",
            "unique_id": 1056
        },
        "softmax_554": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "softmax(-1,1,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_553 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_553": "Data"
            },
            "input_nodes": [
                "add_553"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "softmax_554",
            "op_type": {
                "attrs": [
                    -1,
                    true
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "softmax"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_555 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_555"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "softmax_554",
                "original_op_type": "softmax"
            },
            "type": "softmax",
            "unique_id": 986
        },
        "softmax_607": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "softmax(-1,1,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_606 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_606": "Data"
            },
            "input_nodes": [
                "add_606"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "softmax_607",
            "op_type": {
                "attrs": [
                    -1,
                    true
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "softmax"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_608 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_608"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "softmax_607",
                "original_op_type": "softmax"
            },
            "type": "softmax",
            "unique_id": 916
        },
        "softmax_660": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "softmax(-1,1,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_659 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_659": "Data"
            },
            "input_nodes": [
                "add_659"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "softmax_660",
            "op_type": {
                "attrs": [
                    -1,
                    true
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "softmax"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_661 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_661"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "softmax_660",
                "original_op_type": "softmax"
            },
            "type": "softmax",
            "unique_id": 846
        },
        "softmax_713": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "softmax(-1,1,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_712 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_712": "Data"
            },
            "input_nodes": [
                "add_712"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "softmax_713",
            "op_type": {
                "attrs": [
                    -1,
                    true
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "softmax"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_714 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_714"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "softmax_713",
                "original_op_type": "softmax"
            },
            "type": "softmax",
            "unique_id": 776
        },
        "softmax_766": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "softmax(-1,1,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_765 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_765": "Data"
            },
            "input_nodes": [
                "add_765"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "softmax_766",
            "op_type": {
                "attrs": [
                    -1,
                    true
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "softmax"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_767 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_767"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "softmax_766",
                "original_op_type": "softmax"
            },
            "type": "softmax",
            "unique_id": 706
        },
        "softmax_77": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "softmax(-1,1,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_76 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_76": "Data"
            },
            "input_nodes": [
                "add_76"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "softmax_77",
            "op_type": {
                "attrs": [
                    -1,
                    true
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "softmax"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_78 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_78"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "softmax_77",
                "original_op_type": "softmax"
            },
            "type": "softmax",
            "unique_id": 1607
        },
        "softmax_819": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "softmax(-1,1,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_818 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_818": "Data"
            },
            "input_nodes": [
                "add_818"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "softmax_819",
            "op_type": {
                "attrs": [
                    -1,
                    true
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "softmax"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_820 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_820"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "softmax_819",
                "original_op_type": "softmax"
            },
            "type": "softmax",
            "unique_id": 636
        },
        "softmax_872": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "softmax(-1,1,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_871 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_871": "Data"
            },
            "input_nodes": [
                "add_871"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "softmax_872",
            "op_type": {
                "attrs": [
                    -1,
                    true
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "softmax"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_873 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_873"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "softmax_872",
                "original_op_type": "softmax"
            },
            "type": "softmax",
            "unique_id": 566
        },
        "softmax_925": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "softmax(-1,1,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_924 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_924": "Data"
            },
            "input_nodes": [
                "add_924"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "softmax_925",
            "op_type": {
                "attrs": [
                    -1,
                    true
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "softmax"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_926 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_926"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "softmax_925",
                "original_op_type": "softmax"
            },
            "type": "softmax",
            "unique_id": 496
        },
        "softmax_978": {
            "cache": {
                "shape": [
                    1,
                    16,
                    384,
                    384
                ]
            },
            "class": "softmax(-1,1,)",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: add_977 (port_0)"
            ],
            "input_node_to_edge_type": {
                "add_977": "Data"
            },
            "input_nodes": [
                "add_977"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "softmax_978",
            "op_type": {
                "attrs": [
                    -1,
                    true
                ],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "softmax"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: nop_979 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "nop_979"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "softmax_978",
                "original_op_type": "softmax"
            },
            "type": "softmax",
            "unique_id": 426
        },
        "subtract_21": {
            "cache": {
                "shape": [
                    1,
                    1,
                    1,
                    384
                ]
            },
            "class": "subtract",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: input_0_subtract_21 (port_0)",
                "Data: nop_20 (port_1)"
            ],
            "input_node_to_edge_type": {
                "input_0_subtract_21": "Data",
                "nop_20": "Data"
            },
            "input_nodes": [
                "input_0_subtract_21",
                "nop_20"
            ],
            "input_tms": [
                [
                    {
                        "op_type": {
                            "attrs": [
                                -1,
                                384,
                                false
                            ],
                            "buda_attrs": {},
                            "named_attrs": {},
                            "type": "broadcast"
                        }
                    }
                ],
                []
            ],
            "ir": "pybuda",
            "name": "subtract_21",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {},
                "type": "subtract"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: multiply_22 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "multiply_22"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert",
                "original_op_name": "subtract_21",
                "original_op_type": "subtract"
            },
            "type": "subtract",
            "unique_id": 134
        },
        "transpose_1026": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1025 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_1025": "Data"
            },
            "input_nodes": [
                "reshape_1025"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_1026",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1027 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1027"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_1026",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 441
        },
        "transpose_1039": {
            "cache": {
                "shape": [
                    1,
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_1038 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_1038": "Data"
            },
            "input_nodes": [
                "hslice_1038"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_1039",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1040 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1040"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.19/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "transpose_1039",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 357
        },
        "transpose_1041": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1040 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_1040": "Data"
            },
            "input_nodes": [
                "reshape_1040"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_1041",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1042 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1042"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_1041",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 324
        },
        "transpose_1079": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1078 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_1078": "Data"
            },
            "input_nodes": [
                "reshape_1078"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_1079",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1080 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1080"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_1079",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 371
        },
        "transpose_1092": {
            "cache": {
                "shape": [
                    1,
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_1091 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_1091": "Data"
            },
            "input_nodes": [
                "hslice_1091"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_1092",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1093 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1093"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.20/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "transpose_1092",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 287
        },
        "transpose_1094": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1093 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_1093": "Data"
            },
            "input_nodes": [
                "reshape_1093"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_1094",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1095 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1095"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_1094",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 254
        },
        "transpose_1132": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1131 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_1131": "Data"
            },
            "input_nodes": [
                "reshape_1131"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_1132",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1133 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1133"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_1132",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 301
        },
        "transpose_1145": {
            "cache": {
                "shape": [
                    1,
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_1144 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_1144": "Data"
            },
            "input_nodes": [
                "hslice_1144"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_1145",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1146 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1146"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.21/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "transpose_1145",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 217
        },
        "transpose_1147": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1146 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_1146": "Data"
            },
            "input_nodes": [
                "reshape_1146"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_1147",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1148 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1148"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_1147",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 187
        },
        "transpose_1185": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1184 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_1184": "Data"
            },
            "input_nodes": [
                "reshape_1184"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_1185",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1186 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1186"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_1185",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 231
        },
        "transpose_1198": {
            "cache": {
                "shape": [
                    1,
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_1197 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_1197": "Data"
            },
            "input_nodes": [
                "hslice_1197"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_1198",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1199 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1199"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.22/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "transpose_1198",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 153
        },
        "transpose_1200": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1199 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_1199": "Data"
            },
            "input_nodes": [
                "reshape_1199"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_1200",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1201 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1201"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_1200",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 124
        },
        "transpose_1238": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1237 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_1237": "Data"
            },
            "input_nodes": [
                "reshape_1237"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_1238",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1239 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1239"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_1238",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 164
        },
        "transpose_125": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_124 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_124": "Data"
            },
            "input_nodes": [
                "reshape_124"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_125",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_126 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_126"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_125",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 1617
        },
        "transpose_1251": {
            "cache": {
                "shape": [
                    1,
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_1250 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_1250": "Data"
            },
            "input_nodes": [
                "hslice_1250"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_1251",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_1252 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_1252"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.23/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "transpose_1251",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 98
        },
        "transpose_1253": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_1252 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_1252": "Data"
            },
            "input_nodes": [
                "reshape_1252"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_1253",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_1254 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_1254"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_1253",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 79
        },
        "transpose_138": {
            "cache": {
                "shape": [
                    1,
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_137 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_137": "Data"
            },
            "input_nodes": [
                "hslice_137"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_138",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_139 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_139"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.2/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "transpose_138",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 1547
        },
        "transpose_140": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_139 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_139": "Data"
            },
            "input_nodes": [
                "reshape_139"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_140",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_141 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_141"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_140",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 1514
        },
        "transpose_15": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_14 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_14": "Data"
            },
            "input_nodes": [
                "reshape_14"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_15",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_16 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_16"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_15",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 1688
        },
        "transpose_178": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_177 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_177": "Data"
            },
            "input_nodes": [
                "reshape_177"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_178",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_179 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_179"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_178",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 1559
        },
        "transpose_191": {
            "cache": {
                "shape": [
                    1,
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_190 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_190": "Data"
            },
            "input_nodes": [
                "hslice_190"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_191",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_192 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_192"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.3/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "transpose_191",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 1477
        },
        "transpose_193": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_192 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_192": "Data"
            },
            "input_nodes": [
                "reshape_192"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_193",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_194 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_194"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_193",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 1444
        },
        "transpose_231": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_230 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_230": "Data"
            },
            "input_nodes": [
                "reshape_230"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_231",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_232 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_232"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_231",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 1491
        },
        "transpose_244": {
            "cache": {
                "shape": [
                    1,
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_243 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_243": "Data"
            },
            "input_nodes": [
                "hslice_243"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_244",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_245 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_245"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.4/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "transpose_244",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 1407
        },
        "transpose_246": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_245 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_245": "Data"
            },
            "input_nodes": [
                "reshape_245"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_246",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_247 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_247"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_246",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 1374
        },
        "transpose_284": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_283 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_283": "Data"
            },
            "input_nodes": [
                "reshape_283"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_284",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_285 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_285"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_284",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 1421
        },
        "transpose_297": {
            "cache": {
                "shape": [
                    1,
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_296 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_296": "Data"
            },
            "input_nodes": [
                "hslice_296"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_297",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_298 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_298"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.5/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "transpose_297",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 1337
        },
        "transpose_299": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_298 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_298": "Data"
            },
            "input_nodes": [
                "reshape_298"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_299",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_300 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_300"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_299",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 1304
        },
        "transpose_32": {
            "cache": {
                "shape": [
                    1,
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_31 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_31": "Data"
            },
            "input_nodes": [
                "hslice_31"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_32",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_33 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_33"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "transpose_32",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 1653
        },
        "transpose_337": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_336 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_336": "Data"
            },
            "input_nodes": [
                "reshape_336"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_337",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_338 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_338"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_337",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 1351
        },
        "transpose_34": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_33 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_33": "Data"
            },
            "input_nodes": [
                "reshape_33"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_34",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_35 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_35"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_34",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 1634
        },
        "transpose_350": {
            "cache": {
                "shape": [
                    1,
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_349 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_349": "Data"
            },
            "input_nodes": [
                "hslice_349"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_350",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_351 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_351"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.6/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "transpose_350",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 1267
        },
        "transpose_352": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_351 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_351": "Data"
            },
            "input_nodes": [
                "reshape_351"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_352",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_353 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_353"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_352",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 1234
        },
        "transpose_390": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_389 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_389": "Data"
            },
            "input_nodes": [
                "reshape_389"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_390",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_391 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_391"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_390",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 1281
        },
        "transpose_403": {
            "cache": {
                "shape": [
                    1,
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_402 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_402": "Data"
            },
            "input_nodes": [
                "hslice_402"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_403",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_404 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_404"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.7/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "transpose_403",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 1197
        },
        "transpose_405": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_404 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_404": "Data"
            },
            "input_nodes": [
                "reshape_404"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_405",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_406 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_406"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_405",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 1164
        },
        "transpose_443": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_442 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_442": "Data"
            },
            "input_nodes": [
                "reshape_442"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_443",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_444 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_444"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_443",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 1211
        },
        "transpose_456": {
            "cache": {
                "shape": [
                    1,
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_455 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_455": "Data"
            },
            "input_nodes": [
                "hslice_455"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_456",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_457 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_457"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.8/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "transpose_456",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 1127
        },
        "transpose_458": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_457 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_457": "Data"
            },
            "input_nodes": [
                "reshape_457"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_458",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_459 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_459"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_458",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 1094
        },
        "transpose_496": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_495 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_495": "Data"
            },
            "input_nodes": [
                "reshape_495"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_496",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_497 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_497"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_496",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 1141
        },
        "transpose_509": {
            "cache": {
                "shape": [
                    1,
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_508 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_508": "Data"
            },
            "input_nodes": [
                "hslice_508"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_509",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_510 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_510"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.9/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "transpose_509",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 1057
        },
        "transpose_511": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_510 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_510": "Data"
            },
            "input_nodes": [
                "reshape_510"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_511",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_512 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_512"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_511",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 1024
        },
        "transpose_549": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_548 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_548": "Data"
            },
            "input_nodes": [
                "reshape_548"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_549",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_550 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_550"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_549",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 1071
        },
        "transpose_562": {
            "cache": {
                "shape": [
                    1,
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_561 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_561": "Data"
            },
            "input_nodes": [
                "hslice_561"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_562",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_563 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_563"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.10/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "transpose_562",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 987
        },
        "transpose_564": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_563 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_563": "Data"
            },
            "input_nodes": [
                "reshape_563"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_564",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_565 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_565"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_564",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 954
        },
        "transpose_602": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_601 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_601": "Data"
            },
            "input_nodes": [
                "reshape_601"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_602",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_603 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_603"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_602",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 1001
        },
        "transpose_615": {
            "cache": {
                "shape": [
                    1,
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_614 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_614": "Data"
            },
            "input_nodes": [
                "hslice_614"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_615",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_616 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_616"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.11/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "transpose_615",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 917
        },
        "transpose_617": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_616 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_616": "Data"
            },
            "input_nodes": [
                "reshape_616"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_617",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_618 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_618"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_617",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 884
        },
        "transpose_655": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_654 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_654": "Data"
            },
            "input_nodes": [
                "reshape_654"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_655",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_656 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_656"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_655",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 931
        },
        "transpose_668": {
            "cache": {
                "shape": [
                    1,
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_667 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_667": "Data"
            },
            "input_nodes": [
                "hslice_667"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_668",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_669 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_669"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.12/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "transpose_668",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 847
        },
        "transpose_670": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_669 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_669": "Data"
            },
            "input_nodes": [
                "reshape_669"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_670",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_671 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_671"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_670",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 814
        },
        "transpose_708": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_707 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_707": "Data"
            },
            "input_nodes": [
                "reshape_707"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_708",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_709 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_709"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_708",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 861
        },
        "transpose_72": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_71 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_71": "Data"
            },
            "input_nodes": [
                "reshape_71"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_72",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_73 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_73"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_72",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 1658
        },
        "transpose_721": {
            "cache": {
                "shape": [
                    1,
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_720 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_720": "Data"
            },
            "input_nodes": [
                "hslice_720"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_721",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_722 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_722"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.13/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "transpose_721",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 777
        },
        "transpose_723": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_722 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_722": "Data"
            },
            "input_nodes": [
                "reshape_722"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_723",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_724 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_724"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_723",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 744
        },
        "transpose_761": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_760 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_760": "Data"
            },
            "input_nodes": [
                "reshape_760"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_761",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_762 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_762"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_761",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 791
        },
        "transpose_774": {
            "cache": {
                "shape": [
                    1,
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_773 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_773": "Data"
            },
            "input_nodes": [
                "hslice_773"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_774",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_775 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_775"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.14/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "transpose_774",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 707
        },
        "transpose_776": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_775 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_775": "Data"
            },
            "input_nodes": [
                "reshape_775"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_776",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_777 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_777"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_776",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 674
        },
        "transpose_814": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_813 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_813": "Data"
            },
            "input_nodes": [
                "reshape_813"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_814",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_815 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_815"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_814",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 721
        },
        "transpose_827": {
            "cache": {
                "shape": [
                    1,
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_826 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_826": "Data"
            },
            "input_nodes": [
                "hslice_826"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_827",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_828 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_828"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.15/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "transpose_827",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 637
        },
        "transpose_829": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_828 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_828": "Data"
            },
            "input_nodes": [
                "reshape_828"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_829",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_830 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_830"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_829",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 604
        },
        "transpose_85": {
            "cache": {
                "shape": [
                    1,
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_84 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_84": "Data"
            },
            "input_nodes": [
                "hslice_84"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_85",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_86 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_86"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "transpose_85",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 1608
        },
        "transpose_867": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_866 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_866": "Data"
            },
            "input_nodes": [
                "reshape_866"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_867",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_868 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_868"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_867",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 651
        },
        "transpose_87": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_86 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_86": "Data"
            },
            "input_nodes": [
                "reshape_86"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_87",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_88 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_88"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_87",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 1583
        },
        "transpose_880": {
            "cache": {
                "shape": [
                    1,
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_879 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_879": "Data"
            },
            "input_nodes": [
                "hslice_879"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_880",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_881 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_881"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.16/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "transpose_880",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 567
        },
        "transpose_882": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_881 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_881": "Data"
            },
            "input_nodes": [
                "reshape_881"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_882",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_883 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_883"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_882",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 534
        },
        "transpose_920": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_919 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_919": "Data"
            },
            "input_nodes": [
                "reshape_919"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_920",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_921 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_921"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_920",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 581
        },
        "transpose_933": {
            "cache": {
                "shape": [
                    1,
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_932 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_932": "Data"
            },
            "input_nodes": [
                "hslice_932"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_933",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_934 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_934"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.17/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "transpose_933",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 497
        },
        "transpose_935": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_934 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_934": "Data"
            },
            "input_nodes": [
                "reshape_934"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_935",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_936 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_936"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_935",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 464
        },
        "transpose_973": {
            "cache": {
                "shape": [
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_972 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_972": "Data"
            },
            "input_nodes": [
                "reshape_972"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_973",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_974 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_974"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_973",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 511
        },
        "transpose_986": {
            "cache": {
                "shape": [
                    1,
                    16,
                    64,
                    384
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: hslice_985 (port_0)"
            ],
            "input_node_to_edge_type": {
                "hslice_985": "Data"
            },
            "input_nodes": [
                "hslice_985"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_986",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: reshape_987 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "reshape_987"
            ],
            "pybuda": 1,
            "tags": {
                "layer": "transformers.models.bert.modeling_bert.BertForQuestionAnswering::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.18/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self",
                "original_op_name": "transpose_986",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 427
        },
        "transpose_988": {
            "cache": {
                "shape": [
                    16,
                    384,
                    64
                ]
            },
            "class": "transpose{dim0: -2, dim1: -1, z_dim_slice: -1}",
            "epoch": 0,
            "epoch_type": "Forward",
            "gradient_op": false,
            "incoming_edge_port_info": [
                "Data: reshape_987 (port_0)"
            ],
            "input_node_to_edge_type": {
                "reshape_987": "Data"
            },
            "input_nodes": [
                "reshape_987"
            ],
            "input_tms": [
                []
            ],
            "ir": "pybuda",
            "name": "transpose_988",
            "op_type": {
                "attrs": [],
                "buda_attrs": {},
                "named_attrs": {
                    "dim0": -2,
                    "dim1": -1,
                    "z_dim_slice": -1
                },
                "type": "transpose"
            },
            "opcode": "PyBudaOp",
            "outgoing_edge_port_info": [
                "Data: matmul_989 (port_0)"
            ],
            "output_df": "Float32",
            "output_nodes": [
                "matmul_989"
            ],
            "pybuda": 1,
            "tags": {
                "original_op_name": "transpose_988",
                "original_op_type": "transpose"
            },
            "type": "transpose",
            "unique_id": 394
        }
    },
    "topological_sorted_nodes": [
        "pybuda_6_i0",
        "bert.embeddings.LayerNorm.weight",
        "bert.embeddings.LayerNorm.bias",
        "layernorm_0",
        "nop_1",
        "reshape_2",
        "bert.encoder.layer.0.attention.self.query.weight",
        "matmul_4",
        "reshape_5",
        "bert.encoder.layer.0.attention.self.query.bias",
        "add_6",
        "hslice_7",
        "reshape_8",
        "bert.encoder.layer.0.attention.self.key.weight",
        "matmul_10",
        "reshape_11",
        "bert.encoder.layer.0.attention.self.key.bias",
        "add_12",
        "hslice_13",
        "reshape_14",
        "transpose_15",
        "matmul_16",
        "reshape_17",
        "input_1_multiply_18",
        "multiply_18",
        "input_0_subtract_21",
        "attention_mask_1",
        "reshape_19",
        "nop_20",
        "subtract_21",
        "input_1_multiply_22",
        "multiply_22",
        "add_23",
        "softmax_24",
        "nop_25",
        "reshape_26",
        "bert.encoder.layer.0.attention.self.value.weight",
        "matmul_28",
        "reshape_29",
        "bert.encoder.layer.0.attention.self.value.bias",
        "add_30",
        "hslice_31",
        "transpose_32",
        "reshape_33",
        "transpose_34",
        "matmul_35",
        "reshape_36",
        "hstack_37",
        "bert.encoder.layer.0.attention.output.dense.weight",
        "matmul_39",
        "reshape_40",
        "bert.encoder.layer.0.attention.output.dense.bias",
        "add_41",
        "nop_42",
        "add_43",
        "bert.encoder.layer.0.attention.output.LayerNorm.weight",
        "bert.encoder.layer.0.attention.output.LayerNorm.bias",
        "layernorm_44",
        "reshape_45",
        "bert.encoder.layer.0.intermediate.dense.weight",
        "matmul_47",
        "reshape_48",
        "bert.encoder.layer.0.intermediate.dense.bias",
        "add_49",
        "gelu_50",
        "reshape_51",
        "bert.encoder.layer.0.output.dense.weight",
        "matmul_53",
        "reshape_54",
        "bert.encoder.layer.0.output.dense.bias",
        "add_55",
        "nop_56",
        "add_57",
        "bert.encoder.layer.0.output.LayerNorm.weight",
        "bert.encoder.layer.0.output.LayerNorm.bias",
        "layernorm_58",
        "reshape_59",
        "bert.encoder.layer.1.attention.self.query.weight",
        "matmul_61",
        "reshape_62",
        "bert.encoder.layer.1.attention.self.query.bias",
        "add_63",
        "hslice_64",
        "reshape_65",
        "bert.encoder.layer.1.attention.self.key.weight",
        "matmul_67",
        "reshape_68",
        "bert.encoder.layer.1.attention.self.key.bias",
        "add_69",
        "hslice_70",
        "reshape_71",
        "transpose_72",
        "matmul_73",
        "reshape_74",
        "input_1_multiply_75",
        "multiply_75",
        "add_76",
        "softmax_77",
        "nop_78",
        "reshape_79",
        "bert.encoder.layer.1.attention.self.value.weight",
        "matmul_81",
        "reshape_82",
        "bert.encoder.layer.1.attention.self.value.bias",
        "add_83",
        "hslice_84",
        "transpose_85",
        "reshape_86",
        "transpose_87",
        "matmul_88",
        "reshape_89",
        "hstack_90",
        "bert.encoder.layer.1.attention.output.dense.weight",
        "matmul_92",
        "reshape_93",
        "bert.encoder.layer.1.attention.output.dense.bias",
        "add_94",
        "nop_95",
        "add_96",
        "bert.encoder.layer.1.attention.output.LayerNorm.weight",
        "bert.encoder.layer.1.attention.output.LayerNorm.bias",
        "layernorm_97",
        "reshape_98",
        "bert.encoder.layer.1.intermediate.dense.weight",
        "matmul_100",
        "reshape_101",
        "bert.encoder.layer.1.intermediate.dense.bias",
        "add_102",
        "gelu_103",
        "reshape_104",
        "bert.encoder.layer.1.output.dense.weight",
        "matmul_106",
        "reshape_107",
        "bert.encoder.layer.1.output.dense.bias",
        "add_108",
        "nop_109",
        "add_110",
        "bert.encoder.layer.1.output.LayerNorm.weight",
        "bert.encoder.layer.1.output.LayerNorm.bias",
        "layernorm_111",
        "reshape_112",
        "bert.encoder.layer.2.attention.self.query.weight",
        "matmul_114",
        "reshape_115",
        "bert.encoder.layer.2.attention.self.query.bias",
        "add_116",
        "hslice_117",
        "reshape_118",
        "bert.encoder.layer.2.attention.self.key.weight",
        "matmul_120",
        "reshape_121",
        "bert.encoder.layer.2.attention.self.key.bias",
        "add_122",
        "hslice_123",
        "reshape_124",
        "transpose_125",
        "matmul_126",
        "reshape_127",
        "input_1_multiply_128",
        "multiply_128",
        "add_129",
        "softmax_130",
        "nop_131",
        "reshape_132",
        "bert.encoder.layer.2.attention.self.value.weight",
        "matmul_134",
        "reshape_135",
        "bert.encoder.layer.2.attention.self.value.bias",
        "add_136",
        "hslice_137",
        "transpose_138",
        "reshape_139",
        "transpose_140",
        "matmul_141",
        "reshape_142",
        "hstack_143",
        "bert.encoder.layer.2.attention.output.dense.weight",
        "matmul_145",
        "reshape_146",
        "bert.encoder.layer.2.attention.output.dense.bias",
        "add_147",
        "nop_148",
        "add_149",
        "bert.encoder.layer.2.attention.output.LayerNorm.weight",
        "bert.encoder.layer.2.attention.output.LayerNorm.bias",
        "layernorm_150",
        "reshape_151",
        "bert.encoder.layer.2.intermediate.dense.weight",
        "matmul_153",
        "reshape_154",
        "bert.encoder.layer.2.intermediate.dense.bias",
        "add_155",
        "gelu_156",
        "reshape_157",
        "bert.encoder.layer.2.output.dense.weight",
        "matmul_159",
        "reshape_160",
        "bert.encoder.layer.2.output.dense.bias",
        "add_161",
        "nop_162",
        "add_163",
        "bert.encoder.layer.2.output.LayerNorm.weight",
        "bert.encoder.layer.2.output.LayerNorm.bias",
        "layernorm_164",
        "reshape_165",
        "bert.encoder.layer.3.attention.self.query.weight",
        "matmul_167",
        "reshape_168",
        "bert.encoder.layer.3.attention.self.query.bias",
        "add_169",
        "hslice_170",
        "reshape_171",
        "bert.encoder.layer.3.attention.self.key.weight",
        "matmul_173",
        "reshape_174",
        "bert.encoder.layer.3.attention.self.key.bias",
        "add_175",
        "hslice_176",
        "reshape_177",
        "transpose_178",
        "matmul_179",
        "reshape_180",
        "input_1_multiply_181",
        "multiply_181",
        "add_182",
        "softmax_183",
        "nop_184",
        "reshape_185",
        "bert.encoder.layer.3.attention.self.value.weight",
        "matmul_187",
        "reshape_188",
        "bert.encoder.layer.3.attention.self.value.bias",
        "add_189",
        "hslice_190",
        "transpose_191",
        "reshape_192",
        "transpose_193",
        "matmul_194",
        "reshape_195",
        "hstack_196",
        "bert.encoder.layer.3.attention.output.dense.weight",
        "matmul_198",
        "reshape_199",
        "bert.encoder.layer.3.attention.output.dense.bias",
        "add_200",
        "nop_201",
        "add_202",
        "bert.encoder.layer.3.attention.output.LayerNorm.weight",
        "bert.encoder.layer.3.attention.output.LayerNorm.bias",
        "layernorm_203",
        "reshape_204",
        "bert.encoder.layer.3.intermediate.dense.weight",
        "matmul_206",
        "reshape_207",
        "bert.encoder.layer.3.intermediate.dense.bias",
        "add_208",
        "gelu_209",
        "reshape_210",
        "bert.encoder.layer.3.output.dense.weight",
        "matmul_212",
        "reshape_213",
        "bert.encoder.layer.3.output.dense.bias",
        "add_214",
        "nop_215",
        "add_216",
        "bert.encoder.layer.3.output.LayerNorm.weight",
        "bert.encoder.layer.3.output.LayerNorm.bias",
        "layernorm_217",
        "reshape_218",
        "bert.encoder.layer.4.attention.self.query.weight",
        "matmul_220",
        "reshape_221",
        "bert.encoder.layer.4.attention.self.query.bias",
        "add_222",
        "hslice_223",
        "reshape_224",
        "bert.encoder.layer.4.attention.self.key.weight",
        "matmul_226",
        "reshape_227",
        "bert.encoder.layer.4.attention.self.key.bias",
        "add_228",
        "hslice_229",
        "reshape_230",
        "transpose_231",
        "matmul_232",
        "reshape_233",
        "input_1_multiply_234",
        "multiply_234",
        "add_235",
        "softmax_236",
        "nop_237",
        "reshape_238",
        "bert.encoder.layer.4.attention.self.value.weight",
        "matmul_240",
        "reshape_241",
        "bert.encoder.layer.4.attention.self.value.bias",
        "add_242",
        "hslice_243",
        "transpose_244",
        "reshape_245",
        "transpose_246",
        "matmul_247",
        "reshape_248",
        "hstack_249",
        "bert.encoder.layer.4.attention.output.dense.weight",
        "matmul_251",
        "reshape_252",
        "bert.encoder.layer.4.attention.output.dense.bias",
        "add_253",
        "nop_254",
        "add_255",
        "bert.encoder.layer.4.attention.output.LayerNorm.weight",
        "bert.encoder.layer.4.attention.output.LayerNorm.bias",
        "layernorm_256",
        "reshape_257",
        "bert.encoder.layer.4.intermediate.dense.weight",
        "matmul_259",
        "reshape_260",
        "bert.encoder.layer.4.intermediate.dense.bias",
        "add_261",
        "gelu_262",
        "reshape_263",
        "bert.encoder.layer.4.output.dense.weight",
        "matmul_265",
        "reshape_266",
        "bert.encoder.layer.4.output.dense.bias",
        "add_267",
        "nop_268",
        "add_269",
        "bert.encoder.layer.4.output.LayerNorm.weight",
        "bert.encoder.layer.4.output.LayerNorm.bias",
        "layernorm_270",
        "reshape_271",
        "bert.encoder.layer.5.attention.self.query.weight",
        "matmul_273",
        "reshape_274",
        "bert.encoder.layer.5.attention.self.query.bias",
        "add_275",
        "hslice_276",
        "reshape_277",
        "bert.encoder.layer.5.attention.self.key.weight",
        "matmul_279",
        "reshape_280",
        "bert.encoder.layer.5.attention.self.key.bias",
        "add_281",
        "hslice_282",
        "reshape_283",
        "transpose_284",
        "matmul_285",
        "reshape_286",
        "input_1_multiply_287",
        "multiply_287",
        "add_288",
        "softmax_289",
        "nop_290",
        "reshape_291",
        "bert.encoder.layer.5.attention.self.value.weight",
        "matmul_293",
        "reshape_294",
        "bert.encoder.layer.5.attention.self.value.bias",
        "add_295",
        "hslice_296",
        "transpose_297",
        "reshape_298",
        "transpose_299",
        "matmul_300",
        "reshape_301",
        "hstack_302",
        "bert.encoder.layer.5.attention.output.dense.weight",
        "matmul_304",
        "reshape_305",
        "bert.encoder.layer.5.attention.output.dense.bias",
        "add_306",
        "nop_307",
        "add_308",
        "bert.encoder.layer.5.attention.output.LayerNorm.weight",
        "bert.encoder.layer.5.attention.output.LayerNorm.bias",
        "layernorm_309",
        "reshape_310",
        "bert.encoder.layer.5.intermediate.dense.weight",
        "matmul_312",
        "reshape_313",
        "bert.encoder.layer.5.intermediate.dense.bias",
        "add_314",
        "gelu_315",
        "reshape_316",
        "bert.encoder.layer.5.output.dense.weight",
        "matmul_318",
        "reshape_319",
        "bert.encoder.layer.5.output.dense.bias",
        "add_320",
        "nop_321",
        "add_322",
        "bert.encoder.layer.5.output.LayerNorm.weight",
        "bert.encoder.layer.5.output.LayerNorm.bias",
        "layernorm_323",
        "reshape_324",
        "bert.encoder.layer.6.attention.self.query.weight",
        "matmul_326",
        "reshape_327",
        "bert.encoder.layer.6.attention.self.query.bias",
        "add_328",
        "hslice_329",
        "reshape_330",
        "bert.encoder.layer.6.attention.self.key.weight",
        "matmul_332",
        "reshape_333",
        "bert.encoder.layer.6.attention.self.key.bias",
        "add_334",
        "hslice_335",
        "reshape_336",
        "transpose_337",
        "matmul_338",
        "reshape_339",
        "input_1_multiply_340",
        "multiply_340",
        "add_341",
        "softmax_342",
        "nop_343",
        "reshape_344",
        "bert.encoder.layer.6.attention.self.value.weight",
        "matmul_346",
        "reshape_347",
        "bert.encoder.layer.6.attention.self.value.bias",
        "add_348",
        "hslice_349",
        "transpose_350",
        "reshape_351",
        "transpose_352",
        "matmul_353",
        "reshape_354",
        "hstack_355",
        "bert.encoder.layer.6.attention.output.dense.weight",
        "matmul_357",
        "reshape_358",
        "bert.encoder.layer.6.attention.output.dense.bias",
        "add_359",
        "nop_360",
        "add_361",
        "bert.encoder.layer.6.attention.output.LayerNorm.weight",
        "bert.encoder.layer.6.attention.output.LayerNorm.bias",
        "layernorm_362",
        "reshape_363",
        "bert.encoder.layer.6.intermediate.dense.weight",
        "matmul_365",
        "reshape_366",
        "bert.encoder.layer.6.intermediate.dense.bias",
        "add_367",
        "gelu_368",
        "reshape_369",
        "bert.encoder.layer.6.output.dense.weight",
        "matmul_371",
        "reshape_372",
        "bert.encoder.layer.6.output.dense.bias",
        "add_373",
        "nop_374",
        "add_375",
        "bert.encoder.layer.6.output.LayerNorm.weight",
        "bert.encoder.layer.6.output.LayerNorm.bias",
        "layernorm_376",
        "reshape_377",
        "bert.encoder.layer.7.attention.self.query.weight",
        "matmul_379",
        "reshape_380",
        "bert.encoder.layer.7.attention.self.query.bias",
        "add_381",
        "hslice_382",
        "reshape_383",
        "bert.encoder.layer.7.attention.self.key.weight",
        "matmul_385",
        "reshape_386",
        "bert.encoder.layer.7.attention.self.key.bias",
        "add_387",
        "hslice_388",
        "reshape_389",
        "transpose_390",
        "matmul_391",
        "reshape_392",
        "input_1_multiply_393",
        "multiply_393",
        "add_394",
        "softmax_395",
        "nop_396",
        "reshape_397",
        "bert.encoder.layer.7.attention.self.value.weight",
        "matmul_399",
        "reshape_400",
        "bert.encoder.layer.7.attention.self.value.bias",
        "add_401",
        "hslice_402",
        "transpose_403",
        "reshape_404",
        "transpose_405",
        "matmul_406",
        "reshape_407",
        "hstack_408",
        "bert.encoder.layer.7.attention.output.dense.weight",
        "matmul_410",
        "reshape_411",
        "bert.encoder.layer.7.attention.output.dense.bias",
        "add_412",
        "nop_413",
        "add_414",
        "bert.encoder.layer.7.attention.output.LayerNorm.weight",
        "bert.encoder.layer.7.attention.output.LayerNorm.bias",
        "layernorm_415",
        "reshape_416",
        "bert.encoder.layer.7.intermediate.dense.weight",
        "matmul_418",
        "reshape_419",
        "bert.encoder.layer.7.intermediate.dense.bias",
        "add_420",
        "gelu_421",
        "reshape_422",
        "bert.encoder.layer.7.output.dense.weight",
        "matmul_424",
        "reshape_425",
        "bert.encoder.layer.7.output.dense.bias",
        "add_426",
        "nop_427",
        "add_428",
        "bert.encoder.layer.7.output.LayerNorm.weight",
        "bert.encoder.layer.7.output.LayerNorm.bias",
        "layernorm_429",
        "reshape_430",
        "bert.encoder.layer.8.attention.self.query.weight",
        "matmul_432",
        "reshape_433",
        "bert.encoder.layer.8.attention.self.query.bias",
        "add_434",
        "hslice_435",
        "reshape_436",
        "bert.encoder.layer.8.attention.self.key.weight",
        "matmul_438",
        "reshape_439",
        "bert.encoder.layer.8.attention.self.key.bias",
        "add_440",
        "hslice_441",
        "reshape_442",
        "transpose_443",
        "matmul_444",
        "reshape_445",
        "input_1_multiply_446",
        "multiply_446",
        "add_447",
        "softmax_448",
        "nop_449",
        "reshape_450",
        "bert.encoder.layer.8.attention.self.value.weight",
        "matmul_452",
        "reshape_453",
        "bert.encoder.layer.8.attention.self.value.bias",
        "add_454",
        "hslice_455",
        "transpose_456",
        "reshape_457",
        "transpose_458",
        "matmul_459",
        "reshape_460",
        "hstack_461",
        "bert.encoder.layer.8.attention.output.dense.weight",
        "matmul_463",
        "reshape_464",
        "bert.encoder.layer.8.attention.output.dense.bias",
        "add_465",
        "nop_466",
        "add_467",
        "bert.encoder.layer.8.attention.output.LayerNorm.weight",
        "bert.encoder.layer.8.attention.output.LayerNorm.bias",
        "layernorm_468",
        "reshape_469",
        "bert.encoder.layer.8.intermediate.dense.weight",
        "matmul_471",
        "reshape_472",
        "bert.encoder.layer.8.intermediate.dense.bias",
        "add_473",
        "gelu_474",
        "reshape_475",
        "bert.encoder.layer.8.output.dense.weight",
        "matmul_477",
        "reshape_478",
        "bert.encoder.layer.8.output.dense.bias",
        "add_479",
        "nop_480",
        "add_481",
        "bert.encoder.layer.8.output.LayerNorm.weight",
        "bert.encoder.layer.8.output.LayerNorm.bias",
        "layernorm_482",
        "reshape_483",
        "bert.encoder.layer.9.attention.self.query.weight",
        "matmul_485",
        "reshape_486",
        "bert.encoder.layer.9.attention.self.query.bias",
        "add_487",
        "hslice_488",
        "reshape_489",
        "bert.encoder.layer.9.attention.self.key.weight",
        "matmul_491",
        "reshape_492",
        "bert.encoder.layer.9.attention.self.key.bias",
        "add_493",
        "hslice_494",
        "reshape_495",
        "transpose_496",
        "matmul_497",
        "reshape_498",
        "input_1_multiply_499",
        "multiply_499",
        "add_500",
        "softmax_501",
        "nop_502",
        "reshape_503",
        "bert.encoder.layer.9.attention.self.value.weight",
        "matmul_505",
        "reshape_506",
        "bert.encoder.layer.9.attention.self.value.bias",
        "add_507",
        "hslice_508",
        "transpose_509",
        "reshape_510",
        "transpose_511",
        "matmul_512",
        "reshape_513",
        "hstack_514",
        "bert.encoder.layer.9.attention.output.dense.weight",
        "matmul_516",
        "reshape_517",
        "bert.encoder.layer.9.attention.output.dense.bias",
        "add_518",
        "nop_519",
        "add_520",
        "bert.encoder.layer.9.attention.output.LayerNorm.weight",
        "bert.encoder.layer.9.attention.output.LayerNorm.bias",
        "layernorm_521",
        "reshape_522",
        "bert.encoder.layer.9.intermediate.dense.weight",
        "matmul_524",
        "reshape_525",
        "bert.encoder.layer.9.intermediate.dense.bias",
        "add_526",
        "gelu_527",
        "reshape_528",
        "bert.encoder.layer.9.output.dense.weight",
        "matmul_530",
        "reshape_531",
        "bert.encoder.layer.9.output.dense.bias",
        "add_532",
        "nop_533",
        "add_534",
        "bert.encoder.layer.9.output.LayerNorm.weight",
        "bert.encoder.layer.9.output.LayerNorm.bias",
        "layernorm_535",
        "reshape_536",
        "bert.encoder.layer.10.attention.self.query.weight",
        "matmul_538",
        "reshape_539",
        "bert.encoder.layer.10.attention.self.query.bias",
        "add_540",
        "hslice_541",
        "reshape_542",
        "bert.encoder.layer.10.attention.self.key.weight",
        "matmul_544",
        "reshape_545",
        "bert.encoder.layer.10.attention.self.key.bias",
        "add_546",
        "hslice_547",
        "reshape_548",
        "transpose_549",
        "matmul_550",
        "reshape_551",
        "input_1_multiply_552",
        "multiply_552",
        "add_553",
        "softmax_554",
        "nop_555",
        "reshape_556",
        "bert.encoder.layer.10.attention.self.value.weight",
        "matmul_558",
        "reshape_559",
        "bert.encoder.layer.10.attention.self.value.bias",
        "add_560",
        "hslice_561",
        "transpose_562",
        "reshape_563",
        "transpose_564",
        "matmul_565",
        "reshape_566",
        "hstack_567",
        "bert.encoder.layer.10.attention.output.dense.weight",
        "matmul_569",
        "reshape_570",
        "bert.encoder.layer.10.attention.output.dense.bias",
        "add_571",
        "nop_572",
        "add_573",
        "bert.encoder.layer.10.attention.output.LayerNorm.weight",
        "bert.encoder.layer.10.attention.output.LayerNorm.bias",
        "layernorm_574",
        "reshape_575",
        "bert.encoder.layer.10.intermediate.dense.weight",
        "matmul_577",
        "reshape_578",
        "bert.encoder.layer.10.intermediate.dense.bias",
        "add_579",
        "gelu_580",
        "reshape_581",
        "bert.encoder.layer.10.output.dense.weight",
        "matmul_583",
        "reshape_584",
        "bert.encoder.layer.10.output.dense.bias",
        "add_585",
        "nop_586",
        "add_587",
        "bert.encoder.layer.10.output.LayerNorm.weight",
        "bert.encoder.layer.10.output.LayerNorm.bias",
        "layernorm_588",
        "reshape_589",
        "bert.encoder.layer.11.attention.self.query.weight",
        "matmul_591",
        "reshape_592",
        "bert.encoder.layer.11.attention.self.query.bias",
        "add_593",
        "hslice_594",
        "reshape_595",
        "bert.encoder.layer.11.attention.self.key.weight",
        "matmul_597",
        "reshape_598",
        "bert.encoder.layer.11.attention.self.key.bias",
        "add_599",
        "hslice_600",
        "reshape_601",
        "transpose_602",
        "matmul_603",
        "reshape_604",
        "input_1_multiply_605",
        "multiply_605",
        "add_606",
        "softmax_607",
        "nop_608",
        "reshape_609",
        "bert.encoder.layer.11.attention.self.value.weight",
        "matmul_611",
        "reshape_612",
        "bert.encoder.layer.11.attention.self.value.bias",
        "add_613",
        "hslice_614",
        "transpose_615",
        "reshape_616",
        "transpose_617",
        "matmul_618",
        "reshape_619",
        "hstack_620",
        "bert.encoder.layer.11.attention.output.dense.weight",
        "matmul_622",
        "reshape_623",
        "bert.encoder.layer.11.attention.output.dense.bias",
        "add_624",
        "nop_625",
        "add_626",
        "bert.encoder.layer.11.attention.output.LayerNorm.weight",
        "bert.encoder.layer.11.attention.output.LayerNorm.bias",
        "layernorm_627",
        "reshape_628",
        "bert.encoder.layer.11.intermediate.dense.weight",
        "matmul_630",
        "reshape_631",
        "bert.encoder.layer.11.intermediate.dense.bias",
        "add_632",
        "gelu_633",
        "reshape_634",
        "bert.encoder.layer.11.output.dense.weight",
        "matmul_636",
        "reshape_637",
        "bert.encoder.layer.11.output.dense.bias",
        "add_638",
        "nop_639",
        "add_640",
        "bert.encoder.layer.11.output.LayerNorm.weight",
        "bert.encoder.layer.11.output.LayerNorm.bias",
        "layernorm_641",
        "reshape_642",
        "bert.encoder.layer.12.attention.self.query.weight",
        "matmul_644",
        "reshape_645",
        "bert.encoder.layer.12.attention.self.query.bias",
        "add_646",
        "hslice_647",
        "reshape_648",
        "bert.encoder.layer.12.attention.self.key.weight",
        "matmul_650",
        "reshape_651",
        "bert.encoder.layer.12.attention.self.key.bias",
        "add_652",
        "hslice_653",
        "reshape_654",
        "transpose_655",
        "matmul_656",
        "reshape_657",
        "input_1_multiply_658",
        "multiply_658",
        "add_659",
        "softmax_660",
        "nop_661",
        "reshape_662",
        "bert.encoder.layer.12.attention.self.value.weight",
        "matmul_664",
        "reshape_665",
        "bert.encoder.layer.12.attention.self.value.bias",
        "add_666",
        "hslice_667",
        "transpose_668",
        "reshape_669",
        "transpose_670",
        "matmul_671",
        "reshape_672",
        "hstack_673",
        "bert.encoder.layer.12.attention.output.dense.weight",
        "matmul_675",
        "reshape_676",
        "bert.encoder.layer.12.attention.output.dense.bias",
        "add_677",
        "nop_678",
        "add_679",
        "bert.encoder.layer.12.attention.output.LayerNorm.weight",
        "bert.encoder.layer.12.attention.output.LayerNorm.bias",
        "layernorm_680",
        "reshape_681",
        "bert.encoder.layer.12.intermediate.dense.weight",
        "matmul_683",
        "reshape_684",
        "bert.encoder.layer.12.intermediate.dense.bias",
        "add_685",
        "gelu_686",
        "reshape_687",
        "bert.encoder.layer.12.output.dense.weight",
        "matmul_689",
        "reshape_690",
        "bert.encoder.layer.12.output.dense.bias",
        "add_691",
        "nop_692",
        "add_693",
        "bert.encoder.layer.12.output.LayerNorm.weight",
        "bert.encoder.layer.12.output.LayerNorm.bias",
        "layernorm_694",
        "reshape_695",
        "bert.encoder.layer.13.attention.self.query.weight",
        "matmul_697",
        "reshape_698",
        "bert.encoder.layer.13.attention.self.query.bias",
        "add_699",
        "hslice_700",
        "reshape_701",
        "bert.encoder.layer.13.attention.self.key.weight",
        "matmul_703",
        "reshape_704",
        "bert.encoder.layer.13.attention.self.key.bias",
        "add_705",
        "hslice_706",
        "reshape_707",
        "transpose_708",
        "matmul_709",
        "reshape_710",
        "input_1_multiply_711",
        "multiply_711",
        "add_712",
        "softmax_713",
        "nop_714",
        "reshape_715",
        "bert.encoder.layer.13.attention.self.value.weight",
        "matmul_717",
        "reshape_718",
        "bert.encoder.layer.13.attention.self.value.bias",
        "add_719",
        "hslice_720",
        "transpose_721",
        "reshape_722",
        "transpose_723",
        "matmul_724",
        "reshape_725",
        "hstack_726",
        "bert.encoder.layer.13.attention.output.dense.weight",
        "matmul_728",
        "reshape_729",
        "bert.encoder.layer.13.attention.output.dense.bias",
        "add_730",
        "nop_731",
        "add_732",
        "bert.encoder.layer.13.attention.output.LayerNorm.weight",
        "bert.encoder.layer.13.attention.output.LayerNorm.bias",
        "layernorm_733",
        "reshape_734",
        "bert.encoder.layer.13.intermediate.dense.weight",
        "matmul_736",
        "reshape_737",
        "bert.encoder.layer.13.intermediate.dense.bias",
        "add_738",
        "gelu_739",
        "reshape_740",
        "bert.encoder.layer.13.output.dense.weight",
        "matmul_742",
        "reshape_743",
        "bert.encoder.layer.13.output.dense.bias",
        "add_744",
        "nop_745",
        "add_746",
        "bert.encoder.layer.13.output.LayerNorm.weight",
        "bert.encoder.layer.13.output.LayerNorm.bias",
        "layernorm_747",
        "reshape_748",
        "bert.encoder.layer.14.attention.self.query.weight",
        "matmul_750",
        "reshape_751",
        "bert.encoder.layer.14.attention.self.query.bias",
        "add_752",
        "hslice_753",
        "reshape_754",
        "bert.encoder.layer.14.attention.self.key.weight",
        "matmul_756",
        "reshape_757",
        "bert.encoder.layer.14.attention.self.key.bias",
        "add_758",
        "hslice_759",
        "reshape_760",
        "transpose_761",
        "matmul_762",
        "reshape_763",
        "input_1_multiply_764",
        "multiply_764",
        "add_765",
        "softmax_766",
        "nop_767",
        "reshape_768",
        "bert.encoder.layer.14.attention.self.value.weight",
        "matmul_770",
        "reshape_771",
        "bert.encoder.layer.14.attention.self.value.bias",
        "add_772",
        "hslice_773",
        "transpose_774",
        "reshape_775",
        "transpose_776",
        "matmul_777",
        "reshape_778",
        "hstack_779",
        "bert.encoder.layer.14.attention.output.dense.weight",
        "matmul_781",
        "reshape_782",
        "bert.encoder.layer.14.attention.output.dense.bias",
        "add_783",
        "nop_784",
        "add_785",
        "bert.encoder.layer.14.attention.output.LayerNorm.weight",
        "bert.encoder.layer.14.attention.output.LayerNorm.bias",
        "layernorm_786",
        "reshape_787",
        "bert.encoder.layer.14.intermediate.dense.weight",
        "matmul_789",
        "reshape_790",
        "bert.encoder.layer.14.intermediate.dense.bias",
        "add_791",
        "gelu_792",
        "reshape_793",
        "bert.encoder.layer.14.output.dense.weight",
        "matmul_795",
        "reshape_796",
        "bert.encoder.layer.14.output.dense.bias",
        "add_797",
        "nop_798",
        "add_799",
        "bert.encoder.layer.14.output.LayerNorm.weight",
        "bert.encoder.layer.14.output.LayerNorm.bias",
        "layernorm_800",
        "reshape_801",
        "bert.encoder.layer.15.attention.self.query.weight",
        "matmul_803",
        "reshape_804",
        "bert.encoder.layer.15.attention.self.query.bias",
        "add_805",
        "hslice_806",
        "reshape_807",
        "bert.encoder.layer.15.attention.self.key.weight",
        "matmul_809",
        "reshape_810",
        "bert.encoder.layer.15.attention.self.key.bias",
        "add_811",
        "hslice_812",
        "reshape_813",
        "transpose_814",
        "matmul_815",
        "reshape_816",
        "input_1_multiply_817",
        "multiply_817",
        "add_818",
        "softmax_819",
        "nop_820",
        "reshape_821",
        "bert.encoder.layer.15.attention.self.value.weight",
        "matmul_823",
        "reshape_824",
        "bert.encoder.layer.15.attention.self.value.bias",
        "add_825",
        "hslice_826",
        "transpose_827",
        "reshape_828",
        "transpose_829",
        "matmul_830",
        "reshape_831",
        "hstack_832",
        "bert.encoder.layer.15.attention.output.dense.weight",
        "matmul_834",
        "reshape_835",
        "bert.encoder.layer.15.attention.output.dense.bias",
        "add_836",
        "nop_837",
        "add_838",
        "bert.encoder.layer.15.attention.output.LayerNorm.weight",
        "bert.encoder.layer.15.attention.output.LayerNorm.bias",
        "layernorm_839",
        "reshape_840",
        "bert.encoder.layer.15.intermediate.dense.weight",
        "matmul_842",
        "reshape_843",
        "bert.encoder.layer.15.intermediate.dense.bias",
        "add_844",
        "gelu_845",
        "reshape_846",
        "bert.encoder.layer.15.output.dense.weight",
        "matmul_848",
        "reshape_849",
        "bert.encoder.layer.15.output.dense.bias",
        "add_850",
        "nop_851",
        "add_852",
        "bert.encoder.layer.15.output.LayerNorm.weight",
        "bert.encoder.layer.15.output.LayerNorm.bias",
        "layernorm_853",
        "reshape_854",
        "bert.encoder.layer.16.attention.self.query.weight",
        "matmul_856",
        "reshape_857",
        "bert.encoder.layer.16.attention.self.query.bias",
        "add_858",
        "hslice_859",
        "reshape_860",
        "bert.encoder.layer.16.attention.self.key.weight",
        "matmul_862",
        "reshape_863",
        "bert.encoder.layer.16.attention.self.key.bias",
        "add_864",
        "hslice_865",
        "reshape_866",
        "transpose_867",
        "matmul_868",
        "reshape_869",
        "input_1_multiply_870",
        "multiply_870",
        "add_871",
        "softmax_872",
        "nop_873",
        "reshape_874",
        "bert.encoder.layer.16.attention.self.value.weight",
        "matmul_876",
        "reshape_877",
        "bert.encoder.layer.16.attention.self.value.bias",
        "add_878",
        "hslice_879",
        "transpose_880",
        "reshape_881",
        "transpose_882",
        "matmul_883",
        "reshape_884",
        "hstack_885",
        "bert.encoder.layer.16.attention.output.dense.weight",
        "matmul_887",
        "reshape_888",
        "bert.encoder.layer.16.attention.output.dense.bias",
        "add_889",
        "nop_890",
        "add_891",
        "bert.encoder.layer.16.attention.output.LayerNorm.weight",
        "bert.encoder.layer.16.attention.output.LayerNorm.bias",
        "layernorm_892",
        "reshape_893",
        "bert.encoder.layer.16.intermediate.dense.weight",
        "matmul_895",
        "reshape_896",
        "bert.encoder.layer.16.intermediate.dense.bias",
        "add_897",
        "gelu_898",
        "reshape_899",
        "bert.encoder.layer.16.output.dense.weight",
        "matmul_901",
        "reshape_902",
        "bert.encoder.layer.16.output.dense.bias",
        "add_903",
        "nop_904",
        "add_905",
        "bert.encoder.layer.16.output.LayerNorm.weight",
        "bert.encoder.layer.16.output.LayerNorm.bias",
        "layernorm_906",
        "reshape_907",
        "bert.encoder.layer.17.attention.self.query.weight",
        "matmul_909",
        "reshape_910",
        "bert.encoder.layer.17.attention.self.query.bias",
        "add_911",
        "hslice_912",
        "reshape_913",
        "bert.encoder.layer.17.attention.self.key.weight",
        "matmul_915",
        "reshape_916",
        "bert.encoder.layer.17.attention.self.key.bias",
        "add_917",
        "hslice_918",
        "reshape_919",
        "transpose_920",
        "matmul_921",
        "reshape_922",
        "input_1_multiply_923",
        "multiply_923",
        "add_924",
        "softmax_925",
        "nop_926",
        "reshape_927",
        "bert.encoder.layer.17.attention.self.value.weight",
        "matmul_929",
        "reshape_930",
        "bert.encoder.layer.17.attention.self.value.bias",
        "add_931",
        "hslice_932",
        "transpose_933",
        "reshape_934",
        "transpose_935",
        "matmul_936",
        "reshape_937",
        "hstack_938",
        "bert.encoder.layer.17.attention.output.dense.weight",
        "matmul_940",
        "reshape_941",
        "bert.encoder.layer.17.attention.output.dense.bias",
        "add_942",
        "nop_943",
        "add_944",
        "bert.encoder.layer.17.attention.output.LayerNorm.weight",
        "bert.encoder.layer.17.attention.output.LayerNorm.bias",
        "layernorm_945",
        "reshape_946",
        "bert.encoder.layer.17.intermediate.dense.weight",
        "matmul_948",
        "reshape_949",
        "bert.encoder.layer.17.intermediate.dense.bias",
        "add_950",
        "gelu_951",
        "reshape_952",
        "bert.encoder.layer.17.output.dense.weight",
        "matmul_954",
        "reshape_955",
        "bert.encoder.layer.17.output.dense.bias",
        "add_956",
        "nop_957",
        "add_958",
        "bert.encoder.layer.17.output.LayerNorm.weight",
        "bert.encoder.layer.17.output.LayerNorm.bias",
        "layernorm_959",
        "reshape_960",
        "bert.encoder.layer.18.attention.self.query.weight",
        "matmul_962",
        "reshape_963",
        "bert.encoder.layer.18.attention.self.query.bias",
        "add_964",
        "hslice_965",
        "reshape_966",
        "bert.encoder.layer.18.attention.self.key.weight",
        "matmul_968",
        "reshape_969",
        "bert.encoder.layer.18.attention.self.key.bias",
        "add_970",
        "hslice_971",
        "reshape_972",
        "transpose_973",
        "matmul_974",
        "reshape_975",
        "input_1_multiply_976",
        "multiply_976",
        "add_977",
        "softmax_978",
        "nop_979",
        "reshape_980",
        "bert.encoder.layer.18.attention.self.value.weight",
        "matmul_982",
        "reshape_983",
        "bert.encoder.layer.18.attention.self.value.bias",
        "add_984",
        "hslice_985",
        "transpose_986",
        "reshape_987",
        "transpose_988",
        "matmul_989",
        "reshape_990",
        "hstack_991",
        "bert.encoder.layer.18.attention.output.dense.weight",
        "matmul_993",
        "reshape_994",
        "bert.encoder.layer.18.attention.output.dense.bias",
        "add_995",
        "nop_996",
        "add_997",
        "bert.encoder.layer.18.attention.output.LayerNorm.weight",
        "bert.encoder.layer.18.attention.output.LayerNorm.bias",
        "layernorm_998",
        "reshape_999",
        "bert.encoder.layer.18.intermediate.dense.weight",
        "matmul_1001",
        "reshape_1002",
        "bert.encoder.layer.18.intermediate.dense.bias",
        "add_1003",
        "gelu_1004",
        "reshape_1005",
        "bert.encoder.layer.18.output.dense.weight",
        "matmul_1007",
        "reshape_1008",
        "bert.encoder.layer.18.output.dense.bias",
        "add_1009",
        "nop_1010",
        "add_1011",
        "bert.encoder.layer.18.output.LayerNorm.weight",
        "bert.encoder.layer.18.output.LayerNorm.bias",
        "layernorm_1012",
        "reshape_1013",
        "bert.encoder.layer.19.attention.self.query.weight",
        "matmul_1015",
        "reshape_1016",
        "bert.encoder.layer.19.attention.self.query.bias",
        "add_1017",
        "hslice_1018",
        "reshape_1019",
        "bert.encoder.layer.19.attention.self.key.weight",
        "matmul_1021",
        "reshape_1022",
        "bert.encoder.layer.19.attention.self.key.bias",
        "add_1023",
        "hslice_1024",
        "reshape_1025",
        "transpose_1026",
        "matmul_1027",
        "reshape_1028",
        "input_1_multiply_1029",
        "multiply_1029",
        "add_1030",
        "softmax_1031",
        "nop_1032",
        "reshape_1033",
        "bert.encoder.layer.19.attention.self.value.weight",
        "matmul_1035",
        "reshape_1036",
        "bert.encoder.layer.19.attention.self.value.bias",
        "add_1037",
        "hslice_1038",
        "transpose_1039",
        "reshape_1040",
        "transpose_1041",
        "matmul_1042",
        "reshape_1043",
        "hstack_1044",
        "bert.encoder.layer.19.attention.output.dense.weight",
        "matmul_1046",
        "reshape_1047",
        "bert.encoder.layer.19.attention.output.dense.bias",
        "add_1048",
        "nop_1049",
        "add_1050",
        "bert.encoder.layer.19.attention.output.LayerNorm.weight",
        "bert.encoder.layer.19.attention.output.LayerNorm.bias",
        "layernorm_1051",
        "reshape_1052",
        "bert.encoder.layer.19.intermediate.dense.weight",
        "matmul_1054",
        "reshape_1055",
        "bert.encoder.layer.19.intermediate.dense.bias",
        "add_1056",
        "gelu_1057",
        "reshape_1058",
        "bert.encoder.layer.19.output.dense.weight",
        "matmul_1060",
        "reshape_1061",
        "bert.encoder.layer.19.output.dense.bias",
        "add_1062",
        "nop_1063",
        "add_1064",
        "bert.encoder.layer.19.output.LayerNorm.weight",
        "bert.encoder.layer.19.output.LayerNorm.bias",
        "layernorm_1065",
        "reshape_1066",
        "bert.encoder.layer.20.attention.self.query.weight",
        "matmul_1068",
        "reshape_1069",
        "bert.encoder.layer.20.attention.self.query.bias",
        "add_1070",
        "hslice_1071",
        "reshape_1072",
        "bert.encoder.layer.20.attention.self.key.weight",
        "matmul_1074",
        "reshape_1075",
        "bert.encoder.layer.20.attention.self.key.bias",
        "add_1076",
        "hslice_1077",
        "reshape_1078",
        "transpose_1079",
        "matmul_1080",
        "reshape_1081",
        "input_1_multiply_1082",
        "multiply_1082",
        "add_1083",
        "softmax_1084",
        "nop_1085",
        "reshape_1086",
        "bert.encoder.layer.20.attention.self.value.weight",
        "matmul_1088",
        "reshape_1089",
        "bert.encoder.layer.20.attention.self.value.bias",
        "add_1090",
        "hslice_1091",
        "transpose_1092",
        "reshape_1093",
        "transpose_1094",
        "matmul_1095",
        "reshape_1096",
        "hstack_1097",
        "bert.encoder.layer.20.attention.output.dense.weight",
        "matmul_1099",
        "reshape_1100",
        "bert.encoder.layer.20.attention.output.dense.bias",
        "add_1101",
        "nop_1102",
        "add_1103",
        "bert.encoder.layer.20.attention.output.LayerNorm.weight",
        "bert.encoder.layer.20.attention.output.LayerNorm.bias",
        "layernorm_1104",
        "reshape_1105",
        "bert.encoder.layer.20.intermediate.dense.weight",
        "matmul_1107",
        "reshape_1108",
        "bert.encoder.layer.20.intermediate.dense.bias",
        "add_1109",
        "gelu_1110",
        "reshape_1111",
        "bert.encoder.layer.20.output.dense.weight",
        "matmul_1113",
        "reshape_1114",
        "bert.encoder.layer.20.output.dense.bias",
        "add_1115",
        "nop_1116",
        "add_1117",
        "bert.encoder.layer.20.output.LayerNorm.weight",
        "bert.encoder.layer.20.output.LayerNorm.bias",
        "layernorm_1118",
        "reshape_1119",
        "bert.encoder.layer.21.attention.self.query.weight",
        "matmul_1121",
        "reshape_1122",
        "bert.encoder.layer.21.attention.self.query.bias",
        "add_1123",
        "hslice_1124",
        "reshape_1125",
        "bert.encoder.layer.21.attention.self.key.weight",
        "matmul_1127",
        "reshape_1128",
        "bert.encoder.layer.21.attention.self.key.bias",
        "add_1129",
        "hslice_1130",
        "reshape_1131",
        "transpose_1132",
        "matmul_1133",
        "reshape_1134",
        "input_1_multiply_1135",
        "multiply_1135",
        "add_1136",
        "softmax_1137",
        "nop_1138",
        "reshape_1139",
        "bert.encoder.layer.21.attention.self.value.weight",
        "matmul_1141",
        "reshape_1142",
        "bert.encoder.layer.21.attention.self.value.bias",
        "add_1143",
        "hslice_1144",
        "transpose_1145",
        "reshape_1146",
        "transpose_1147",
        "matmul_1148",
        "reshape_1149",
        "hstack_1150",
        "bert.encoder.layer.21.attention.output.dense.weight",
        "matmul_1152",
        "reshape_1153",
        "bert.encoder.layer.21.attention.output.dense.bias",
        "add_1154",
        "nop_1155",
        "add_1156",
        "bert.encoder.layer.21.attention.output.LayerNorm.weight",
        "bert.encoder.layer.21.attention.output.LayerNorm.bias",
        "layernorm_1157",
        "reshape_1158",
        "bert.encoder.layer.21.intermediate.dense.weight",
        "matmul_1160",
        "reshape_1161",
        "bert.encoder.layer.21.intermediate.dense.bias",
        "add_1162",
        "gelu_1163",
        "reshape_1164",
        "bert.encoder.layer.21.output.dense.weight",
        "matmul_1166",
        "reshape_1167",
        "bert.encoder.layer.21.output.dense.bias",
        "add_1168",
        "nop_1169",
        "add_1170",
        "bert.encoder.layer.21.output.LayerNorm.weight",
        "bert.encoder.layer.21.output.LayerNorm.bias",
        "layernorm_1171",
        "reshape_1172",
        "bert.encoder.layer.22.attention.self.query.weight",
        "matmul_1174",
        "reshape_1175",
        "bert.encoder.layer.22.attention.self.query.bias",
        "add_1176",
        "hslice_1177",
        "reshape_1178",
        "bert.encoder.layer.22.attention.self.key.weight",
        "matmul_1180",
        "reshape_1181",
        "bert.encoder.layer.22.attention.self.key.bias",
        "add_1182",
        "hslice_1183",
        "reshape_1184",
        "transpose_1185",
        "matmul_1186",
        "reshape_1187",
        "input_1_multiply_1188",
        "multiply_1188",
        "add_1189",
        "softmax_1190",
        "nop_1191",
        "reshape_1192",
        "bert.encoder.layer.22.attention.self.value.weight",
        "matmul_1194",
        "reshape_1195",
        "bert.encoder.layer.22.attention.self.value.bias",
        "add_1196",
        "hslice_1197",
        "transpose_1198",
        "reshape_1199",
        "transpose_1200",
        "matmul_1201",
        "reshape_1202",
        "hstack_1203",
        "bert.encoder.layer.22.attention.output.dense.weight",
        "matmul_1205",
        "reshape_1206",
        "bert.encoder.layer.22.attention.output.dense.bias",
        "add_1207",
        "nop_1208",
        "add_1209",
        "bert.encoder.layer.22.attention.output.LayerNorm.weight",
        "bert.encoder.layer.22.attention.output.LayerNorm.bias",
        "layernorm_1210",
        "reshape_1211",
        "bert.encoder.layer.22.intermediate.dense.weight",
        "matmul_1213",
        "reshape_1214",
        "bert.encoder.layer.22.intermediate.dense.bias",
        "add_1215",
        "gelu_1216",
        "reshape_1217",
        "bert.encoder.layer.22.output.dense.weight",
        "matmul_1219",
        "reshape_1220",
        "bert.encoder.layer.22.output.dense.bias",
        "add_1221",
        "nop_1222",
        "add_1223",
        "bert.encoder.layer.22.output.LayerNorm.weight",
        "bert.encoder.layer.22.output.LayerNorm.bias",
        "layernorm_1224",
        "reshape_1225",
        "bert.encoder.layer.23.attention.self.query.weight",
        "matmul_1227",
        "reshape_1228",
        "bert.encoder.layer.23.attention.self.query.bias",
        "add_1229",
        "hslice_1230",
        "reshape_1231",
        "bert.encoder.layer.23.attention.self.key.weight",
        "matmul_1233",
        "reshape_1234",
        "bert.encoder.layer.23.attention.self.key.bias",
        "add_1235",
        "hslice_1236",
        "reshape_1237",
        "transpose_1238",
        "matmul_1239",
        "reshape_1240",
        "input_1_multiply_1241",
        "multiply_1241",
        "add_1242",
        "softmax_1243",
        "nop_1244",
        "reshape_1245",
        "bert.encoder.layer.23.attention.self.value.weight",
        "matmul_1247",
        "reshape_1248",
        "bert.encoder.layer.23.attention.self.value.bias",
        "add_1249",
        "hslice_1250",
        "transpose_1251",
        "reshape_1252",
        "transpose_1253",
        "matmul_1254",
        "reshape_1255",
        "hstack_1256",
        "bert.encoder.layer.23.attention.output.dense.weight",
        "matmul_1258",
        "reshape_1259",
        "bert.encoder.layer.23.attention.output.dense.bias",
        "add_1260",
        "nop_1261",
        "add_1262",
        "bert.encoder.layer.23.attention.output.LayerNorm.weight",
        "bert.encoder.layer.23.attention.output.LayerNorm.bias",
        "layernorm_1263",
        "reshape_1264",
        "bert.encoder.layer.23.intermediate.dense.weight",
        "matmul_1266",
        "reshape_1267",
        "bert.encoder.layer.23.intermediate.dense.bias",
        "add_1268",
        "gelu_1269",
        "reshape_1270",
        "bert.encoder.layer.23.output.dense.weight",
        "matmul_1272",
        "reshape_1273",
        "bert.encoder.layer.23.output.dense.bias",
        "add_1274",
        "nop_1275",
        "add_1276",
        "bert.encoder.layer.23.output.LayerNorm.weight",
        "bert.encoder.layer.23.output.LayerNorm.bias",
        "layernorm_1277",
        "reshape_1278",
        "qa_outputs.weight",
        "matmul_1281",
        "qa_outputs.bias",
        "add_1283",
        "reshape_1284",
        "reshape_1285",
        "bert_large_tt_1.output_reshape_1285",
        "qa_outputs.weight_fork_clone19",
        "matmul_1288",
        "qa_outputs.bias_fork_clone12",
        "add_1290",
        "reshape_1291",
        "reshape_1292",
        "bert_large_tt_1.output_reshape_1292"
    ]
}