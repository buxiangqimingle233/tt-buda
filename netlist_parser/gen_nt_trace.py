
from tt_simple_netlist import SimpNetlist
from netrace.nt_trace_handler import NTHeader, NTPacket, NTTrace
from tt_pipe import Pipe
from copy import deepcopy
from collections import deque
from typing import List
from tt_coordinate import OnChipCoordinate


def generate_packets(pipe: Pipe, current_cycle) -> List[NTPacket]:
    '''
        This translate the pipe with probably multiple sources and destinations to a list of unicast packets. 
    '''
    ret = []

    # FIXME: Collectives may involve multiple unicast packets, but they share the same pipe id for now. Fix this
    for src in pipe.src_buffers():
        for dst in pipe.dst_buffers():
            nt_packet = NTPacket()
            nt_packet.cycle = current_cycle
            nt_packet.id = pipe._id
            nt_packet.pkt_size = pipe.msg_size            # In bytes
            nt_packet.type = src.root["is_scatter"]       # Change this
            nt_packet.src = src.loc().to("netrace")
            nt_packet.dst = dst.loc().to("netrace")
            nt_packet.node_types = 0                     # FIXME: Not support Scatter/Reduce/Boradcast right now
            nt_packet.deps = [p._id for p in pipe.succeed_pipes()]
            nt_packet.num_deps = len(nt_packet.deps)
            ret.append(nt_packet)
    return ret
    

def generate_trace(netlist, out_trace):
    
    for name, temporal_epoch in netlist.temporal_epochs.items():
        # Prepocess phase 0: connect the buffers that feeds to an op and is generated by that op. 
        
        # The precise buffer connections are maintained by tensix kernels. The compiler may introduce many latency optimization methods such as async noc write. 
        # FIXME: Our approximation here just assumes that all op's output buffers/pipes should be blocked until all inputs arrives. 
        # This could collapse the temporal dimensions (the 't' dim in operations) thus disallows all pipelines. 
        for graph in temporal_epoch.graphs.values():
            for op in graph.op_names():
                in_buffers = graph.get_buffers(graph.ops[op], "out")       # "out" means the buffer is the output of the pipe, thus it is the input of the op
                out_buffers = graph.get_buffers(graph.ops[op], "in")
                coords = [
                    OnChipCoordinate(*c, "netlist", graph.device)  # netlist coordinates
                    for c in graph.get_op_netlist_coords(op)       
                ]
                # Get the input and output buffers for op at coords c
                for c in coords:
                    in_buffers_at_c = [i for i in in_buffers.values() if i._id in graph.get_core_buffers(c.to("netlist"))]
                    out_buffers_at_c = [o for o in out_buffers.values() if o._id in graph.get_core_buffers(c.to("netlist"))]

                    # We maintain an all to all connection for buffers that are generated by the same op at the same core.
                    for i in in_buffers_at_c:
                        i.connect_buffers_of_same_op(out_buffers_at_c, "out")
                    for o in out_buffers_at_c:
                        o.connect_buffers_of_same_op(in_buffers_at_c, "in")

        for i in temporal_epoch.buffers.values():
            if not i.is_dram_buffer():
                assert len(i.input_buffer_of_same_op) > 0 or len(i.output_buffer_of_same_op) > 0

        # Preprocess: phase-1: get the number of dependent nodes, reduce bfs algorithm complexity
        pipes = temporal_epoch.pipes
        streams = temporal_epoch.streams

        for p in pipes.values():
            p.dep_size = len(p.predecessor_pipes())
            p.msg_size = p.get_msg_size()

        # find the start node
        def from_dram(p):
            return all("dram" in src.root["buffer_type"] for src in p.src_buffers())

        queue = deque()
        for p in pipes.values():
            # no dependent nodes and all sources in dram
            if p.dep_size == 0 and from_dram(p):
                queue.append(p)

        current_cycle = 0
        # BFS to traverse the pipe graph
        visited = set()
        while queue:
            p = queue.popleft()
            visited.add(p)

            # CNSim do not support collectives such as multicast / gather / broadcast for now, so we need to translate 
            # the collectives to unicast packets right now. It could greatly hurts performance. 
            packets = generate_packets(p, current_cycle)

            # TODO: Check tt-buda/backend for computation time, may be we could calculate it with op name in direct_pt_netlist.yaml

            print("\n".join(str(p) for p in packets))    

            for child in p.succeed_pipes():
                if child not in visited:
                    child.dep_size -= 1
                    if child.dep_size == 0:
                        queue.append(child)


if __name__ == "__main__":
    simple_list = SimpNetlist("/home/tt-buda/direct_pt_netlist.yaml", "/home/tt-buda/net2pipe_output")
    out_trace = NTTrace()
    generate_trace(simple_list, out_trace)

    out_trace.nt_print_trace()